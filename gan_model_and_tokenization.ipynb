{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b26b6e1b-a96d-4b8f-b69c-97caad5edb75",
   "metadata": {},
   "source": [
    "# Training a Simple GAN Model for Sentence Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9f13714e-a0d7-40f6-adce-bbdc125592ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.datasets as datasets\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.tensorboard import SummaryWriter  # to print to tensorboard\n",
    "import pandas as pd\n",
    "import ast\n",
    "import numpy as np\n",
    "\n",
    "MAX_LENGTH = 768\n",
    "\n",
    "\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, in_features):\n",
    "        super().__init__()\n",
    "        self.disc = nn.Sequential(\n",
    "            nn.Linear(in_features, 128),\n",
    "            nn.LeakyReLU(0.01),\n",
    "            nn.Linear(128, 1),\n",
    "            nn.Sigmoid(),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.disc(x)\n",
    "\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self, z_dim, emb_dim):\n",
    "        super().__init__()\n",
    "        self.gen = nn.Sequential(\n",
    "            nn.Linear(z_dim, 256),\n",
    "            nn.LeakyReLU(0.01),\n",
    "            nn.Linear(256, emb_dim),\n",
    "            nn.Tanh(),  # Assuming you want to normalize the outputs\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.gen(x)\n",
    "\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, embeddings):\n",
    "        self.embeddings = embeddings\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.embeddings)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.embeddings[idx]\n",
    "\n",
    "\n",
    "\n",
    "# Hyperparameters etc.\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "lr = 3e-4\n",
    "z_dim = 64\n",
    "embed_dim = MAX_LENGTH  # 784\n",
    "batch_size = 32\n",
    "num_epochs = 50\n",
    "\n",
    "disc = Discriminator(embed_dim).to(device)\n",
    "gen = Generator(z_dim, embed_dim).to(device)\n",
    "fixed_noise = torch.randn((batch_size, z_dim)).to(device)\n",
    "transforms = transforms.Compose(\n",
    "    [\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.5,), (0.5,)),\n",
    "    ]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f5696eb-9815-4a42-a1cd-85ed58a42387",
   "metadata": {},
   "source": [
    "# Preprocessing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "c1cc60b7-a136-4ea6-9575-1c1f96f65ab4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>author_labels</th>\n",
       "      <th>cls_tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>[0.278237104415893, -0.33750003576278603, 0.84...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>[-0.13946822285652102, 0.093057677149772, 0.73...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>[-1.493581295013427, 0.748962104320526, 1.0086...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>[-0.9141901135444641, 0.6804959774017331, 0.90...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>[0.040547348558902005, 0.08085644245147701, 1....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32592</th>\n",
       "      <td>32592</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.036508537828922, -0.9830706119537351, 0.194...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32593</th>\n",
       "      <td>32593</td>\n",
       "      <td>0</td>\n",
       "      <td>[-0.49513417482376104, -0.42453348636627203, 0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32594</th>\n",
       "      <td>32594</td>\n",
       "      <td>0</td>\n",
       "      <td>[-0.6720252633094781, -0.37544131278991705, 2....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32595</th>\n",
       "      <td>32595</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.153745874762535, -0.533583104610443, -0.371...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32596</th>\n",
       "      <td>32596</td>\n",
       "      <td>0</td>\n",
       "      <td>[-0.7284083962440491, -0.38280051946640004, -0...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>32597 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Unnamed: 0  author_labels  \\\n",
       "0               0              1   \n",
       "1               1              1   \n",
       "2               2              1   \n",
       "3               3              1   \n",
       "4               4              1   \n",
       "...           ...            ...   \n",
       "32592       32592              0   \n",
       "32593       32593              0   \n",
       "32594       32594              0   \n",
       "32595       32595              0   \n",
       "32596       32596              0   \n",
       "\n",
       "                                              cls_tokens  \n",
       "0      [0.278237104415893, -0.33750003576278603, 0.84...  \n",
       "1      [-0.13946822285652102, 0.093057677149772, 0.73...  \n",
       "2      [-1.493581295013427, 0.748962104320526, 1.0086...  \n",
       "3      [-0.9141901135444641, 0.6804959774017331, 0.90...  \n",
       "4      [0.040547348558902005, 0.08085644245147701, 1....  \n",
       "...                                                  ...  \n",
       "32592  [0.036508537828922, -0.9830706119537351, 0.194...  \n",
       "32593  [-0.49513417482376104, -0.42453348636627203, 0...  \n",
       "32594  [-0.6720252633094781, -0.37544131278991705, 2....  \n",
       "32595  [0.153745874762535, -0.533583104610443, -0.371...  \n",
       "32596  [-0.7284083962440491, -0.38280051946640004, -0...  \n",
       "\n",
       "[32597 rows x 3 columns]"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Limit number of rows for experimentation\n",
    "df = pd.read_csv('author_csv.csv')\n",
    "num_rows = len(df)\n",
    "df = df[:num_rows]\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "bab501a5-f34e-46e9-ba16-b1dfa185bfa0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.2782, -0.3375,  0.8478,  ..., -0.7637, -0.5338,  0.3607],\n",
       "        [-0.1395,  0.0931,  0.7390,  ...,  0.8308, -0.3973,  1.1339],\n",
       "        [-1.4936,  0.7490,  1.0087,  ...,  1.5033, -1.2829, -0.5658],\n",
       "        ...,\n",
       "        [-0.6720, -0.3754,  2.1211,  ...,  1.0377, -0.6048, -0.7254],\n",
       "        [ 0.1537, -0.5336, -0.3715,  ...,  1.6371,  0.6499, -0.5228],\n",
       "        [-0.7284, -0.3828, -0.2024,  ...,  1.4613,  0.1921,  1.5803]])"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the text lines\n",
    "\n",
    "embeddings = df['cls_tokens']\n",
    "\n",
    "#Turn EagerTensors list to Normal Tensors list\n",
    "embeddings_pytorch = [torch.tensor(np.array(ast.literal_eval(e)), dtype=torch.float32) for e in embeddings]\n",
    "\n",
    "# Convert list of tensors to a single tensor\n",
    "embeddings_tensor = torch.stack(embeddings_pytorch).squeeze(1)  # Adjust dimensions as needed\n",
    "\n",
    "embeddings_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "6bd18ba9-ad03-441b-ae16-6c57b6299035",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32597, 768])"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings_tensor.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "794028ae-f73a-4a32-ac2f-a684d6aa1979",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = embeddings_tensor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "9ffebf5c-56d3-4a3b-85d9-2f8d7609f250",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Initialize Dataset\n",
    "embeddings = embeddings_tensor \n",
    "\n",
    "# Instantiate the custom dataset\n",
    "dataset = CustomDataset(embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7316bd16-7e44-41eb-8334-b087d75a97bf",
   "metadata": {},
   "source": [
    "# Training the Actual GAN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "b23b0d57-0fad-44ba-ae37-797cbf65d23a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [0/10] Batch 0/510               Loss D: 0.2026, loss G: 1.8443\n",
      "Epoch [0/10] Batch 1/510               Loss D: 0.2227, loss G: 1.9815\n",
      "Epoch [0/10] Batch 2/510               Loss D: 0.3088, loss G: 2.0508\n",
      "Epoch [0/10] Batch 3/510               Loss D: 0.1642, loss G: 2.1059\n",
      "Epoch [0/10] Batch 4/510               Loss D: 0.1674, loss G: 2.1592\n",
      "Epoch [0/10] Batch 5/510               Loss D: 0.1575, loss G: 2.2047\n",
      "Epoch [0/10] Batch 6/510               Loss D: 0.1380, loss G: 2.2544\n",
      "Epoch [0/10] Batch 7/510               Loss D: 0.1456, loss G: 2.3149\n",
      "Epoch [0/10] Batch 8/510               Loss D: 0.1931, loss G: 2.3384\n",
      "Epoch [0/10] Batch 9/510               Loss D: 0.1785, loss G: 2.3505\n",
      "Epoch [0/10] Batch 10/510               Loss D: 0.1213, loss G: 2.3451\n",
      "Epoch [0/10] Batch 11/510               Loss D: 0.1086, loss G: 2.3685\n",
      "Epoch [0/10] Batch 12/510               Loss D: 0.1304, loss G: 2.3658\n",
      "Epoch [0/10] Batch 13/510               Loss D: 0.2303, loss G: 2.3554\n",
      "Epoch [0/10] Batch 14/510               Loss D: 0.1076, loss G: 2.3188\n",
      "Epoch [0/10] Batch 15/510               Loss D: 0.0949, loss G: 2.3118\n",
      "Epoch [0/10] Batch 16/510               Loss D: 0.1624, loss G: 2.3080\n",
      "Epoch [0/10] Batch 17/510               Loss D: 0.1094, loss G: 2.2912\n",
      "Epoch [0/10] Batch 18/510               Loss D: 0.1347, loss G: 2.2763\n",
      "Epoch [0/10] Batch 19/510               Loss D: 0.0885, loss G: 2.2463\n",
      "Epoch [0/10] Batch 20/510               Loss D: 0.1099, loss G: 2.2699\n",
      "Epoch [0/10] Batch 21/510               Loss D: 0.1019, loss G: 2.2745\n",
      "Epoch [0/10] Batch 22/510               Loss D: 0.1258, loss G: 2.2662\n",
      "Epoch [0/10] Batch 23/510               Loss D: 0.1127, loss G: 2.2519\n",
      "Epoch [0/10] Batch 24/510               Loss D: 0.1048, loss G: 2.2495\n",
      "Epoch [0/10] Batch 25/510               Loss D: 0.1032, loss G: 2.2640\n",
      "Epoch [0/10] Batch 26/510               Loss D: 0.0916, loss G: 2.2986\n",
      "Epoch [0/10] Batch 27/510               Loss D: 0.0814, loss G: 2.3015\n",
      "Epoch [0/10] Batch 28/510               Loss D: 0.0910, loss G: 2.3095\n",
      "Epoch [0/10] Batch 29/510               Loss D: 0.1267, loss G: 2.3126\n",
      "Epoch [0/10] Batch 30/510               Loss D: 0.0922, loss G: 2.3061\n",
      "Epoch [0/10] Batch 31/510               Loss D: 0.1307, loss G: 2.3094\n",
      "Epoch [0/10] Batch 32/510               Loss D: 0.0810, loss G: 2.3187\n",
      "Epoch [0/10] Batch 33/510               Loss D: 0.0978, loss G: 2.3318\n",
      "Epoch [0/10] Batch 34/510               Loss D: 0.1103, loss G: 2.3354\n",
      "Epoch [0/10] Batch 35/510               Loss D: 0.1595, loss G: 2.3179\n",
      "Epoch [0/10] Batch 36/510               Loss D: 0.0938, loss G: 2.2887\n",
      "Epoch [0/10] Batch 37/510               Loss D: 0.0906, loss G: 2.2643\n",
      "Epoch [0/10] Batch 38/510               Loss D: 0.1087, loss G: 2.2614\n",
      "Epoch [0/10] Batch 39/510               Loss D: 0.0976, loss G: 2.2440\n",
      "Epoch [0/10] Batch 40/510               Loss D: 0.1809, loss G: 2.1877\n",
      "Epoch [0/10] Batch 41/510               Loss D: 0.1521, loss G: 2.1624\n",
      "Epoch [0/10] Batch 42/510               Loss D: 0.1181, loss G: 2.1277\n",
      "Epoch [0/10] Batch 43/510               Loss D: 0.1492, loss G: 2.1002\n",
      "Epoch [0/10] Batch 44/510               Loss D: 0.0975, loss G: 2.0533\n",
      "Epoch [0/10] Batch 45/510               Loss D: 0.1180, loss G: 2.0475\n",
      "Epoch [0/10] Batch 46/510               Loss D: 0.1069, loss G: 2.0185\n",
      "Epoch [0/10] Batch 47/510               Loss D: 0.1161, loss G: 2.0136\n",
      "Epoch [0/10] Batch 48/510               Loss D: 0.1220, loss G: 2.0033\n",
      "Epoch [0/10] Batch 49/510               Loss D: 0.1286, loss G: 2.0153\n",
      "Epoch [0/10] Batch 50/510               Loss D: 0.1537, loss G: 2.0186\n",
      "Epoch [0/10] Batch 51/510               Loss D: 0.1307, loss G: 1.9896\n",
      "Epoch [0/10] Batch 52/510               Loss D: 0.1753, loss G: 1.9791\n",
      "Epoch [0/10] Batch 53/510               Loss D: 0.1518, loss G: 1.9599\n",
      "Epoch [0/10] Batch 54/510               Loss D: 0.1734, loss G: 1.9400\n",
      "Epoch [0/10] Batch 55/510               Loss D: 0.1271, loss G: 1.9163\n",
      "Epoch [0/10] Batch 56/510               Loss D: 0.1348, loss G: 1.8990\n",
      "Epoch [0/10] Batch 57/510               Loss D: 0.1271, loss G: 1.8794\n",
      "Epoch [0/10] Batch 58/510               Loss D: 0.2288, loss G: 1.8400\n",
      "Epoch [0/10] Batch 59/510               Loss D: 0.1205, loss G: 1.8111\n",
      "Epoch [0/10] Batch 60/510               Loss D: 0.1339, loss G: 1.7848\n",
      "Epoch [0/10] Batch 61/510               Loss D: 0.1494, loss G: 1.7758\n",
      "Epoch [0/10] Batch 62/510               Loss D: 0.1645, loss G: 1.7398\n",
      "Epoch [0/10] Batch 63/510               Loss D: 0.1562, loss G: 1.7129\n",
      "Epoch [0/10] Batch 64/510               Loss D: 0.1470, loss G: 1.6804\n",
      "Epoch [0/10] Batch 65/510               Loss D: 0.1558, loss G: 1.6632\n",
      "Epoch [0/10] Batch 66/510               Loss D: 0.1526, loss G: 1.6529\n",
      "Epoch [0/10] Batch 67/510               Loss D: 0.1602, loss G: 1.6379\n",
      "Epoch [0/10] Batch 68/510               Loss D: 0.1983, loss G: 1.6121\n",
      "Epoch [0/10] Batch 69/510               Loss D: 0.1737, loss G: 1.5817\n",
      "Epoch [0/10] Batch 70/510               Loss D: 0.1908, loss G: 1.5624\n",
      "Epoch [0/10] Batch 71/510               Loss D: 0.2850, loss G: 1.5178\n",
      "Epoch [0/10] Batch 72/510               Loss D: 0.1963, loss G: 1.4755\n",
      "Epoch [0/10] Batch 73/510               Loss D: 0.2285, loss G: 1.4255\n",
      "Epoch [0/10] Batch 74/510               Loss D: 0.1996, loss G: 1.3905\n",
      "Epoch [0/10] Batch 75/510               Loss D: 0.2121, loss G: 1.3567\n",
      "Epoch [0/10] Batch 76/510               Loss D: 0.2414, loss G: 1.3210\n",
      "Epoch [0/10] Batch 77/510               Loss D: 0.2453, loss G: 1.2892\n",
      "Epoch [0/10] Batch 78/510               Loss D: 0.2500, loss G: 1.2615\n",
      "Epoch [0/10] Batch 79/510               Loss D: 0.3192, loss G: 1.2449\n",
      "Epoch [0/10] Batch 80/510               Loss D: 0.3405, loss G: 1.1924\n",
      "Epoch [0/10] Batch 81/510               Loss D: 0.3054, loss G: 1.1679\n",
      "Epoch [0/10] Batch 82/510               Loss D: 0.4367, loss G: 1.1168\n",
      "Epoch [0/10] Batch 83/510               Loss D: 0.2891, loss G: 1.0811\n",
      "Epoch [0/10] Batch 84/510               Loss D: 0.3169, loss G: 1.0576\n",
      "Epoch [0/10] Batch 85/510               Loss D: 0.3808, loss G: 1.0226\n",
      "Epoch [0/10] Batch 86/510               Loss D: 0.3208, loss G: 1.0024\n",
      "Epoch [0/10] Batch 87/510               Loss D: 0.3601, loss G: 0.9927\n",
      "Epoch [0/10] Batch 88/510               Loss D: 0.3234, loss G: 0.9820\n",
      "Epoch [0/10] Batch 89/510               Loss D: 0.4005, loss G: 0.9891\n",
      "Epoch [0/10] Batch 90/510               Loss D: 0.4108, loss G: 0.9924\n",
      "Epoch [0/10] Batch 91/510               Loss D: 0.4035, loss G: 0.9946\n",
      "Epoch [0/10] Batch 92/510               Loss D: 0.3212, loss G: 1.0024\n",
      "Epoch [0/10] Batch 93/510               Loss D: 0.3919, loss G: 1.0175\n",
      "Epoch [0/10] Batch 94/510               Loss D: 0.3917, loss G: 1.0257\n",
      "Epoch [0/10] Batch 95/510               Loss D: 0.3324, loss G: 1.0417\n",
      "Epoch [0/10] Batch 96/510               Loss D: 0.3644, loss G: 1.0531\n",
      "Epoch [0/10] Batch 97/510               Loss D: 0.4066, loss G: 1.0652\n",
      "Epoch [0/10] Batch 98/510               Loss D: 0.3706, loss G: 1.0787\n",
      "Epoch [0/10] Batch 99/510               Loss D: 0.3120, loss G: 1.0935\n",
      "Epoch [0/10] Batch 100/510               Loss D: 0.3484, loss G: 1.1102\n",
      "Epoch [0/10] Batch 101/510               Loss D: 0.2546, loss G: 1.1354\n",
      "Epoch [0/10] Batch 102/510               Loss D: 0.2469, loss G: 1.1783\n",
      "Epoch [0/10] Batch 103/510               Loss D: 0.2464, loss G: 1.2200\n",
      "Epoch [0/10] Batch 104/510               Loss D: 0.3206, loss G: 1.2666\n",
      "Epoch [0/10] Batch 105/510               Loss D: 0.2504, loss G: 1.3213\n",
      "Epoch [0/10] Batch 106/510               Loss D: 0.2766, loss G: 1.3612\n",
      "Epoch [0/10] Batch 107/510               Loss D: 0.2130, loss G: 1.4141\n",
      "Epoch [0/10] Batch 108/510               Loss D: 0.1912, loss G: 1.4625\n",
      "Epoch [0/10] Batch 109/510               Loss D: 0.3949, loss G: 1.4776\n",
      "Epoch [0/10] Batch 110/510               Loss D: 0.2988, loss G: 1.4800\n",
      "Epoch [0/10] Batch 111/510               Loss D: 0.2253, loss G: 1.4769\n",
      "Epoch [0/10] Batch 112/510               Loss D: 0.3104, loss G: 1.4524\n",
      "Epoch [0/10] Batch 113/510               Loss D: 0.2283, loss G: 1.4320\n",
      "Epoch [0/10] Batch 114/510               Loss D: 0.2360, loss G: 1.4052\n",
      "Epoch [0/10] Batch 115/510               Loss D: 0.2790, loss G: 1.3730\n",
      "Epoch [0/10] Batch 116/510               Loss D: 0.3727, loss G: 1.3398\n",
      "Epoch [0/10] Batch 117/510               Loss D: 0.3464, loss G: 1.3013\n",
      "Epoch [0/10] Batch 118/510               Loss D: 0.3198, loss G: 1.2447\n",
      "Epoch [0/10] Batch 119/510               Loss D: 0.3012, loss G: 1.2155\n",
      "Epoch [0/10] Batch 120/510               Loss D: 0.4425, loss G: 1.1539\n",
      "Epoch [0/10] Batch 121/510               Loss D: 0.3873, loss G: 1.1137\n",
      "Epoch [0/10] Batch 122/510               Loss D: 0.3019, loss G: 1.0863\n",
      "Epoch [0/10] Batch 123/510               Loss D: 0.4388, loss G: 1.0509\n",
      "Epoch [0/10] Batch 124/510               Loss D: 0.4896, loss G: 1.0181\n",
      "Epoch [0/10] Batch 125/510               Loss D: 0.3929, loss G: 0.9965\n",
      "Epoch [0/10] Batch 126/510               Loss D: 0.4597, loss G: 0.9760\n",
      "Epoch [0/10] Batch 127/510               Loss D: 0.3604, loss G: 0.9592\n",
      "Epoch [0/10] Batch 128/510               Loss D: 0.4879, loss G: 0.9484\n",
      "Epoch [0/10] Batch 129/510               Loss D: 0.4158, loss G: 0.9393\n",
      "Epoch [0/10] Batch 130/510               Loss D: 0.4054, loss G: 0.9405\n",
      "Epoch [0/10] Batch 131/510               Loss D: 0.3777, loss G: 0.9439\n",
      "Epoch [0/10] Batch 132/510               Loss D: 0.3395, loss G: 0.9706\n",
      "Epoch [0/10] Batch 133/510               Loss D: 0.4220, loss G: 0.9859\n",
      "Epoch [0/10] Batch 134/510               Loss D: 0.4171, loss G: 1.0033\n",
      "Epoch [0/10] Batch 135/510               Loss D: 0.2914, loss G: 1.0343\n",
      "Epoch [0/10] Batch 136/510               Loss D: 0.4829, loss G: 1.0383\n",
      "Epoch [0/10] Batch 137/510               Loss D: 0.3730, loss G: 1.0379\n",
      "Epoch [0/10] Batch 138/510               Loss D: 0.4280, loss G: 1.0337\n",
      "Epoch [0/10] Batch 139/510               Loss D: 0.4046, loss G: 1.0214\n",
      "Epoch [0/10] Batch 140/510               Loss D: 0.3495, loss G: 1.0261\n",
      "Epoch [0/10] Batch 141/510               Loss D: 0.4937, loss G: 1.0068\n",
      "Epoch [0/10] Batch 142/510               Loss D: 0.9985, loss G: 0.9438\n",
      "Epoch [0/10] Batch 143/510               Loss D: 0.5964, loss G: 0.8753\n",
      "Epoch [0/10] Batch 144/510               Loss D: 0.6135, loss G: 0.7983\n",
      "Epoch [0/10] Batch 145/510               Loss D: 0.5906, loss G: 0.7372\n",
      "Epoch [0/10] Batch 146/510               Loss D: 0.7263, loss G: 0.6997\n",
      "Epoch [0/10] Batch 147/510               Loss D: 0.4716, loss G: 0.6972\n",
      "Epoch [0/10] Batch 148/510               Loss D: 0.5650, loss G: 0.7052\n",
      "Epoch [0/10] Batch 149/510               Loss D: 0.4574, loss G: 0.7355\n",
      "Epoch [0/10] Batch 150/510               Loss D: 0.3908, loss G: 0.7940\n",
      "Epoch [0/10] Batch 151/510               Loss D: 0.5080, loss G: 0.8585\n",
      "Epoch [0/10] Batch 152/510               Loss D: 0.3602, loss G: 0.9368\n",
      "Epoch [0/10] Batch 153/510               Loss D: 0.3713, loss G: 1.0214\n",
      "Epoch [0/10] Batch 154/510               Loss D: 0.3632, loss G: 1.1180\n",
      "Epoch [0/10] Batch 155/510               Loss D: 0.3142, loss G: 1.2052\n",
      "Epoch [0/10] Batch 156/510               Loss D: 0.3310, loss G: 1.2898\n",
      "Epoch [0/10] Batch 157/510               Loss D: 0.2661, loss G: 1.3580\n",
      "Epoch [0/10] Batch 158/510               Loss D: 0.3302, loss G: 1.4238\n",
      "Epoch [0/10] Batch 159/510               Loss D: 0.2242, loss G: 1.4889\n",
      "Epoch [0/10] Batch 160/510               Loss D: 0.1843, loss G: 1.5340\n",
      "Epoch [0/10] Batch 161/510               Loss D: 0.1690, loss G: 1.5844\n",
      "Epoch [0/10] Batch 162/510               Loss D: 0.2543, loss G: 1.5992\n",
      "Epoch [0/10] Batch 163/510               Loss D: 0.2228, loss G: 1.6095\n",
      "Epoch [0/10] Batch 164/510               Loss D: 0.2695, loss G: 1.6217\n",
      "Epoch [0/10] Batch 165/510               Loss D: 0.2444, loss G: 1.5929\n",
      "Epoch [0/10] Batch 166/510               Loss D: 0.5939, loss G: 1.5398\n",
      "Epoch [0/10] Batch 167/510               Loss D: 0.2895, loss G: 1.4456\n",
      "Epoch [0/10] Batch 168/510               Loss D: 0.3180, loss G: 1.3650\n",
      "Epoch [0/10] Batch 169/510               Loss D: 0.2673, loss G: 1.3027\n",
      "Epoch [0/10] Batch 170/510               Loss D: 0.2728, loss G: 1.2261\n",
      "Epoch [0/10] Batch 171/510               Loss D: 0.2737, loss G: 1.1772\n",
      "Epoch [0/10] Batch 172/510               Loss D: 0.3489, loss G: 1.1098\n",
      "Epoch [0/10] Batch 173/510               Loss D: 0.3347, loss G: 1.0724\n",
      "Epoch [0/10] Batch 174/510               Loss D: 0.3027, loss G: 1.0302\n",
      "Epoch [0/10] Batch 175/510               Loss D: 0.3155, loss G: 1.0081\n",
      "Epoch [0/10] Batch 176/510               Loss D: 0.3224, loss G: 0.9875\n",
      "Epoch [0/10] Batch 177/510               Loss D: 0.3011, loss G: 0.9919\n",
      "Epoch [0/10] Batch 178/510               Loss D: 0.3089, loss G: 0.9932\n",
      "Epoch [0/10] Batch 179/510               Loss D: 0.3540, loss G: 1.0077\n",
      "Epoch [0/10] Batch 180/510               Loss D: 0.2986, loss G: 1.0317\n",
      "Epoch [0/10] Batch 181/510               Loss D: 0.4035, loss G: 1.0498\n",
      "Epoch [0/10] Batch 182/510               Loss D: 0.4615, loss G: 1.0542\n",
      "Epoch [0/10] Batch 183/510               Loss D: 0.3945, loss G: 1.0566\n",
      "Epoch [0/10] Batch 184/510               Loss D: 0.3370, loss G: 1.0563\n",
      "Epoch [0/10] Batch 185/510               Loss D: 0.4558, loss G: 1.0509\n",
      "Epoch [0/10] Batch 186/510               Loss D: 0.3144, loss G: 1.0481\n",
      "Epoch [0/10] Batch 187/510               Loss D: 0.3715, loss G: 1.0369\n",
      "Epoch [0/10] Batch 188/510               Loss D: 0.3434, loss G: 1.0238\n",
      "Epoch [0/10] Batch 189/510               Loss D: 0.3675, loss G: 1.0108\n",
      "Epoch [0/10] Batch 190/510               Loss D: 0.6724, loss G: 0.9782\n",
      "Epoch [0/10] Batch 191/510               Loss D: 0.3208, loss G: 0.9542\n",
      "Epoch [0/10] Batch 192/510               Loss D: 0.3355, loss G: 0.9329\n",
      "Epoch [0/10] Batch 193/510               Loss D: 0.4545, loss G: 0.9115\n",
      "Epoch [0/10] Batch 194/510               Loss D: 0.4024, loss G: 0.8966\n",
      "Epoch [0/10] Batch 195/510               Loss D: 0.3567, loss G: 0.8903\n",
      "Epoch [0/10] Batch 196/510               Loss D: 0.3517, loss G: 0.8878\n",
      "Epoch [0/10] Batch 197/510               Loss D: 0.3990, loss G: 0.8959\n",
      "Epoch [0/10] Batch 198/510               Loss D: 0.3966, loss G: 0.9046\n",
      "Epoch [0/10] Batch 199/510               Loss D: 0.3727, loss G: 0.9121\n",
      "Epoch [0/10] Batch 200/510               Loss D: 0.3636, loss G: 0.9223\n",
      "Epoch [0/10] Batch 201/510               Loss D: 0.4808, loss G: 0.9269\n",
      "Epoch [0/10] Batch 202/510               Loss D: 0.3319, loss G: 0.9392\n",
      "Epoch [0/10] Batch 203/510               Loss D: 0.3267, loss G: 0.9482\n",
      "Epoch [0/10] Batch 204/510               Loss D: 0.3170, loss G: 0.9749\n",
      "Epoch [0/10] Batch 205/510               Loss D: 0.3326, loss G: 0.9883\n",
      "Epoch [0/10] Batch 206/510               Loss D: 0.3749, loss G: 1.0115\n",
      "Epoch [0/10] Batch 207/510               Loss D: 0.6478, loss G: 1.0007\n",
      "Epoch [0/10] Batch 208/510               Loss D: 0.3127, loss G: 1.0005\n",
      "Epoch [0/10] Batch 209/510               Loss D: 0.3560, loss G: 0.9918\n",
      "Epoch [0/10] Batch 210/510               Loss D: 0.6141, loss G: 0.9642\n",
      "Epoch [0/10] Batch 211/510               Loss D: 0.4264, loss G: 0.9397\n",
      "Epoch [0/10] Batch 212/510               Loss D: 0.6091, loss G: 0.9069\n",
      "Epoch [0/10] Batch 213/510               Loss D: 0.4151, loss G: 0.8762\n",
      "Epoch [0/10] Batch 214/510               Loss D: 0.3486, loss G: 0.8554\n",
      "Epoch [0/10] Batch 215/510               Loss D: 0.4283, loss G: 0.8455\n",
      "Epoch [0/10] Batch 216/510               Loss D: 0.6454, loss G: 0.8333\n",
      "Epoch [0/10] Batch 217/510               Loss D: 0.4928, loss G: 0.8145\n",
      "Epoch [0/10] Batch 218/510               Loss D: 0.5044, loss G: 0.8043\n",
      "Epoch [0/10] Batch 219/510               Loss D: 0.3900, loss G: 0.8002\n",
      "Epoch [0/10] Batch 220/510               Loss D: 0.4059, loss G: 0.8122\n",
      "Epoch [0/10] Batch 221/510               Loss D: 0.4896, loss G: 0.8173\n",
      "Epoch [0/10] Batch 222/510               Loss D: 0.3531, loss G: 0.8342\n",
      "Epoch [0/10] Batch 223/510               Loss D: 0.3468, loss G: 0.8633\n",
      "Epoch [0/10] Batch 224/510               Loss D: 0.4651, loss G: 0.8794\n",
      "Epoch [0/10] Batch 225/510               Loss D: 0.3752, loss G: 0.9097\n",
      "Epoch [0/10] Batch 226/510               Loss D: 0.5250, loss G: 0.9341\n",
      "Epoch [0/10] Batch 227/510               Loss D: 0.3982, loss G: 0.9516\n",
      "Epoch [0/10] Batch 228/510               Loss D: 0.3447, loss G: 0.9787\n",
      "Epoch [0/10] Batch 229/510               Loss D: 0.3027, loss G: 1.0165\n",
      "Epoch [0/10] Batch 230/510               Loss D: 0.3505, loss G: 1.0399\n",
      "Epoch [0/10] Batch 231/510               Loss D: 0.2651, loss G: 1.0766\n",
      "Epoch [0/10] Batch 232/510               Loss D: 0.2557, loss G: 1.1153\n",
      "Epoch [0/10] Batch 233/510               Loss D: 0.2564, loss G: 1.1535\n",
      "Epoch [0/10] Batch 234/510               Loss D: 0.3556, loss G: 1.1838\n",
      "Epoch [0/10] Batch 235/510               Loss D: 0.2775, loss G: 1.2096\n",
      "Epoch [0/10] Batch 236/510               Loss D: 0.2395, loss G: 1.2334\n",
      "Epoch [0/10] Batch 237/510               Loss D: 0.2414, loss G: 1.2564\n",
      "Epoch [0/10] Batch 238/510               Loss D: 0.2401, loss G: 1.2670\n",
      "Epoch [0/10] Batch 239/510               Loss D: 0.3741, loss G: 1.2806\n",
      "Epoch [0/10] Batch 240/510               Loss D: 0.3826, loss G: 1.2714\n",
      "Epoch [0/10] Batch 241/510               Loss D: 0.4649, loss G: 1.2455\n",
      "Epoch [0/10] Batch 242/510               Loss D: 0.2463, loss G: 1.2348\n",
      "Epoch [0/10] Batch 243/510               Loss D: 0.2460, loss G: 1.2121\n",
      "Epoch [0/10] Batch 244/510               Loss D: 0.2654, loss G: 1.1969\n",
      "Epoch [0/10] Batch 245/510               Loss D: 0.2689, loss G: 1.1820\n",
      "Epoch [0/10] Batch 246/510               Loss D: 0.2728, loss G: 1.1737\n",
      "Epoch [0/10] Batch 247/510               Loss D: 0.2655, loss G: 1.1590\n",
      "Epoch [0/10] Batch 248/510               Loss D: 0.2823, loss G: 1.1515\n",
      "Epoch [0/10] Batch 249/510               Loss D: 0.2498, loss G: 1.1590\n",
      "Epoch [0/10] Batch 250/510               Loss D: 0.2883, loss G: 1.1648\n",
      "Epoch [0/10] Batch 251/510               Loss D: 0.2566, loss G: 1.1674\n",
      "Epoch [0/10] Batch 252/510               Loss D: 0.2476, loss G: 1.1855\n",
      "Epoch [0/10] Batch 253/510               Loss D: 0.2616, loss G: 1.1961\n",
      "Epoch [0/10] Batch 254/510               Loss D: 0.2327, loss G: 1.2071\n",
      "Epoch [0/10] Batch 255/510               Loss D: 0.5050, loss G: 1.2179\n",
      "Epoch [0/10] Batch 256/510               Loss D: 0.2466, loss G: 1.2267\n",
      "Epoch [0/10] Batch 257/510               Loss D: 0.2469, loss G: 1.2231\n",
      "Epoch [0/10] Batch 258/510               Loss D: 0.2531, loss G: 1.2418\n",
      "Epoch [0/10] Batch 259/510               Loss D: 0.2993, loss G: 1.2563\n",
      "Epoch [0/10] Batch 260/510               Loss D: 0.4782, loss G: 1.2455\n",
      "Epoch [0/10] Batch 261/510               Loss D: 0.2848, loss G: 1.2783\n",
      "Epoch [0/10] Batch 262/510               Loss D: 0.3873, loss G: 1.2603\n",
      "Epoch [0/10] Batch 263/510               Loss D: 0.2429, loss G: 1.2652\n",
      "Epoch [0/10] Batch 264/510               Loss D: 0.2401, loss G: 1.2809\n",
      "Epoch [0/10] Batch 265/510               Loss D: 0.4131, loss G: 1.2927\n",
      "Epoch [0/10] Batch 266/510               Loss D: 0.7458, loss G: 1.2699\n",
      "Epoch [0/10] Batch 267/510               Loss D: 0.2685, loss G: 1.2641\n",
      "Epoch [0/10] Batch 268/510               Loss D: 0.4891, loss G: 1.2561\n",
      "Epoch [0/10] Batch 269/510               Loss D: 0.2335, loss G: 1.2641\n",
      "Epoch [0/10] Batch 270/510               Loss D: 0.2504, loss G: 1.2805\n",
      "Epoch [0/10] Batch 271/510               Loss D: 0.1960, loss G: 1.3110\n",
      "Epoch [0/10] Batch 272/510               Loss D: 0.2765, loss G: 1.3366\n",
      "Epoch [0/10] Batch 273/510               Loss D: 0.2038, loss G: 1.3682\n",
      "Epoch [0/10] Batch 274/510               Loss D: 0.2017, loss G: 1.4177\n",
      "Epoch [0/10] Batch 275/510               Loss D: 0.1712, loss G: 1.4689\n",
      "Epoch [0/10] Batch 276/510               Loss D: 0.1614, loss G: 1.5332\n",
      "Epoch [0/10] Batch 277/510               Loss D: 0.1555, loss G: 1.5957\n",
      "Epoch [0/10] Batch 278/510               Loss D: 0.2628, loss G: 1.6494\n",
      "Epoch [0/10] Batch 279/510               Loss D: 0.1385, loss G: 1.7012\n",
      "Epoch [0/10] Batch 280/510               Loss D: 0.1390, loss G: 1.7515\n",
      "Epoch [0/10] Batch 281/510               Loss D: 0.1571, loss G: 1.7954\n",
      "Epoch [0/10] Batch 282/510               Loss D: 0.1303, loss G: 1.8377\n",
      "Epoch [0/10] Batch 283/510               Loss D: 0.1253, loss G: 1.8719\n",
      "Epoch [0/10] Batch 284/510               Loss D: 0.1099, loss G: 1.8904\n",
      "Epoch [0/10] Batch 285/510               Loss D: 0.1338, loss G: 1.9315\n",
      "Epoch [0/10] Batch 286/510               Loss D: 0.1118, loss G: 1.9472\n",
      "Epoch [0/10] Batch 287/510               Loss D: 0.1361, loss G: 1.9549\n",
      "Epoch [0/10] Batch 288/510               Loss D: 0.1277, loss G: 1.9444\n",
      "Epoch [0/10] Batch 289/510               Loss D: 0.1091, loss G: 1.9414\n",
      "Epoch [0/10] Batch 290/510               Loss D: 0.1085, loss G: 1.9316\n",
      "Epoch [0/10] Batch 291/510               Loss D: 0.6101, loss G: 1.8728\n",
      "Epoch [0/10] Batch 292/510               Loss D: 0.1595, loss G: 1.8242\n",
      "Epoch [0/10] Batch 293/510               Loss D: 0.1125, loss G: 1.7703\n",
      "Epoch [0/10] Batch 294/510               Loss D: 0.1463, loss G: 1.7105\n",
      "Epoch [0/10] Batch 295/510               Loss D: 0.1430, loss G: 1.6865\n",
      "Epoch [0/10] Batch 296/510               Loss D: 0.2594, loss G: 1.6268\n",
      "Epoch [0/10] Batch 297/510               Loss D: 0.1392, loss G: 1.5966\n",
      "Epoch [0/10] Batch 298/510               Loss D: 0.1446, loss G: 1.5796\n",
      "Epoch [0/10] Batch 299/510               Loss D: 0.1379, loss G: 1.5853\n",
      "Epoch [0/10] Batch 300/510               Loss D: 0.1396, loss G: 1.5784\n",
      "Epoch [0/10] Batch 301/510               Loss D: 0.1416, loss G: 1.5874\n",
      "Epoch [0/10] Batch 302/510               Loss D: 0.1506, loss G: 1.6091\n",
      "Epoch [0/10] Batch 303/510               Loss D: 0.1372, loss G: 1.6275\n",
      "Epoch [0/10] Batch 304/510               Loss D: 0.1609, loss G: 1.6400\n",
      "Epoch [0/10] Batch 305/510               Loss D: 0.2275, loss G: 1.6614\n",
      "Epoch [0/10] Batch 306/510               Loss D: 0.2180, loss G: 1.6582\n",
      "Epoch [0/10] Batch 307/510               Loss D: 0.1664, loss G: 1.6662\n",
      "Epoch [0/10] Batch 308/510               Loss D: 0.1726, loss G: 1.6692\n",
      "Epoch [0/10] Batch 309/510               Loss D: 0.1987, loss G: 1.6505\n",
      "Epoch [0/10] Batch 310/510               Loss D: 0.2639, loss G: 1.6419\n",
      "Epoch [0/10] Batch 311/510               Loss D: 0.1371, loss G: 1.6230\n",
      "Epoch [0/10] Batch 312/510               Loss D: 0.1477, loss G: 1.6075\n",
      "Epoch [0/10] Batch 313/510               Loss D: 0.1684, loss G: 1.6185\n",
      "Epoch [0/10] Batch 314/510               Loss D: 0.1585, loss G: 1.5962\n",
      "Epoch [0/10] Batch 315/510               Loss D: 0.1558, loss G: 1.6088\n",
      "Epoch [0/10] Batch 316/510               Loss D: 0.1521, loss G: 1.6054\n",
      "Epoch [0/10] Batch 317/510               Loss D: 0.1754, loss G: 1.5894\n",
      "Epoch [0/10] Batch 318/510               Loss D: 0.1634, loss G: 1.5997\n",
      "Epoch [0/10] Batch 319/510               Loss D: 0.1579, loss G: 1.6026\n",
      "Epoch [0/10] Batch 320/510               Loss D: 0.1595, loss G: 1.5832\n",
      "Epoch [0/10] Batch 321/510               Loss D: 0.1417, loss G: 1.5893\n",
      "Epoch [0/10] Batch 322/510               Loss D: 0.1583, loss G: 1.6022\n",
      "Epoch [0/10] Batch 323/510               Loss D: 0.1772, loss G: 1.5782\n",
      "Epoch [0/10] Batch 324/510               Loss D: 0.1497, loss G: 1.5723\n",
      "Epoch [0/10] Batch 325/510               Loss D: 0.1883, loss G: 1.5623\n",
      "Epoch [0/10] Batch 326/510               Loss D: 0.3111, loss G: 1.5251\n",
      "Epoch [0/10] Batch 327/510               Loss D: 0.1618, loss G: 1.4815\n",
      "Epoch [0/10] Batch 328/510               Loss D: 0.1660, loss G: 1.4620\n",
      "Epoch [0/10] Batch 329/510               Loss D: 0.1768, loss G: 1.4342\n",
      "Epoch [0/10] Batch 330/510               Loss D: 0.2072, loss G: 1.4099\n",
      "Epoch [0/10] Batch 331/510               Loss D: 0.1947, loss G: 1.3784\n",
      "Epoch [0/10] Batch 332/510               Loss D: 0.1932, loss G: 1.3574\n",
      "Epoch [0/10] Batch 333/510               Loss D: 0.1975, loss G: 1.3319\n",
      "Epoch [0/10] Batch 334/510               Loss D: 0.2169, loss G: 1.3201\n",
      "Epoch [0/10] Batch 335/510               Loss D: 0.2097, loss G: 1.2957\n",
      "Epoch [0/10] Batch 336/510               Loss D: 0.2166, loss G: 1.2805\n",
      "Epoch [0/10] Batch 337/510               Loss D: 0.2833, loss G: 1.2553\n",
      "Epoch [0/10] Batch 338/510               Loss D: 0.2853, loss G: 1.2190\n",
      "Epoch [0/10] Batch 339/510               Loss D: 0.2459, loss G: 1.2009\n",
      "Epoch [0/10] Batch 340/510               Loss D: 0.2619, loss G: 1.1624\n",
      "Epoch [0/10] Batch 341/510               Loss D: 0.3464, loss G: 1.1384\n",
      "Epoch [0/10] Batch 342/510               Loss D: 0.2514, loss G: 1.1074\n",
      "Epoch [0/10] Batch 343/510               Loss D: 0.3165, loss G: 1.0803\n",
      "Epoch [0/10] Batch 344/510               Loss D: 0.2717, loss G: 1.0596\n",
      "Epoch [0/10] Batch 345/510               Loss D: 0.6313, loss G: 1.0220\n",
      "Epoch [0/10] Batch 346/510               Loss D: 0.3834, loss G: 0.9870\n",
      "Epoch [0/10] Batch 347/510               Loss D: 0.3115, loss G: 0.9361\n",
      "Epoch [0/10] Batch 348/510               Loss D: 0.3660, loss G: 0.9143\n",
      "Epoch [0/10] Batch 349/510               Loss D: 0.5366, loss G: 0.8668\n",
      "Epoch [0/10] Batch 350/510               Loss D: 0.4101, loss G: 0.8377\n",
      "Epoch [0/10] Batch 351/510               Loss D: 0.6435, loss G: 0.8019\n",
      "Epoch [0/10] Batch 352/510               Loss D: 0.3711, loss G: 0.7832\n",
      "Epoch [0/10] Batch 353/510               Loss D: 0.4694, loss G: 0.7652\n",
      "Epoch [0/10] Batch 354/510               Loss D: 0.4115, loss G: 0.7562\n",
      "Epoch [0/10] Batch 355/510               Loss D: 0.4616, loss G: 0.7404\n",
      "Epoch [0/10] Batch 356/510               Loss D: 0.3644, loss G: 0.7399\n",
      "Epoch [0/10] Batch 357/510               Loss D: 0.3517, loss G: 0.7673\n",
      "Epoch [0/10] Batch 358/510               Loss D: 0.3635, loss G: 0.7824\n",
      "Epoch [0/10] Batch 359/510               Loss D: 0.3851, loss G: 0.8130\n",
      "Epoch [0/10] Batch 360/510               Loss D: 0.3191, loss G: 0.8384\n",
      "Epoch [0/10] Batch 361/510               Loss D: 0.3362, loss G: 0.8755\n",
      "Epoch [0/10] Batch 362/510               Loss D: 0.3122, loss G: 0.9176\n",
      "Epoch [0/10] Batch 363/510               Loss D: 0.2948, loss G: 0.9667\n",
      "Epoch [0/10] Batch 364/510               Loss D: 0.2795, loss G: 1.0130\n",
      "Epoch [0/10] Batch 365/510               Loss D: 0.2876, loss G: 1.0634\n",
      "Epoch [0/10] Batch 366/510               Loss D: 0.2501, loss G: 1.1174\n",
      "Epoch [0/10] Batch 367/510               Loss D: 0.2911, loss G: 1.1675\n",
      "Epoch [0/10] Batch 368/510               Loss D: 0.2163, loss G: 1.2092\n",
      "Epoch [0/10] Batch 369/510               Loss D: 0.2189, loss G: 1.2557\n",
      "Epoch [0/10] Batch 370/510               Loss D: 0.1877, loss G: 1.2998\n",
      "Epoch [0/10] Batch 371/510               Loss D: 0.1941, loss G: 1.3428\n",
      "Epoch [0/10] Batch 372/510               Loss D: 0.2277, loss G: 1.3865\n",
      "Epoch [0/10] Batch 373/510               Loss D: 0.1769, loss G: 1.4224\n",
      "Epoch [0/10] Batch 374/510               Loss D: 0.1878, loss G: 1.4600\n",
      "Epoch [0/10] Batch 375/510               Loss D: 0.2028, loss G: 1.4703\n",
      "Epoch [0/10] Batch 376/510               Loss D: 0.2662, loss G: 1.4703\n",
      "Epoch [0/10] Batch 377/510               Loss D: 0.2190, loss G: 1.4807\n",
      "Epoch [0/10] Batch 378/510               Loss D: 0.1967, loss G: 1.4874\n",
      "Epoch [0/10] Batch 379/510               Loss D: 0.1625, loss G: 1.4803\n",
      "Epoch [0/10] Batch 380/510               Loss D: 0.1749, loss G: 1.4659\n",
      "Epoch [0/10] Batch 381/510               Loss D: 0.3299, loss G: 1.4540\n",
      "Epoch [0/10] Batch 382/510               Loss D: 0.1690, loss G: 1.4412\n",
      "Epoch [0/10] Batch 383/510               Loss D: 0.1675, loss G: 1.4297\n",
      "Epoch [0/10] Batch 384/510               Loss D: 0.1661, loss G: 1.4153\n",
      "Epoch [0/10] Batch 385/510               Loss D: 0.2129, loss G: 1.4115\n",
      "Epoch [0/10] Batch 386/510               Loss D: 0.1826, loss G: 1.3913\n",
      "Epoch [0/10] Batch 387/510               Loss D: 0.1573, loss G: 1.3970\n",
      "Epoch [0/10] Batch 388/510               Loss D: 0.1798, loss G: 1.3935\n",
      "Epoch [0/10] Batch 389/510               Loss D: 0.1699, loss G: 1.3788\n",
      "Epoch [0/10] Batch 390/510               Loss D: 0.2010, loss G: 1.3817\n",
      "Epoch [0/10] Batch 391/510               Loss D: 0.1842, loss G: 1.3756\n",
      "Epoch [0/10] Batch 392/510               Loss D: 0.1672, loss G: 1.3716\n",
      "Epoch [0/10] Batch 393/510               Loss D: 0.1725, loss G: 1.3717\n",
      "Epoch [0/10] Batch 394/510               Loss D: 0.1799, loss G: 1.3729\n",
      "Epoch [0/10] Batch 395/510               Loss D: 0.1730, loss G: 1.3687\n",
      "Epoch [0/10] Batch 396/510               Loss D: 0.2850, loss G: 1.3506\n",
      "Epoch [0/10] Batch 397/510               Loss D: 0.1720, loss G: 1.3491\n",
      "Epoch [0/10] Batch 398/510               Loss D: 0.1987, loss G: 1.3421\n",
      "Epoch [0/10] Batch 399/510               Loss D: 0.1809, loss G: 1.3223\n",
      "Epoch [0/10] Batch 400/510               Loss D: 0.1935, loss G: 1.3276\n",
      "Epoch [0/10] Batch 401/510               Loss D: 0.1761, loss G: 1.3114\n",
      "Epoch [0/10] Batch 402/510               Loss D: 0.2623, loss G: 1.2881\n",
      "Epoch [0/10] Batch 403/510               Loss D: 0.2798, loss G: 1.2780\n",
      "Epoch [0/10] Batch 404/510               Loss D: 0.3006, loss G: 1.2466\n",
      "Epoch [0/10] Batch 405/510               Loss D: 0.2194, loss G: 1.2256\n",
      "Epoch [0/10] Batch 406/510               Loss D: 0.2639, loss G: 1.1980\n",
      "Epoch [0/10] Batch 407/510               Loss D: 0.2085, loss G: 1.1758\n",
      "Epoch [0/10] Batch 408/510               Loss D: 0.2201, loss G: 1.1491\n",
      "Epoch [0/10] Batch 409/510               Loss D: 0.2348, loss G: 1.1223\n",
      "Epoch [0/10] Batch 410/510               Loss D: 0.2295, loss G: 1.1127\n",
      "Epoch [0/10] Batch 411/510               Loss D: 0.2366, loss G: 1.0858\n",
      "Epoch [0/10] Batch 412/510               Loss D: 0.2386, loss G: 1.0772\n",
      "Epoch [0/10] Batch 413/510               Loss D: 0.3058, loss G: 1.0557\n",
      "Epoch [0/10] Batch 414/510               Loss D: 0.3260, loss G: 1.0333\n",
      "Epoch [0/10] Batch 415/510               Loss D: 0.2529, loss G: 1.0270\n",
      "Epoch [0/10] Batch 416/510               Loss D: 0.2780, loss G: 0.9975\n",
      "Epoch [0/10] Batch 417/510               Loss D: 0.2650, loss G: 0.9838\n",
      "Epoch [0/10] Batch 418/510               Loss D: 0.3204, loss G: 0.9692\n",
      "Epoch [0/10] Batch 419/510               Loss D: 0.2729, loss G: 0.9442\n",
      "Epoch [0/10] Batch 420/510               Loss D: 0.3377, loss G: 0.9406\n",
      "Epoch [0/10] Batch 421/510               Loss D: 0.3006, loss G: 0.9317\n",
      "Epoch [0/10] Batch 422/510               Loss D: 0.3107, loss G: 0.9102\n",
      "Epoch [0/10] Batch 423/510               Loss D: 0.6693, loss G: 0.8907\n",
      "Epoch [0/10] Batch 424/510               Loss D: 0.3356, loss G: 0.8685\n",
      "Epoch [0/10] Batch 425/510               Loss D: 0.3834, loss G: 0.8485\n",
      "Epoch [0/10] Batch 426/510               Loss D: 0.3794, loss G: 0.8303\n",
      "Epoch [0/10] Batch 427/510               Loss D: 0.3322, loss G: 0.8147\n",
      "Epoch [0/10] Batch 428/510               Loss D: 0.3201, loss G: 0.8084\n",
      "Epoch [0/10] Batch 429/510               Loss D: 0.3347, loss G: 0.7996\n",
      "Epoch [0/10] Batch 430/510               Loss D: 0.3492, loss G: 0.7920\n",
      "Epoch [0/10] Batch 431/510               Loss D: 0.3540, loss G: 0.7861\n",
      "Epoch [0/10] Batch 432/510               Loss D: 0.3495, loss G: 0.7908\n",
      "Epoch [0/10] Batch 433/510               Loss D: 0.3351, loss G: 0.7862\n",
      "Epoch [0/10] Batch 434/510               Loss D: 0.3468, loss G: 0.7867\n",
      "Epoch [0/10] Batch 435/510               Loss D: 0.3275, loss G: 0.7909\n",
      "Epoch [0/10] Batch 436/510               Loss D: 0.3686, loss G: 0.7905\n",
      "Epoch [0/10] Batch 437/510               Loss D: 0.5173, loss G: 0.7900\n",
      "Epoch [0/10] Batch 438/510               Loss D: 0.3568, loss G: 0.7843\n",
      "Epoch [0/10] Batch 439/510               Loss D: 0.3586, loss G: 0.7812\n",
      "Epoch [0/10] Batch 440/510               Loss D: 0.3496, loss G: 0.7826\n",
      "Epoch [0/10] Batch 441/510               Loss D: 0.3463, loss G: 0.7911\n",
      "Epoch [0/10] Batch 442/510               Loss D: 0.3708, loss G: 0.7879\n",
      "Epoch [0/10] Batch 443/510               Loss D: 0.3732, loss G: 0.7958\n",
      "Epoch [0/10] Batch 444/510               Loss D: 0.4041, loss G: 0.7834\n",
      "Epoch [0/10] Batch 445/510               Loss D: 0.3677, loss G: 0.7877\n",
      "Epoch [0/10] Batch 446/510               Loss D: 0.4219, loss G: 0.7901\n",
      "Epoch [0/10] Batch 447/510               Loss D: 0.3462, loss G: 0.7907\n",
      "Epoch [0/10] Batch 448/510               Loss D: 0.4097, loss G: 0.7775\n",
      "Epoch [0/10] Batch 449/510               Loss D: 0.3888, loss G: 0.7748\n",
      "Epoch [0/10] Batch 450/510               Loss D: 0.4423, loss G: 0.7792\n",
      "Epoch [0/10] Batch 451/510               Loss D: 0.4595, loss G: 0.7731\n",
      "Epoch [0/10] Batch 452/510               Loss D: 0.3515, loss G: 0.7716\n",
      "Epoch [0/10] Batch 453/510               Loss D: 0.3654, loss G: 0.7734\n",
      "Epoch [0/10] Batch 454/510               Loss D: 0.3496, loss G: 0.7811\n",
      "Epoch [0/10] Batch 455/510               Loss D: 0.3413, loss G: 0.8020\n",
      "Epoch [0/10] Batch 456/510               Loss D: 0.4847, loss G: 0.8128\n",
      "Epoch [0/10] Batch 457/510               Loss D: 0.3689, loss G: 0.8273\n",
      "Epoch [0/10] Batch 458/510               Loss D: 0.4332, loss G: 0.8400\n",
      "Epoch [0/10] Batch 459/510               Loss D: 0.4235, loss G: 0.8549\n",
      "Epoch [0/10] Batch 460/510               Loss D: 0.3860, loss G: 0.8670\n",
      "Epoch [0/10] Batch 461/510               Loss D: 0.3267, loss G: 0.8904\n",
      "Epoch [0/10] Batch 462/510               Loss D: 0.3387, loss G: 0.9102\n",
      "Epoch [0/10] Batch 463/510               Loss D: 0.2962, loss G: 0.9349\n",
      "Epoch [0/10] Batch 464/510               Loss D: 0.4292, loss G: 0.9538\n",
      "Epoch [0/10] Batch 465/510               Loss D: 0.3375, loss G: 0.9714\n",
      "Epoch [0/10] Batch 466/510               Loss D: 0.2931, loss G: 0.9929\n",
      "Epoch [0/10] Batch 467/510               Loss D: 0.3220, loss G: 1.0088\n",
      "Epoch [0/10] Batch 468/510               Loss D: 0.2956, loss G: 1.0304\n",
      "Epoch [0/10] Batch 469/510               Loss D: 0.4360, loss G: 1.0411\n",
      "Epoch [0/10] Batch 470/510               Loss D: 0.2750, loss G: 1.0547\n",
      "Epoch [0/10] Batch 471/510               Loss D: 0.3158, loss G: 1.0670\n",
      "Epoch [0/10] Batch 472/510               Loss D: 0.3111, loss G: 1.0669\n",
      "Epoch [0/10] Batch 473/510               Loss D: 0.2668, loss G: 1.0752\n",
      "Epoch [0/10] Batch 474/510               Loss D: 0.3816, loss G: 1.0741\n",
      "Epoch [0/10] Batch 475/510               Loss D: 0.2756, loss G: 1.0742\n",
      "Epoch [0/10] Batch 476/510               Loss D: 0.2430, loss G: 1.0788\n",
      "Epoch [0/10] Batch 477/510               Loss D: 0.2475, loss G: 1.0898\n",
      "Epoch [0/10] Batch 478/510               Loss D: 0.2856, loss G: 1.0987\n",
      "Epoch [0/10] Batch 479/510               Loss D: 0.2478, loss G: 1.1064\n",
      "Epoch [0/10] Batch 480/510               Loss D: 0.3506, loss G: 1.1138\n",
      "Epoch [0/10] Batch 481/510               Loss D: 0.2434, loss G: 1.1158\n",
      "Epoch [0/10] Batch 482/510               Loss D: 0.2375, loss G: 1.1327\n",
      "Epoch [0/10] Batch 483/510               Loss D: 0.2493, loss G: 1.1403\n",
      "Epoch [0/10] Batch 484/510               Loss D: 0.2679, loss G: 1.1437\n",
      "Epoch [0/10] Batch 485/510               Loss D: 0.2683, loss G: 1.1486\n",
      "Epoch [0/10] Batch 486/510               Loss D: 0.2319, loss G: 1.1469\n",
      "Epoch [0/10] Batch 487/510               Loss D: 0.2229, loss G: 1.1606\n",
      "Epoch [0/10] Batch 488/510               Loss D: 0.2631, loss G: 1.1615\n",
      "Epoch [0/10] Batch 489/510               Loss D: 0.3188, loss G: 1.1536\n",
      "Epoch [0/10] Batch 490/510               Loss D: 0.2324, loss G: 1.1586\n",
      "Epoch [0/10] Batch 491/510               Loss D: 0.2868, loss G: 1.1530\n",
      "Epoch [0/10] Batch 492/510               Loss D: 0.3441, loss G: 1.1358\n",
      "Epoch [0/10] Batch 493/510               Loss D: 0.4729, loss G: 1.0930\n",
      "Epoch [0/10] Batch 494/510               Loss D: 0.3305, loss G: 1.0604\n",
      "Epoch [0/10] Batch 495/510               Loss D: 0.2663, loss G: 1.0292\n",
      "Epoch [0/10] Batch 496/510               Loss D: 0.3550, loss G: 0.9995\n",
      "Epoch [0/10] Batch 497/510               Loss D: 0.3792, loss G: 0.9718\n",
      "Epoch [0/10] Batch 498/510               Loss D: 0.3430, loss G: 0.9432\n",
      "Epoch [0/10] Batch 499/510               Loss D: 0.3513, loss G: 0.9283\n",
      "Epoch [0/10] Batch 500/510               Loss D: 0.4000, loss G: 0.9025\n",
      "Epoch [0/10] Batch 501/510               Loss D: 0.3065, loss G: 0.8923\n",
      "Epoch [0/10] Batch 502/510               Loss D: 0.3106, loss G: 0.8805\n",
      "Epoch [0/10] Batch 503/510               Loss D: 0.3368, loss G: 0.8763\n",
      "Epoch [0/10] Batch 504/510               Loss D: 0.2943, loss G: 0.8873\n",
      "Epoch [0/10] Batch 505/510               Loss D: 0.3555, loss G: 0.8852\n",
      "Epoch [0/10] Batch 506/510               Loss D: 0.3423, loss G: 0.8928\n",
      "Epoch [0/10] Batch 507/510               Loss D: 0.3095, loss G: 0.9032\n",
      "Epoch [0/10] Batch 508/510               Loss D: 0.3013, loss G: 0.9145\n",
      "Epoch [0/10] Batch 509/510               Loss D: 0.5633, loss G: 0.9096\n",
      "Epoch [1/10] Batch 0/510               Loss D: 0.2857, loss G: 0.9163\n",
      "Epoch [1/10] Batch 1/510               Loss D: 0.2932, loss G: 0.9257\n",
      "Epoch [1/10] Batch 2/510               Loss D: 0.2966, loss G: 0.9425\n",
      "Epoch [1/10] Batch 3/510               Loss D: 0.2797, loss G: 0.9633\n",
      "Epoch [1/10] Batch 4/510               Loss D: 0.3730, loss G: 0.9676\n",
      "Epoch [1/10] Batch 5/510               Loss D: 0.2649, loss G: 0.9849\n",
      "Epoch [1/10] Batch 6/510               Loss D: 0.2700, loss G: 1.0050\n",
      "Epoch [1/10] Batch 7/510               Loss D: 0.2665, loss G: 1.0249\n",
      "Epoch [1/10] Batch 8/510               Loss D: 0.2985, loss G: 1.0475\n",
      "Epoch [1/10] Batch 9/510               Loss D: 0.3369, loss G: 1.0538\n",
      "Epoch [1/10] Batch 10/510               Loss D: 0.2683, loss G: 1.0731\n",
      "Epoch [1/10] Batch 11/510               Loss D: 0.2865, loss G: 1.0799\n",
      "Epoch [1/10] Batch 12/510               Loss D: 0.4883, loss G: 1.0714\n",
      "Epoch [1/10] Batch 13/510               Loss D: 0.2478, loss G: 1.0549\n",
      "Epoch [1/10] Batch 14/510               Loss D: 0.2688, loss G: 1.0551\n",
      "Epoch [1/10] Batch 15/510               Loss D: 0.3341, loss G: 1.0418\n",
      "Epoch [1/10] Batch 16/510               Loss D: 0.2542, loss G: 1.0371\n",
      "Epoch [1/10] Batch 17/510               Loss D: 0.3118, loss G: 1.0325\n",
      "Epoch [1/10] Batch 18/510               Loss D: 0.3618, loss G: 1.0204\n",
      "Epoch [1/10] Batch 19/510               Loss D: 0.3853, loss G: 1.0030\n",
      "Epoch [1/10] Batch 20/510               Loss D: 0.3679, loss G: 0.9813\n",
      "Epoch [1/10] Batch 21/510               Loss D: 0.3632, loss G: 0.9592\n",
      "Epoch [1/10] Batch 22/510               Loss D: 0.3159, loss G: 0.9363\n",
      "Epoch [1/10] Batch 23/510               Loss D: 0.2914, loss G: 0.9213\n",
      "Epoch [1/10] Batch 24/510               Loss D: 0.3057, loss G: 0.9199\n",
      "Epoch [1/10] Batch 25/510               Loss D: 0.5326, loss G: 0.8995\n",
      "Epoch [1/10] Batch 26/510               Loss D: 0.3556, loss G: 0.8809\n",
      "Epoch [1/10] Batch 27/510               Loss D: 0.3275, loss G: 0.8777\n",
      "Epoch [1/10] Batch 28/510               Loss D: 0.5401, loss G: 0.8585\n",
      "Epoch [1/10] Batch 29/510               Loss D: 0.7637, loss G: 0.8161\n",
      "Epoch [1/10] Batch 30/510               Loss D: 0.3116, loss G: 0.8006\n",
      "Epoch [1/10] Batch 31/510               Loss D: 0.3334, loss G: 0.7753\n",
      "Epoch [1/10] Batch 32/510               Loss D: 0.4390, loss G: 0.7657\n",
      "Epoch [1/10] Batch 33/510               Loss D: 0.3433, loss G: 0.7562\n",
      "Epoch [1/10] Batch 34/510               Loss D: 0.3409, loss G: 0.7563\n",
      "Epoch [1/10] Batch 35/510               Loss D: 0.3386, loss G: 0.7689\n",
      "Epoch [1/10] Batch 36/510               Loss D: 0.3395, loss G: 0.7907\n",
      "Epoch [1/10] Batch 37/510               Loss D: 0.3379, loss G: 0.8112\n",
      "Epoch [1/10] Batch 38/510               Loss D: 0.3340, loss G: 0.8349\n",
      "Epoch [1/10] Batch 39/510               Loss D: 0.3127, loss G: 0.8628\n",
      "Epoch [1/10] Batch 40/510               Loss D: 0.2984, loss G: 0.9056\n",
      "Epoch [1/10] Batch 41/510               Loss D: 0.4620, loss G: 0.9356\n",
      "Epoch [1/10] Batch 42/510               Loss D: 0.3758, loss G: 0.9497\n",
      "Epoch [1/10] Batch 43/510               Loss D: 0.2777, loss G: 0.9705\n",
      "Epoch [1/10] Batch 44/510               Loss D: 0.2655, loss G: 0.9949\n",
      "Epoch [1/10] Batch 45/510               Loss D: 0.3280, loss G: 1.0139\n",
      "Epoch [1/10] Batch 46/510               Loss D: 0.3054, loss G: 1.0385\n",
      "Epoch [1/10] Batch 47/510               Loss D: 0.2758, loss G: 1.0555\n",
      "Epoch [1/10] Batch 48/510               Loss D: 0.6017, loss G: 1.0448\n",
      "Epoch [1/10] Batch 49/510               Loss D: 0.4989, loss G: 1.0257\n",
      "Epoch [1/10] Batch 50/510               Loss D: 0.2477, loss G: 1.0036\n",
      "Epoch [1/10] Batch 51/510               Loss D: 0.3321, loss G: 0.9908\n",
      "Epoch [1/10] Batch 52/510               Loss D: 0.2687, loss G: 0.9722\n",
      "Epoch [1/10] Batch 53/510               Loss D: 0.2631, loss G: 0.9770\n",
      "Epoch [1/10] Batch 54/510               Loss D: 0.2838, loss G: 0.9820\n",
      "Epoch [1/10] Batch 55/510               Loss D: 0.2807, loss G: 0.9804\n",
      "Epoch [1/10] Batch 56/510               Loss D: 0.3112, loss G: 0.9843\n",
      "Epoch [1/10] Batch 57/510               Loss D: 0.2636, loss G: 1.0017\n",
      "Epoch [1/10] Batch 58/510               Loss D: 0.3196, loss G: 1.0038\n",
      "Epoch [1/10] Batch 59/510               Loss D: 0.2626, loss G: 1.0352\n",
      "Epoch [1/10] Batch 60/510               Loss D: 0.2663, loss G: 1.0484\n",
      "Epoch [1/10] Batch 61/510               Loss D: 0.4522, loss G: 1.0456\n",
      "Epoch [1/10] Batch 62/510               Loss D: 0.2519, loss G: 1.0366\n",
      "Epoch [1/10] Batch 63/510               Loss D: 0.2594, loss G: 1.0419\n",
      "Epoch [1/10] Batch 64/510               Loss D: 0.2998, loss G: 1.0517\n",
      "Epoch [1/10] Batch 65/510               Loss D: 0.2977, loss G: 1.0485\n",
      "Epoch [1/10] Batch 66/510               Loss D: 0.2599, loss G: 1.0474\n",
      "Epoch [1/10] Batch 67/510               Loss D: 0.3937, loss G: 1.0570\n",
      "Epoch [1/10] Batch 68/510               Loss D: 0.5043, loss G: 1.0277\n",
      "Epoch [1/10] Batch 69/510               Loss D: 0.3317, loss G: 1.0217\n",
      "Epoch [1/10] Batch 70/510               Loss D: 0.2566, loss G: 1.0039\n",
      "Epoch [1/10] Batch 71/510               Loss D: 0.3279, loss G: 0.9833\n",
      "Epoch [1/10] Batch 72/510               Loss D: 0.4969, loss G: 0.9571\n",
      "Epoch [1/10] Batch 73/510               Loss D: 0.3128, loss G: 0.9446\n",
      "Epoch [1/10] Batch 74/510               Loss D: 0.3336, loss G: 0.9275\n",
      "Epoch [1/10] Batch 75/510               Loss D: 0.3561, loss G: 0.9170\n",
      "Epoch [1/10] Batch 76/510               Loss D: 0.2869, loss G: 0.9080\n",
      "Epoch [1/10] Batch 77/510               Loss D: 0.3162, loss G: 0.9129\n",
      "Epoch [1/10] Batch 78/510               Loss D: 0.4185, loss G: 0.9054\n",
      "Epoch [1/10] Batch 79/510               Loss D: 0.4906, loss G: 0.8963\n",
      "Epoch [1/10] Batch 80/510               Loss D: 0.2939, loss G: 0.8792\n",
      "Epoch [1/10] Batch 81/510               Loss D: 0.3228, loss G: 0.8918\n",
      "Epoch [1/10] Batch 82/510               Loss D: 0.3428, loss G: 0.8965\n",
      "Epoch [1/10] Batch 83/510               Loss D: 0.4001, loss G: 0.9060\n",
      "Epoch [1/10] Batch 84/510               Loss D: 0.2903, loss G: 0.9100\n",
      "Epoch [1/10] Batch 85/510               Loss D: 0.3232, loss G: 0.9234\n",
      "Epoch [1/10] Batch 86/510               Loss D: 0.2904, loss G: 0.9432\n",
      "Epoch [1/10] Batch 87/510               Loss D: 0.4478, loss G: 0.9498\n",
      "Epoch [1/10] Batch 88/510               Loss D: 0.3910, loss G: 0.9589\n",
      "Epoch [1/10] Batch 89/510               Loss D: 0.2709, loss G: 0.9756\n",
      "Epoch [1/10] Batch 90/510               Loss D: 0.2897, loss G: 0.9856\n",
      "Epoch [1/10] Batch 91/510               Loss D: 0.2965, loss G: 1.0021\n",
      "Epoch [1/10] Batch 92/510               Loss D: 0.2958, loss G: 1.0139\n",
      "Epoch [1/10] Batch 93/510               Loss D: 0.2833, loss G: 1.0384\n",
      "Epoch [1/10] Batch 94/510               Loss D: 0.3405, loss G: 1.0542\n",
      "Epoch [1/10] Batch 95/510               Loss D: 0.2608, loss G: 1.0721\n",
      "Epoch [1/10] Batch 96/510               Loss D: 0.2645, loss G: 1.0967\n",
      "Epoch [1/10] Batch 97/510               Loss D: 0.3615, loss G: 1.1017\n",
      "Epoch [1/10] Batch 98/510               Loss D: 0.2625, loss G: 1.1120\n",
      "Epoch [1/10] Batch 99/510               Loss D: 0.2286, loss G: 1.1301\n",
      "Epoch [1/10] Batch 100/510               Loss D: 0.2755, loss G: 1.1379\n",
      "Epoch [1/10] Batch 101/510               Loss D: 0.3009, loss G: 1.1461\n",
      "Epoch [1/10] Batch 102/510               Loss D: 0.2367, loss G: 1.1573\n",
      "Epoch [1/10] Batch 103/510               Loss D: 0.2974, loss G: 1.1641\n",
      "Epoch [1/10] Batch 104/510               Loss D: 0.7879, loss G: 1.1211\n",
      "Epoch [1/10] Batch 105/510               Loss D: 0.2692, loss G: 1.0991\n",
      "Epoch [1/10] Batch 106/510               Loss D: 0.3261, loss G: 1.0518\n",
      "Epoch [1/10] Batch 107/510               Loss D: 0.2650, loss G: 1.0411\n",
      "Epoch [1/10] Batch 108/510               Loss D: 0.2818, loss G: 1.0337\n",
      "Epoch [1/10] Batch 109/510               Loss D: 0.5555, loss G: 0.9836\n",
      "Epoch [1/10] Batch 110/510               Loss D: 0.2764, loss G: 0.9623\n",
      "Epoch [1/10] Batch 111/510               Loss D: 0.3043, loss G: 0.9496\n",
      "Epoch [1/10] Batch 112/510               Loss D: 0.3122, loss G: 0.9306\n",
      "Epoch [1/10] Batch 113/510               Loss D: 0.2870, loss G: 0.9237\n",
      "Epoch [1/10] Batch 114/510               Loss D: 0.3110, loss G: 0.9392\n",
      "Epoch [1/10] Batch 115/510               Loss D: 0.3070, loss G: 0.9549\n",
      "Epoch [1/10] Batch 116/510               Loss D: 0.3947, loss G: 0.9648\n",
      "Epoch [1/10] Batch 117/510               Loss D: 0.3535, loss G: 0.9806\n",
      "Epoch [1/10] Batch 118/510               Loss D: 0.2681, loss G: 1.0100\n",
      "Epoch [1/10] Batch 119/510               Loss D: 0.2711, loss G: 1.0362\n",
      "Epoch [1/10] Batch 120/510               Loss D: 0.2989, loss G: 1.0733\n",
      "Epoch [1/10] Batch 121/510               Loss D: 0.2773, loss G: 1.1114\n",
      "Epoch [1/10] Batch 122/510               Loss D: 0.3236, loss G: 1.1426\n",
      "Epoch [1/10] Batch 123/510               Loss D: 0.2995, loss G: 1.1616\n",
      "Epoch [1/10] Batch 124/510               Loss D: 0.2376, loss G: 1.2248\n",
      "Epoch [1/10] Batch 125/510               Loss D: 0.2698, loss G: 1.2445\n",
      "Epoch [1/10] Batch 126/510               Loss D: 0.2035, loss G: 1.2878\n",
      "Epoch [1/10] Batch 127/510               Loss D: 0.4662, loss G: 1.3036\n",
      "Epoch [1/10] Batch 128/510               Loss D: 0.2119, loss G: 1.2991\n",
      "Epoch [1/10] Batch 129/510               Loss D: 0.2074, loss G: 1.2941\n",
      "Epoch [1/10] Batch 130/510               Loss D: 0.4972, loss G: 1.2997\n",
      "Epoch [1/10] Batch 131/510               Loss D: 0.3573, loss G: 1.2849\n",
      "Epoch [1/10] Batch 132/510               Loss D: 0.2061, loss G: 1.2607\n",
      "Epoch [1/10] Batch 133/510               Loss D: 0.2600, loss G: 1.2547\n",
      "Epoch [1/10] Batch 134/510               Loss D: 0.2890, loss G: 1.2281\n",
      "Epoch [1/10] Batch 135/510               Loss D: 0.2981, loss G: 1.2101\n",
      "Epoch [1/10] Batch 136/510               Loss D: 0.2572, loss G: 1.1954\n",
      "Epoch [1/10] Batch 137/510               Loss D: 0.2950, loss G: 1.1775\n",
      "Epoch [1/10] Batch 138/510               Loss D: 0.2493, loss G: 1.1693\n",
      "Epoch [1/10] Batch 139/510               Loss D: 0.2314, loss G: 1.1671\n",
      "Epoch [1/10] Batch 140/510               Loss D: 0.2244, loss G: 1.1791\n",
      "Epoch [1/10] Batch 141/510               Loss D: 0.2549, loss G: 1.1845\n",
      "Epoch [1/10] Batch 142/510               Loss D: 0.2203, loss G: 1.2113\n",
      "Epoch [1/10] Batch 143/510               Loss D: 0.2346, loss G: 1.2300\n",
      "Epoch [1/10] Batch 144/510               Loss D: 0.4351, loss G: 1.2224\n",
      "Epoch [1/10] Batch 145/510               Loss D: 0.2292, loss G: 1.2022\n",
      "Epoch [1/10] Batch 146/510               Loss D: 0.5898, loss G: 1.1770\n",
      "Epoch [1/10] Batch 147/510               Loss D: 0.2379, loss G: 1.1595\n",
      "Epoch [1/10] Batch 148/510               Loss D: 0.2415, loss G: 1.1419\n",
      "Epoch [1/10] Batch 149/510               Loss D: 0.2535, loss G: 1.1433\n",
      "Epoch [1/10] Batch 150/510               Loss D: 0.2461, loss G: 1.1536\n",
      "Epoch [1/10] Batch 151/510               Loss D: 0.2284, loss G: 1.1769\n",
      "Epoch [1/10] Batch 152/510               Loss D: 0.2630, loss G: 1.1984\n",
      "Epoch [1/10] Batch 153/510               Loss D: 0.2194, loss G: 1.2162\n",
      "Epoch [1/10] Batch 154/510               Loss D: 0.2115, loss G: 1.2651\n",
      "Epoch [1/10] Batch 155/510               Loss D: 0.3203, loss G: 1.2958\n",
      "Epoch [1/10] Batch 156/510               Loss D: 0.2081, loss G: 1.3118\n",
      "Epoch [1/10] Batch 157/510               Loss D: 0.2270, loss G: 1.3605\n",
      "Epoch [1/10] Batch 158/510               Loss D: 0.2187, loss G: 1.4039\n",
      "Epoch [1/10] Batch 159/510               Loss D: 0.2568, loss G: 1.4312\n",
      "Epoch [1/10] Batch 160/510               Loss D: 0.1826, loss G: 1.4483\n",
      "Epoch [1/10] Batch 161/510               Loss D: 0.4710, loss G: 1.4416\n",
      "Epoch [1/10] Batch 162/510               Loss D: 0.2363, loss G: 1.4375\n",
      "Epoch [1/10] Batch 163/510               Loss D: 0.1652, loss G: 1.4404\n",
      "Epoch [1/10] Batch 164/510               Loss D: 0.2243, loss G: 1.4536\n",
      "Epoch [1/10] Batch 165/510               Loss D: 0.2514, loss G: 1.4644\n",
      "Epoch [1/10] Batch 166/510               Loss D: 0.1629, loss G: 1.4547\n",
      "Epoch [1/10] Batch 167/510               Loss D: 0.1986, loss G: 1.4746\n",
      "Epoch [1/10] Batch 168/510               Loss D: 0.1738, loss G: 1.4672\n",
      "Epoch [1/10] Batch 169/510               Loss D: 0.1674, loss G: 1.5032\n",
      "Epoch [1/10] Batch 170/510               Loss D: 0.1670, loss G: 1.5373\n",
      "Epoch [1/10] Batch 171/510               Loss D: 0.1616, loss G: 1.5759\n",
      "Epoch [1/10] Batch 172/510               Loss D: 0.1866, loss G: 1.6015\n",
      "Epoch [1/10] Batch 173/510               Loss D: 0.1502, loss G: 1.6166\n",
      "Epoch [1/10] Batch 174/510               Loss D: 0.1410, loss G: 1.6565\n",
      "Epoch [1/10] Batch 175/510               Loss D: 0.2903, loss G: 1.6691\n",
      "Epoch [1/10] Batch 176/510               Loss D: 0.1339, loss G: 1.6833\n",
      "Epoch [1/10] Batch 177/510               Loss D: 0.1777, loss G: 1.6835\n",
      "Epoch [1/10] Batch 178/510               Loss D: 0.1468, loss G: 1.7048\n",
      "Epoch [1/10] Batch 179/510               Loss D: 0.1350, loss G: 1.7110\n",
      "Epoch [1/10] Batch 180/510               Loss D: 0.1565, loss G: 1.7287\n",
      "Epoch [1/10] Batch 181/510               Loss D: 0.1379, loss G: 1.7405\n",
      "Epoch [1/10] Batch 182/510               Loss D: 0.1687, loss G: 1.7321\n",
      "Epoch [1/10] Batch 183/510               Loss D: 0.1460, loss G: 1.7167\n",
      "Epoch [1/10] Batch 184/510               Loss D: 0.1837, loss G: 1.7164\n",
      "Epoch [1/10] Batch 185/510               Loss D: 0.1552, loss G: 1.7085\n",
      "Epoch [1/10] Batch 186/510               Loss D: 0.1422, loss G: 1.7002\n",
      "Epoch [1/10] Batch 187/510               Loss D: 0.1830, loss G: 1.7009\n",
      "Epoch [1/10] Batch 188/510               Loss D: 0.2576, loss G: 1.6751\n",
      "Epoch [1/10] Batch 189/510               Loss D: 0.3046, loss G: 1.6093\n",
      "Epoch [1/10] Batch 190/510               Loss D: 0.6445, loss G: 1.5417\n",
      "Epoch [1/10] Batch 191/510               Loss D: 0.1804, loss G: 1.4590\n",
      "Epoch [1/10] Batch 192/510               Loss D: 0.3803, loss G: 1.3912\n",
      "Epoch [1/10] Batch 193/510               Loss D: 0.2444, loss G: 1.3216\n",
      "Epoch [1/10] Batch 194/510               Loss D: 0.1842, loss G: 1.2830\n",
      "Epoch [1/10] Batch 195/510               Loss D: 0.2142, loss G: 1.2399\n",
      "Epoch [1/10] Batch 196/510               Loss D: 0.3440, loss G: 1.1910\n",
      "Epoch [1/10] Batch 197/510               Loss D: 0.2179, loss G: 1.1776\n",
      "Epoch [1/10] Batch 198/510               Loss D: 0.2268, loss G: 1.1743\n",
      "Epoch [1/10] Batch 199/510               Loss D: 0.3606, loss G: 1.1581\n",
      "Epoch [1/10] Batch 200/510               Loss D: 0.2342, loss G: 1.1642\n",
      "Epoch [1/10] Batch 201/510               Loss D: 0.2718, loss G: 1.1625\n",
      "Epoch [1/10] Batch 202/510               Loss D: 0.2389, loss G: 1.1772\n",
      "Epoch [1/10] Batch 203/510               Loss D: 0.3015, loss G: 1.2032\n",
      "Epoch [1/10] Batch 204/510               Loss D: 0.2207, loss G: 1.2315\n",
      "Epoch [1/10] Batch 205/510               Loss D: 0.2531, loss G: 1.2422\n",
      "Epoch [1/10] Batch 206/510               Loss D: 0.2203, loss G: 1.2933\n",
      "Epoch [1/10] Batch 207/510               Loss D: 0.1934, loss G: 1.3450\n",
      "Epoch [1/10] Batch 208/510               Loss D: 0.2011, loss G: 1.3705\n",
      "Epoch [1/10] Batch 209/510               Loss D: 0.2578, loss G: 1.4074\n",
      "Epoch [1/10] Batch 210/510               Loss D: 0.3388, loss G: 1.4197\n",
      "Epoch [1/10] Batch 211/510               Loss D: 0.1926, loss G: 1.4439\n",
      "Epoch [1/10] Batch 212/510               Loss D: 0.1935, loss G: 1.4559\n",
      "Epoch [1/10] Batch 213/510               Loss D: 0.1647, loss G: 1.4959\n",
      "Epoch [1/10] Batch 214/510               Loss D: 0.2362, loss G: 1.4963\n",
      "Epoch [1/10] Batch 215/510               Loss D: 0.1583, loss G: 1.5114\n",
      "Epoch [1/10] Batch 216/510               Loss D: 0.3434, loss G: 1.4987\n",
      "Epoch [1/10] Batch 217/510               Loss D: 0.1618, loss G: 1.4971\n",
      "Epoch [1/10] Batch 218/510               Loss D: 0.1729, loss G: 1.5030\n",
      "Epoch [1/10] Batch 219/510               Loss D: 0.1577, loss G: 1.5070\n",
      "Epoch [1/10] Batch 220/510               Loss D: 0.2425, loss G: 1.5316\n",
      "Epoch [1/10] Batch 221/510               Loss D: 0.1789, loss G: 1.5377\n",
      "Epoch [1/10] Batch 222/510               Loss D: 0.2927, loss G: 1.5133\n",
      "Epoch [1/10] Batch 223/510               Loss D: 0.1527, loss G: 1.5047\n",
      "Epoch [1/10] Batch 224/510               Loss D: 0.1785, loss G: 1.5131\n",
      "Epoch [1/10] Batch 225/510               Loss D: 0.1654, loss G: 1.5146\n",
      "Epoch [1/10] Batch 226/510               Loss D: 0.1848, loss G: 1.5042\n",
      "Epoch [1/10] Batch 227/510               Loss D: 0.1722, loss G: 1.5161\n",
      "Epoch [1/10] Batch 228/510               Loss D: 0.2006, loss G: 1.5239\n",
      "Epoch [1/10] Batch 229/510               Loss D: 0.1562, loss G: 1.5253\n",
      "Epoch [1/10] Batch 230/510               Loss D: 0.1772, loss G: 1.5245\n",
      "Epoch [1/10] Batch 231/510               Loss D: 0.1779, loss G: 1.5087\n",
      "Epoch [1/10] Batch 232/510               Loss D: 0.2612, loss G: 1.5129\n",
      "Epoch [1/10] Batch 233/510               Loss D: 0.3834, loss G: 1.4819\n",
      "Epoch [1/10] Batch 234/510               Loss D: 0.1711, loss G: 1.4553\n",
      "Epoch [1/10] Batch 235/510               Loss D: 0.2087, loss G: 1.4471\n",
      "Epoch [1/10] Batch 236/510               Loss D: 0.1604, loss G: 1.4509\n",
      "Epoch [1/10] Batch 237/510               Loss D: 0.1656, loss G: 1.4372\n",
      "Epoch [1/10] Batch 238/510               Loss D: 0.2694, loss G: 1.4139\n",
      "Epoch [1/10] Batch 239/510               Loss D: 0.2279, loss G: 1.3906\n",
      "Epoch [1/10] Batch 240/510               Loss D: 0.2198, loss G: 1.3720\n",
      "Epoch [1/10] Batch 241/510               Loss D: 0.1802, loss G: 1.3673\n",
      "Epoch [1/10] Batch 242/510               Loss D: 0.1839, loss G: 1.3698\n",
      "Epoch [1/10] Batch 243/510               Loss D: 0.2492, loss G: 1.3688\n",
      "Epoch [1/10] Batch 244/510               Loss D: 0.3604, loss G: 1.3421\n",
      "Epoch [1/10] Batch 245/510               Loss D: 0.2135, loss G: 1.3155\n",
      "Epoch [1/10] Batch 246/510               Loss D: 0.2245, loss G: 1.2992\n",
      "Epoch [1/10] Batch 247/510               Loss D: 0.1970, loss G: 1.2820\n",
      "Epoch [1/10] Batch 248/510               Loss D: 0.3688, loss G: 1.2719\n",
      "Epoch [1/10] Batch 249/510               Loss D: 0.2003, loss G: 1.2457\n",
      "Epoch [1/10] Batch 250/510               Loss D: 0.2081, loss G: 1.2439\n",
      "Epoch [1/10] Batch 251/510               Loss D: 0.2021, loss G: 1.2360\n",
      "Epoch [1/10] Batch 252/510               Loss D: 0.2066, loss G: 1.2458\n",
      "Epoch [1/10] Batch 253/510               Loss D: 0.3203, loss G: 1.2448\n",
      "Epoch [1/10] Batch 254/510               Loss D: 0.2203, loss G: 1.2485\n",
      "Epoch [1/10] Batch 255/510               Loss D: 0.1965, loss G: 1.2691\n",
      "Epoch [1/10] Batch 256/510               Loss D: 0.1952, loss G: 1.2926\n",
      "Epoch [1/10] Batch 257/510               Loss D: 0.2004, loss G: 1.3193\n",
      "Epoch [1/10] Batch 258/510               Loss D: 0.1991, loss G: 1.3553\n",
      "Epoch [1/10] Batch 259/510               Loss D: 0.2487, loss G: 1.3615\n",
      "Epoch [1/10] Batch 260/510               Loss D: 0.1936, loss G: 1.4031\n",
      "Epoch [1/10] Batch 261/510               Loss D: 0.1646, loss G: 1.4230\n",
      "Epoch [1/10] Batch 262/510               Loss D: 0.1872, loss G: 1.4570\n",
      "Epoch [1/10] Batch 263/510               Loss D: 0.2697, loss G: 1.4647\n",
      "Epoch [1/10] Batch 264/510               Loss D: 0.2470, loss G: 1.4715\n",
      "Epoch [1/10] Batch 265/510               Loss D: 0.1592, loss G: 1.4914\n",
      "Epoch [1/10] Batch 266/510               Loss D: 0.1676, loss G: 1.4986\n",
      "Epoch [1/10] Batch 267/510               Loss D: 0.3007, loss G: 1.5024\n",
      "Epoch [1/10] Batch 268/510               Loss D: 0.2059, loss G: 1.4840\n",
      "Epoch [1/10] Batch 269/510               Loss D: 0.1840, loss G: 1.4693\n",
      "Epoch [1/10] Batch 270/510               Loss D: 0.1693, loss G: 1.4690\n",
      "Epoch [1/10] Batch 271/510               Loss D: 0.1471, loss G: 1.4799\n",
      "Epoch [1/10] Batch 272/510               Loss D: 0.5572, loss G: 1.4451\n",
      "Epoch [1/10] Batch 273/510               Loss D: 0.3470, loss G: 1.3989\n",
      "Epoch [1/10] Batch 274/510               Loss D: 0.1645, loss G: 1.3674\n",
      "Epoch [1/10] Batch 275/510               Loss D: 0.1774, loss G: 1.3409\n",
      "Epoch [1/10] Batch 276/510               Loss D: 0.2192, loss G: 1.3343\n",
      "Epoch [1/10] Batch 277/510               Loss D: 0.2748, loss G: 1.3017\n",
      "Epoch [1/10] Batch 278/510               Loss D: 0.1947, loss G: 1.2923\n",
      "Epoch [1/10] Batch 279/510               Loss D: 0.2414, loss G: 1.2769\n",
      "Epoch [1/10] Batch 280/510               Loss D: 0.1924, loss G: 1.2795\n",
      "Epoch [1/10] Batch 281/510               Loss D: 0.1879, loss G: 1.2820\n",
      "Epoch [1/10] Batch 282/510               Loss D: 0.2703, loss G: 1.2735\n",
      "Epoch [1/10] Batch 283/510               Loss D: 0.2343, loss G: 1.2774\n",
      "Epoch [1/10] Batch 284/510               Loss D: 0.1871, loss G: 1.2887\n",
      "Epoch [1/10] Batch 285/510               Loss D: 0.1830, loss G: 1.3103\n",
      "Epoch [1/10] Batch 286/510               Loss D: 0.1853, loss G: 1.3214\n",
      "Epoch [1/10] Batch 287/510               Loss D: 0.1799, loss G: 1.3374\n",
      "Epoch [1/10] Batch 288/510               Loss D: 0.3560, loss G: 1.3466\n",
      "Epoch [1/10] Batch 289/510               Loss D: 0.1970, loss G: 1.3431\n",
      "Epoch [1/10] Batch 290/510               Loss D: 0.2803, loss G: 1.3449\n",
      "Epoch [1/10] Batch 291/510               Loss D: 0.1737, loss G: 1.3599\n",
      "Epoch [1/10] Batch 292/510               Loss D: 0.1753, loss G: 1.3562\n",
      "Epoch [1/10] Batch 293/510               Loss D: 0.2041, loss G: 1.3711\n",
      "Epoch [1/10] Batch 294/510               Loss D: 0.1712, loss G: 1.4165\n",
      "Epoch [1/10] Batch 295/510               Loss D: 0.1698, loss G: 1.4339\n",
      "Epoch [1/10] Batch 296/510               Loss D: 0.1659, loss G: 1.4807\n",
      "Epoch [1/10] Batch 297/510               Loss D: 0.1676, loss G: 1.4923\n",
      "Epoch [1/10] Batch 298/510               Loss D: 0.4439, loss G: 1.4972\n",
      "Epoch [1/10] Batch 299/510               Loss D: 0.5708, loss G: 1.4292\n",
      "Epoch [1/10] Batch 300/510               Loss D: 0.1669, loss G: 1.3699\n",
      "Epoch [1/10] Batch 301/510               Loss D: 0.1757, loss G: 1.3420\n",
      "Epoch [1/10] Batch 302/510               Loss D: 0.2578, loss G: 1.3207\n",
      "Epoch [1/10] Batch 303/510               Loss D: 0.2075, loss G: 1.2911\n",
      "Epoch [1/10] Batch 304/510               Loss D: 0.2409, loss G: 1.2794\n",
      "Epoch [1/10] Batch 305/510               Loss D: 0.1989, loss G: 1.2632\n",
      "Epoch [1/10] Batch 306/510               Loss D: 0.2110, loss G: 1.2880\n",
      "Epoch [1/10] Batch 307/510               Loss D: 0.3121, loss G: 1.2870\n",
      "Epoch [1/10] Batch 308/510               Loss D: 0.1895, loss G: 1.2913\n",
      "Epoch [1/10] Batch 309/510               Loss D: 0.2446, loss G: 1.3192\n",
      "Epoch [1/10] Batch 310/510               Loss D: 0.2446, loss G: 1.3407\n",
      "Epoch [1/10] Batch 311/510               Loss D: 0.3619, loss G: 1.3443\n",
      "Epoch [1/10] Batch 312/510               Loss D: 0.1841, loss G: 1.3540\n",
      "Epoch [1/10] Batch 313/510               Loss D: 0.1800, loss G: 1.3811\n",
      "Epoch [1/10] Batch 314/510               Loss D: 0.1937, loss G: 1.4133\n",
      "Epoch [1/10] Batch 315/510               Loss D: 0.2037, loss G: 1.4658\n",
      "Epoch [1/10] Batch 316/510               Loss D: 0.1589, loss G: 1.4987\n",
      "Epoch [1/10] Batch 317/510               Loss D: 0.2607, loss G: 1.5352\n",
      "Epoch [1/10] Batch 318/510               Loss D: 0.1705, loss G: 1.5726\n",
      "Epoch [1/10] Batch 319/510               Loss D: 0.1540, loss G: 1.6100\n",
      "Epoch [1/10] Batch 320/510               Loss D: 0.1638, loss G: 1.6473\n",
      "Epoch [1/10] Batch 321/510               Loss D: 0.1558, loss G: 1.6925\n",
      "Epoch [1/10] Batch 322/510               Loss D: 0.3752, loss G: 1.6814\n",
      "Epoch [1/10] Batch 323/510               Loss D: 0.1268, loss G: 1.6961\n",
      "Epoch [1/10] Batch 324/510               Loss D: 0.2867, loss G: 1.6637\n",
      "Epoch [1/10] Batch 325/510               Loss D: 0.1263, loss G: 1.6630\n",
      "Epoch [1/10] Batch 326/510               Loss D: 0.1310, loss G: 1.6619\n",
      "Epoch [1/10] Batch 327/510               Loss D: 0.1961, loss G: 1.6703\n",
      "Epoch [1/10] Batch 328/510               Loss D: 0.1581, loss G: 1.6437\n",
      "Epoch [1/10] Batch 329/510               Loss D: 0.2367, loss G: 1.6390\n",
      "Epoch [1/10] Batch 330/510               Loss D: 0.1518, loss G: 1.6378\n",
      "Epoch [1/10] Batch 331/510               Loss D: 0.3092, loss G: 1.5828\n",
      "Epoch [1/10] Batch 332/510               Loss D: 0.1435, loss G: 1.5472\n",
      "Epoch [1/10] Batch 333/510               Loss D: 0.7478, loss G: 1.4585\n",
      "Epoch [1/10] Batch 334/510               Loss D: 0.2004, loss G: 1.3644\n",
      "Epoch [1/10] Batch 335/510               Loss D: 0.1733, loss G: 1.3121\n",
      "Epoch [1/10] Batch 336/510               Loss D: 0.2059, loss G: 1.2844\n",
      "Epoch [1/10] Batch 337/510               Loss D: 0.2878, loss G: 1.2334\n",
      "Epoch [1/10] Batch 338/510               Loss D: 0.2313, loss G: 1.2145\n",
      "Epoch [1/10] Batch 339/510               Loss D: 0.3423, loss G: 1.1809\n",
      "Epoch [1/10] Batch 340/510               Loss D: 0.5482, loss G: 1.1227\n",
      "Epoch [1/10] Batch 341/510               Loss D: 0.2619, loss G: 1.0681\n",
      "Epoch [1/10] Batch 342/510               Loss D: 0.2444, loss G: 1.0602\n",
      "Epoch [1/10] Batch 343/510               Loss D: 0.2534, loss G: 1.0687\n",
      "Epoch [1/10] Batch 344/510               Loss D: 0.2820, loss G: 1.0787\n",
      "Epoch [1/10] Batch 345/510               Loss D: 0.4630, loss G: 1.0706\n",
      "Epoch [1/10] Batch 346/510               Loss D: 0.2573, loss G: 1.1018\n",
      "Epoch [1/10] Batch 347/510               Loss D: 0.2443, loss G: 1.1475\n",
      "Epoch [1/10] Batch 348/510               Loss D: 0.2499, loss G: 1.1803\n",
      "Epoch [1/10] Batch 349/510               Loss D: 0.2463, loss G: 1.2535\n",
      "Epoch [1/10] Batch 350/510               Loss D: 0.2267, loss G: 1.3174\n",
      "Epoch [1/10] Batch 351/510               Loss D: 0.2275, loss G: 1.3733\n",
      "Epoch [1/10] Batch 352/510               Loss D: 0.1811, loss G: 1.4511\n",
      "Epoch [1/10] Batch 353/510               Loss D: 0.1675, loss G: 1.5456\n",
      "Epoch [1/10] Batch 354/510               Loss D: 0.1990, loss G: 1.6024\n",
      "Epoch [1/10] Batch 355/510               Loss D: 0.1541, loss G: 1.6360\n",
      "Epoch [1/10] Batch 356/510               Loss D: 0.2148, loss G: 1.7074\n",
      "Epoch [1/10] Batch 357/510               Loss D: 0.1340, loss G: 1.7365\n",
      "Epoch [1/10] Batch 358/510               Loss D: 0.5664, loss G: 1.7236\n",
      "Epoch [1/10] Batch 359/510               Loss D: 0.3346, loss G: 1.6767\n",
      "Epoch [1/10] Batch 360/510               Loss D: 0.1476, loss G: 1.6106\n",
      "Epoch [1/10] Batch 361/510               Loss D: 0.1567, loss G: 1.6090\n",
      "Epoch [1/10] Batch 362/510               Loss D: 0.4915, loss G: 1.4857\n",
      "Epoch [1/10] Batch 363/510               Loss D: 0.1696, loss G: 1.4140\n",
      "Epoch [1/10] Batch 364/510               Loss D: 0.2059, loss G: 1.3920\n",
      "Epoch [1/10] Batch 365/510               Loss D: 0.1921, loss G: 1.3472\n",
      "Epoch [1/10] Batch 366/510               Loss D: 0.5237, loss G: 1.2528\n",
      "Epoch [1/10] Batch 367/510               Loss D: 0.2170, loss G: 1.2004\n",
      "Epoch [1/10] Batch 368/510               Loss D: 0.2378, loss G: 1.1754\n",
      "Epoch [1/10] Batch 369/510               Loss D: 0.3970, loss G: 1.1398\n",
      "Epoch [1/10] Batch 370/510               Loss D: 0.2770, loss G: 1.1166\n",
      "Epoch [1/10] Batch 371/510               Loss D: 0.2664, loss G: 1.1066\n",
      "Epoch [1/10] Batch 372/510               Loss D: 0.2505, loss G: 1.1394\n",
      "Epoch [1/10] Batch 373/510               Loss D: 0.3253, loss G: 1.1628\n",
      "Epoch [1/10] Batch 374/510               Loss D: 0.2281, loss G: 1.2314\n",
      "Epoch [1/10] Batch 375/510               Loss D: 0.2364, loss G: 1.2525\n",
      "Epoch [1/10] Batch 376/510               Loss D: 0.2446, loss G: 1.2901\n",
      "Epoch [1/10] Batch 377/510               Loss D: 0.1976, loss G: 1.3809\n",
      "Epoch [1/10] Batch 378/510               Loss D: 0.3433, loss G: 1.3941\n",
      "Epoch [1/10] Batch 379/510               Loss D: 0.1959, loss G: 1.4425\n",
      "Epoch [1/10] Batch 380/510               Loss D: 0.1759, loss G: 1.5217\n",
      "Epoch [1/10] Batch 381/510               Loss D: 0.1718, loss G: 1.5429\n",
      "Epoch [1/10] Batch 382/510               Loss D: 0.1527, loss G: 1.6225\n",
      "Epoch [1/10] Batch 383/510               Loss D: 0.1793, loss G: 1.6509\n",
      "Epoch [1/10] Batch 384/510               Loss D: 0.1519, loss G: 1.7000\n",
      "Epoch [1/10] Batch 385/510               Loss D: 0.1396, loss G: 1.7187\n",
      "Epoch [1/10] Batch 386/510               Loss D: 0.2257, loss G: 1.7331\n",
      "Epoch [1/10] Batch 387/510               Loss D: 0.1269, loss G: 1.7898\n",
      "Epoch [1/10] Batch 388/510               Loss D: 0.1705, loss G: 1.7667\n",
      "Epoch [1/10] Batch 389/510               Loss D: 0.1489, loss G: 1.7419\n",
      "Epoch [1/10] Batch 390/510               Loss D: 0.1233, loss G: 1.7945\n",
      "Epoch [1/10] Batch 391/510               Loss D: 0.6567, loss G: 1.7021\n",
      "Epoch [1/10] Batch 392/510               Loss D: 0.6971, loss G: 1.5456\n",
      "Epoch [1/10] Batch 393/510               Loss D: 0.1718, loss G: 1.4599\n",
      "Epoch [1/10] Batch 394/510               Loss D: 0.2041, loss G: 1.3524\n",
      "Epoch [1/10] Batch 395/510               Loss D: 0.4088, loss G: 1.2628\n",
      "Epoch [1/10] Batch 396/510               Loss D: 0.2553, loss G: 1.1713\n",
      "Epoch [1/10] Batch 397/510               Loss D: 0.2249, loss G: 1.1478\n",
      "Epoch [1/10] Batch 398/510               Loss D: 0.2668, loss G: 1.0918\n",
      "Epoch [1/10] Batch 399/510               Loss D: 0.2471, loss G: 1.1064\n",
      "Epoch [1/10] Batch 400/510               Loss D: 0.2449, loss G: 1.0976\n",
      "Epoch [1/10] Batch 401/510               Loss D: 0.2422, loss G: 1.1291\n",
      "Epoch [1/10] Batch 402/510               Loss D: 0.2470, loss G: 1.1364\n",
      "Epoch [1/10] Batch 403/510               Loss D: 0.3108, loss G: 1.1836\n",
      "Epoch [1/10] Batch 404/510               Loss D: 0.2216, loss G: 1.2255\n",
      "Epoch [1/10] Batch 405/510               Loss D: 0.2170, loss G: 1.2746\n",
      "Epoch [1/10] Batch 406/510               Loss D: 0.2077, loss G: 1.3224\n",
      "Epoch [1/10] Batch 407/510               Loss D: 0.1978, loss G: 1.3602\n",
      "Epoch [1/10] Batch 408/510               Loss D: 0.1785, loss G: 1.4467\n",
      "Epoch [1/10] Batch 409/510               Loss D: 0.1904, loss G: 1.4876\n",
      "Epoch [1/10] Batch 410/510               Loss D: 0.2403, loss G: 1.5556\n",
      "Epoch [1/10] Batch 411/510               Loss D: 0.1608, loss G: 1.6128\n",
      "Epoch [1/10] Batch 412/510               Loss D: 0.1517, loss G: 1.6238\n",
      "Epoch [1/10] Batch 413/510               Loss D: 0.1402, loss G: 1.6807\n",
      "Epoch [1/10] Batch 414/510               Loss D: 0.1324, loss G: 1.7703\n",
      "Epoch [1/10] Batch 415/510               Loss D: 0.1285, loss G: 1.7627\n",
      "Epoch [1/10] Batch 416/510               Loss D: 0.1252, loss G: 1.8330\n",
      "Epoch [1/10] Batch 417/510               Loss D: 0.1296, loss G: 1.8470\n",
      "Epoch [1/10] Batch 418/510               Loss D: 0.1172, loss G: 1.8714\n",
      "Epoch [1/10] Batch 419/510               Loss D: 0.2174, loss G: 1.8726\n",
      "Epoch [1/10] Batch 420/510               Loss D: 0.1369, loss G: 1.8489\n",
      "Epoch [1/10] Batch 421/510               Loss D: 0.1218, loss G: 1.8366\n",
      "Epoch [1/10] Batch 422/510               Loss D: 0.1874, loss G: 1.8375\n",
      "Epoch [1/10] Batch 423/510               Loss D: 0.1439, loss G: 1.8111\n",
      "Epoch [1/10] Batch 424/510               Loss D: 0.1498, loss G: 1.8027\n",
      "Epoch [1/10] Batch 425/510               Loss D: 0.2133, loss G: 1.6855\n",
      "Epoch [1/10] Batch 426/510               Loss D: 0.1416, loss G: 1.7157\n",
      "Epoch [1/10] Batch 427/510               Loss D: 0.1506, loss G: 1.6614\n",
      "Epoch [1/10] Batch 428/510               Loss D: 0.1549, loss G: 1.6385\n",
      "Epoch [1/10] Batch 429/510               Loss D: 0.2042, loss G: 1.6247\n",
      "Epoch [1/10] Batch 430/510               Loss D: 0.1684, loss G: 1.6057\n",
      "Epoch [1/10] Batch 431/510               Loss D: 0.1855, loss G: 1.5874\n",
      "Epoch [1/10] Batch 432/510               Loss D: 0.2020, loss G: 1.5700\n",
      "Epoch [1/10] Batch 433/510               Loss D: 0.9393, loss G: 1.5123\n",
      "Epoch [1/10] Batch 434/510               Loss D: 0.1933, loss G: 1.4335\n",
      "Epoch [1/10] Batch 435/510               Loss D: 0.1946, loss G: 1.3721\n",
      "Epoch [1/10] Batch 436/510               Loss D: 0.2058, loss G: 1.3736\n",
      "Epoch [1/10] Batch 437/510               Loss D: 0.3517, loss G: 1.3556\n",
      "Epoch [1/10] Batch 438/510               Loss D: 0.2911, loss G: 1.3077\n",
      "Epoch [1/10] Batch 439/510               Loss D: 0.2152, loss G: 1.2918\n",
      "Epoch [1/10] Batch 440/510               Loss D: 0.2188, loss G: 1.2831\n",
      "Epoch [1/10] Batch 441/510               Loss D: 0.2110, loss G: 1.3353\n",
      "Epoch [1/10] Batch 442/510               Loss D: 0.2151, loss G: 1.3212\n",
      "Epoch [1/10] Batch 443/510               Loss D: 0.2009, loss G: 1.3843\n",
      "Epoch [1/10] Batch 444/510               Loss D: 0.5824, loss G: 1.3658\n",
      "Epoch [1/10] Batch 445/510               Loss D: 0.1888, loss G: 1.4171\n",
      "Epoch [1/10] Batch 446/510               Loss D: 0.2010, loss G: 1.3849\n",
      "Epoch [1/10] Batch 447/510               Loss D: 0.2153, loss G: 1.4254\n",
      "Epoch [1/10] Batch 448/510               Loss D: 0.1908, loss G: 1.4371\n",
      "Epoch [1/10] Batch 449/510               Loss D: 0.3693, loss G: 1.4672\n",
      "Epoch [1/10] Batch 450/510               Loss D: 0.1939, loss G: 1.4782\n",
      "Epoch [1/10] Batch 451/510               Loss D: 0.3663, loss G: 1.4852\n",
      "Epoch [1/10] Batch 452/510               Loss D: 0.2722, loss G: 1.4866\n",
      "Epoch [1/10] Batch 453/510               Loss D: 0.1869, loss G: 1.4731\n",
      "Epoch [1/10] Batch 454/510               Loss D: 0.1926, loss G: 1.4585\n",
      "Epoch [1/10] Batch 455/510               Loss D: 0.2411, loss G: 1.4824\n",
      "Epoch [1/10] Batch 456/510               Loss D: 0.3500, loss G: 1.4827\n",
      "Epoch [1/10] Batch 457/510               Loss D: 0.1798, loss G: 1.4773\n",
      "Epoch [1/10] Batch 458/510               Loss D: 0.1854, loss G: 1.5188\n",
      "Epoch [1/10] Batch 459/510               Loss D: 0.3404, loss G: 1.4726\n",
      "Epoch [1/10] Batch 460/510               Loss D: 0.7903, loss G: 1.4200\n",
      "Epoch [1/10] Batch 461/510               Loss D: 0.1881, loss G: 1.3781\n",
      "Epoch [1/10] Batch 462/510               Loss D: 0.1919, loss G: 1.3692\n",
      "Epoch [1/10] Batch 463/510               Loss D: 0.2018, loss G: 1.3552\n",
      "Epoch [1/10] Batch 464/510               Loss D: 0.2340, loss G: 1.3578\n",
      "Epoch [1/10] Batch 465/510               Loss D: 0.2543, loss G: 1.3571\n",
      "Epoch [1/10] Batch 466/510               Loss D: 0.2108, loss G: 1.3627\n",
      "Epoch [1/10] Batch 467/510               Loss D: 0.2054, loss G: 1.3737\n",
      "Epoch [1/10] Batch 468/510               Loss D: 0.1948, loss G: 1.4240\n",
      "Epoch [1/10] Batch 469/510               Loss D: 0.4456, loss G: 1.4340\n",
      "Epoch [1/10] Batch 470/510               Loss D: 0.1870, loss G: 1.4543\n",
      "Epoch [1/10] Batch 471/510               Loss D: 0.1904, loss G: 1.4489\n",
      "Epoch [1/10] Batch 472/510               Loss D: 0.1806, loss G: 1.5042\n",
      "Epoch [1/10] Batch 473/510               Loss D: 0.1757, loss G: 1.5383\n",
      "Epoch [1/10] Batch 474/510               Loss D: 0.1666, loss G: 1.6226\n",
      "Epoch [1/10] Batch 475/510               Loss D: 0.1559, loss G: 1.6877\n",
      "Epoch [1/10] Batch 476/510               Loss D: 0.2013, loss G: 1.7443\n",
      "Epoch [1/10] Batch 477/510               Loss D: 0.1656, loss G: 1.7765\n",
      "Epoch [1/10] Batch 478/510               Loss D: 0.1598, loss G: 1.8432\n",
      "Epoch [1/10] Batch 479/510               Loss D: 0.1315, loss G: 1.8533\n",
      "Epoch [1/10] Batch 480/510               Loss D: 0.1200, loss G: 1.9231\n",
      "Epoch [1/10] Batch 481/510               Loss D: 0.1414, loss G: 1.9194\n",
      "Epoch [1/10] Batch 482/510               Loss D: 0.1117, loss G: 2.0007\n",
      "Epoch [1/10] Batch 483/510               Loss D: 0.1322, loss G: 2.0166\n",
      "Epoch [1/10] Batch 484/510               Loss D: 0.1450, loss G: 2.0172\n",
      "Epoch [1/10] Batch 485/510               Loss D: 0.1266, loss G: 2.0205\n",
      "Epoch [1/10] Batch 486/510               Loss D: 0.2024, loss G: 2.0151\n",
      "Epoch [1/10] Batch 487/510               Loss D: 0.1720, loss G: 2.0214\n",
      "Epoch [1/10] Batch 488/510               Loss D: 0.1226, loss G: 2.0267\n",
      "Epoch [1/10] Batch 489/510               Loss D: 0.4760, loss G: 1.9527\n",
      "Epoch [1/10] Batch 490/510               Loss D: 0.1356, loss G: 1.8597\n",
      "Epoch [1/10] Batch 491/510               Loss D: 0.1440, loss G: 1.8039\n",
      "Epoch [1/10] Batch 492/510               Loss D: 1.2250, loss G: 1.6401\n",
      "Epoch [1/10] Batch 493/510               Loss D: 0.2061, loss G: 1.5263\n",
      "Epoch [1/10] Batch 494/510               Loss D: 0.3049, loss G: 1.4199\n",
      "Epoch [1/10] Batch 495/510               Loss D: 0.4054, loss G: 1.3112\n",
      "Epoch [1/10] Batch 496/510               Loss D: 0.5191, loss G: 1.2098\n",
      "Epoch [1/10] Batch 497/510               Loss D: 0.3220, loss G: 1.1287\n",
      "Epoch [1/10] Batch 498/510               Loss D: 0.3364, loss G: 1.1156\n",
      "Epoch [1/10] Batch 499/510               Loss D: 0.2682, loss G: 1.1007\n",
      "Epoch [1/10] Batch 500/510               Loss D: 0.2658, loss G: 1.1121\n",
      "Epoch [1/10] Batch 501/510               Loss D: 0.2487, loss G: 1.1984\n",
      "Epoch [1/10] Batch 502/510               Loss D: 0.2323, loss G: 1.3045\n",
      "Epoch [1/10] Batch 503/510               Loss D: 0.2059, loss G: 1.4378\n",
      "Epoch [1/10] Batch 504/510               Loss D: 0.1770, loss G: 1.5699\n",
      "Epoch [1/10] Batch 505/510               Loss D: 0.1529, loss G: 1.7191\n",
      "Epoch [1/10] Batch 506/510               Loss D: 0.1715, loss G: 1.9039\n",
      "Epoch [1/10] Batch 507/510               Loss D: 0.1148, loss G: 2.0391\n",
      "Epoch [1/10] Batch 508/510               Loss D: 0.0988, loss G: 2.1670\n",
      "Epoch [1/10] Batch 509/510               Loss D: 0.0907, loss G: 2.3175\n",
      "Epoch [2/10] Batch 0/510               Loss D: 0.2006, loss G: 2.4223\n",
      "Epoch [2/10] Batch 1/510               Loss D: 0.0734, loss G: 2.5045\n",
      "Epoch [2/10] Batch 2/510               Loss D: 0.1254, loss G: 2.5000\n",
      "Epoch [2/10] Batch 3/510               Loss D: 0.0628, loss G: 2.5424\n",
      "Epoch [2/10] Batch 4/510               Loss D: 0.1852, loss G: 2.5919\n",
      "Epoch [2/10] Batch 5/510               Loss D: 0.1055, loss G: 2.4846\n",
      "Epoch [2/10] Batch 6/510               Loss D: 0.0685, loss G: 2.4890\n",
      "Epoch [2/10] Batch 7/510               Loss D: 0.0699, loss G: 2.4801\n",
      "Epoch [2/10] Batch 8/510               Loss D: 0.0788, loss G: 2.4578\n",
      "Epoch [2/10] Batch 9/510               Loss D: 0.1334, loss G: 2.3597\n",
      "Epoch [2/10] Batch 10/510               Loss D: 0.0896, loss G: 2.3045\n",
      "Epoch [2/10] Batch 11/510               Loss D: 0.0812, loss G: 2.2737\n",
      "Epoch [2/10] Batch 12/510               Loss D: 0.0910, loss G: 2.2537\n",
      "Epoch [2/10] Batch 13/510               Loss D: 0.4951, loss G: 2.1546\n",
      "Epoch [2/10] Batch 14/510               Loss D: 0.1069, loss G: 2.0547\n",
      "Epoch [2/10] Batch 15/510               Loss D: 0.1524, loss G: 1.9980\n",
      "Epoch [2/10] Batch 16/510               Loss D: 0.1089, loss G: 1.9360\n",
      "Epoch [2/10] Batch 17/510               Loss D: 0.1155, loss G: 1.8642\n",
      "Epoch [2/10] Batch 18/510               Loss D: 0.1185, loss G: 1.8445\n",
      "Epoch [2/10] Batch 19/510               Loss D: 0.1172, loss G: 1.8411\n",
      "Epoch [2/10] Batch 20/510               Loss D: 0.1227, loss G: 1.8150\n",
      "Epoch [2/10] Batch 21/510               Loss D: 0.2110, loss G: 1.8445\n",
      "Epoch [2/10] Batch 22/510               Loss D: 0.1207, loss G: 1.8375\n",
      "Epoch [2/10] Batch 23/510               Loss D: 0.1204, loss G: 1.8332\n",
      "Epoch [2/10] Batch 24/510               Loss D: 0.1791, loss G: 1.8316\n",
      "Epoch [2/10] Batch 25/510               Loss D: 0.1163, loss G: 1.8690\n",
      "Epoch [2/10] Batch 26/510               Loss D: 0.1123, loss G: 1.9509\n",
      "Epoch [2/10] Batch 27/510               Loss D: 0.1792, loss G: 1.9780\n",
      "Epoch [2/10] Batch 28/510               Loss D: 0.1203, loss G: 1.9823\n",
      "Epoch [2/10] Batch 29/510               Loss D: 0.1260, loss G: 2.0176\n",
      "Epoch [2/10] Batch 30/510               Loss D: 0.1107, loss G: 2.0307\n",
      "Epoch [2/10] Batch 31/510               Loss D: 0.1438, loss G: 2.0617\n",
      "Epoch [2/10] Batch 32/510               Loss D: 0.0940, loss G: 2.1373\n",
      "Epoch [2/10] Batch 33/510               Loss D: 0.1512, loss G: 2.1257\n",
      "Epoch [2/10] Batch 34/510               Loss D: 0.1355, loss G: 2.1284\n",
      "Epoch [2/10] Batch 35/510               Loss D: 0.2257, loss G: 2.0954\n",
      "Epoch [2/10] Batch 36/510               Loss D: 0.3998, loss G: 2.0084\n",
      "Epoch [2/10] Batch 37/510               Loss D: 0.1632, loss G: 1.9530\n",
      "Epoch [2/10] Batch 38/510               Loss D: 0.1965, loss G: 1.8613\n",
      "Epoch [2/10] Batch 39/510               Loss D: 0.2967, loss G: 1.7815\n",
      "Epoch [2/10] Batch 40/510               Loss D: 0.3020, loss G: 1.6926\n",
      "Epoch [2/10] Batch 41/510               Loss D: 0.1397, loss G: 1.6043\n",
      "Epoch [2/10] Batch 42/510               Loss D: 0.1643, loss G: 1.5654\n",
      "Epoch [2/10] Batch 43/510               Loss D: 0.1551, loss G: 1.5619\n",
      "Epoch [2/10] Batch 44/510               Loss D: 0.1576, loss G: 1.5848\n",
      "Epoch [2/10] Batch 45/510               Loss D: 0.1918, loss G: 1.5900\n",
      "Epoch [2/10] Batch 46/510               Loss D: 0.1874, loss G: 1.6251\n",
      "Epoch [2/10] Batch 47/510               Loss D: 0.1520, loss G: 1.7048\n",
      "Epoch [2/10] Batch 48/510               Loss D: 0.2124, loss G: 1.7433\n",
      "Epoch [2/10] Batch 49/510               Loss D: 0.1517, loss G: 1.7790\n",
      "Epoch [2/10] Batch 50/510               Loss D: 0.1384, loss G: 1.8648\n",
      "Epoch [2/10] Batch 51/510               Loss D: 0.1387, loss G: 1.9252\n",
      "Epoch [2/10] Batch 52/510               Loss D: 0.1194, loss G: 1.9639\n",
      "Epoch [2/10] Batch 53/510               Loss D: 0.1963, loss G: 2.0203\n",
      "Epoch [2/10] Batch 54/510               Loss D: 0.1102, loss G: 2.0494\n",
      "Epoch [2/10] Batch 55/510               Loss D: 0.1177, loss G: 2.1188\n",
      "Epoch [2/10] Batch 56/510               Loss D: 0.1565, loss G: 2.1334\n",
      "Epoch [2/10] Batch 57/510               Loss D: 0.1796, loss G: 2.1135\n",
      "Epoch [2/10] Batch 58/510               Loss D: 0.0880, loss G: 2.1734\n",
      "Epoch [2/10] Batch 59/510               Loss D: 0.5223, loss G: 2.1082\n",
      "Epoch [2/10] Batch 60/510               Loss D: 0.1295, loss G: 2.0559\n",
      "Epoch [2/10] Batch 61/510               Loss D: 0.1065, loss G: 2.0602\n",
      "Epoch [2/10] Batch 62/510               Loss D: 1.9245, loss G: 1.9107\n",
      "Epoch [2/10] Batch 63/510               Loss D: 0.2543, loss G: 1.8143\n",
      "Epoch [2/10] Batch 64/510               Loss D: 0.1136, loss G: 1.7201\n",
      "Epoch [2/10] Batch 65/510               Loss D: 0.1747, loss G: 1.6662\n",
      "Epoch [2/10] Batch 66/510               Loss D: 0.1488, loss G: 1.6083\n",
      "Epoch [2/10] Batch 67/510               Loss D: 0.1644, loss G: 1.6022\n",
      "Epoch [2/10] Batch 68/510               Loss D: 0.1390, loss G: 1.5868\n",
      "Epoch [2/10] Batch 69/510               Loss D: 0.1491, loss G: 1.5891\n",
      "Epoch [2/10] Batch 70/510               Loss D: 0.1466, loss G: 1.6280\n",
      "Epoch [2/10] Batch 71/510               Loss D: 0.1589, loss G: 1.6741\n",
      "Epoch [2/10] Batch 72/510               Loss D: 0.1448, loss G: 1.7151\n",
      "Epoch [2/10] Batch 73/510               Loss D: 0.1268, loss G: 1.7794\n",
      "Epoch [2/10] Batch 74/510               Loss D: 0.1496, loss G: 1.8761\n",
      "Epoch [2/10] Batch 75/510               Loss D: 0.1069, loss G: 1.9388\n",
      "Epoch [2/10] Batch 76/510               Loss D: 0.0995, loss G: 2.0486\n",
      "Epoch [2/10] Batch 77/510               Loss D: 0.1759, loss G: 2.1073\n",
      "Epoch [2/10] Batch 78/510               Loss D: 0.0895, loss G: 2.1964\n",
      "Epoch [2/10] Batch 79/510               Loss D: 0.0788, loss G: 2.3056\n",
      "Epoch [2/10] Batch 80/510               Loss D: 0.4257, loss G: 2.2991\n",
      "Epoch [2/10] Batch 81/510               Loss D: 0.0791, loss G: 2.3159\n",
      "Epoch [2/10] Batch 82/510               Loss D: 0.0826, loss G: 2.3566\n",
      "Epoch [2/10] Batch 83/510               Loss D: 0.0627, loss G: 2.3808\n",
      "Epoch [2/10] Batch 84/510               Loss D: 0.0632, loss G: 2.4004\n",
      "Epoch [2/10] Batch 85/510               Loss D: 0.0617, loss G: 2.4099\n",
      "Epoch [2/10] Batch 86/510               Loss D: 0.0626, loss G: 2.4176\n",
      "Epoch [2/10] Batch 87/510               Loss D: 0.0718, loss G: 2.3995\n",
      "Epoch [2/10] Batch 88/510               Loss D: 0.0606, loss G: 2.4494\n",
      "Epoch [2/10] Batch 89/510               Loss D: 0.0670, loss G: 2.4215\n",
      "Epoch [2/10] Batch 90/510               Loss D: 0.0697, loss G: 2.4256\n",
      "Epoch [2/10] Batch 91/510               Loss D: 0.0843, loss G: 2.4008\n",
      "Epoch [2/10] Batch 92/510               Loss D: 0.0730, loss G: 2.3987\n",
      "Epoch [2/10] Batch 93/510               Loss D: 0.0647, loss G: 2.3874\n",
      "Epoch [2/10] Batch 94/510               Loss D: 0.0635, loss G: 2.3888\n",
      "Epoch [2/10] Batch 95/510               Loss D: 0.0996, loss G: 2.3441\n",
      "Epoch [2/10] Batch 96/510               Loss D: 0.0828, loss G: 2.3401\n",
      "Epoch [2/10] Batch 97/510               Loss D: 0.1242, loss G: 2.2991\n",
      "Epoch [2/10] Batch 98/510               Loss D: 0.1016, loss G: 2.2409\n",
      "Epoch [2/10] Batch 99/510               Loss D: 0.1769, loss G: 2.1755\n",
      "Epoch [2/10] Batch 100/510               Loss D: 0.0811, loss G: 2.1410\n",
      "Epoch [2/10] Batch 101/510               Loss D: 0.0854, loss G: 2.0968\n",
      "Epoch [2/10] Batch 102/510               Loss D: 0.1050, loss G: 2.0433\n",
      "Epoch [2/10] Batch 103/510               Loss D: 0.1031, loss G: 2.0330\n",
      "Epoch [2/10] Batch 104/510               Loss D: 0.1118, loss G: 2.0030\n",
      "Epoch [2/10] Batch 105/510               Loss D: 0.1297, loss G: 1.9840\n",
      "Epoch [2/10] Batch 106/510               Loss D: 0.1819, loss G: 1.9373\n",
      "Epoch [2/10] Batch 107/510               Loss D: 0.1045, loss G: 1.9352\n",
      "Epoch [2/10] Batch 108/510               Loss D: 0.1555, loss G: 1.9108\n",
      "Epoch [2/10] Batch 109/510               Loss D: 0.1581, loss G: 1.8954\n",
      "Epoch [2/10] Batch 110/510               Loss D: 0.1113, loss G: 1.8582\n",
      "Epoch [2/10] Batch 111/510               Loss D: 0.2189, loss G: 1.8203\n",
      "Epoch [2/10] Batch 112/510               Loss D: 0.1163, loss G: 1.8284\n",
      "Epoch [2/10] Batch 113/510               Loss D: 0.1618, loss G: 1.8136\n",
      "Epoch [2/10] Batch 114/510               Loss D: 0.1225, loss G: 1.7991\n",
      "Epoch [2/10] Batch 115/510               Loss D: 0.7103, loss G: 1.7323\n",
      "Epoch [2/10] Batch 116/510               Loss D: 0.1457, loss G: 1.6731\n",
      "Epoch [2/10] Batch 117/510               Loss D: 0.1446, loss G: 1.6336\n",
      "Epoch [2/10] Batch 118/510               Loss D: 0.1739, loss G: 1.6230\n",
      "Epoch [2/10] Batch 119/510               Loss D: 0.1903, loss G: 1.6171\n",
      "Epoch [2/10] Batch 120/510               Loss D: 0.1470, loss G: 1.6225\n",
      "Epoch [2/10] Batch 121/510               Loss D: 0.1480, loss G: 1.6491\n",
      "Epoch [2/10] Batch 122/510               Loss D: 0.1380, loss G: 1.6993\n",
      "Epoch [2/10] Batch 123/510               Loss D: 0.1273, loss G: 1.7689\n",
      "Epoch [2/10] Batch 124/510               Loss D: 0.1879, loss G: 1.8364\n",
      "Epoch [2/10] Batch 125/510               Loss D: 0.1599, loss G: 1.9192\n",
      "Epoch [2/10] Batch 126/510               Loss D: 0.1020, loss G: 1.9898\n",
      "Epoch [2/10] Batch 127/510               Loss D: 0.1864, loss G: 2.0575\n",
      "Epoch [2/10] Batch 128/510               Loss D: 0.1061, loss G: 2.1189\n",
      "Epoch [2/10] Batch 129/510               Loss D: 0.0849, loss G: 2.1631\n",
      "Epoch [2/10] Batch 130/510               Loss D: 0.0920, loss G: 2.2003\n",
      "Epoch [2/10] Batch 131/510               Loss D: 1.5969, loss G: 2.1097\n",
      "Epoch [2/10] Batch 132/510               Loss D: 0.0909, loss G: 2.0326\n",
      "Epoch [2/10] Batch 133/510               Loss D: 0.0916, loss G: 1.9719\n",
      "Epoch [2/10] Batch 134/510               Loss D: 0.1049, loss G: 1.9327\n",
      "Epoch [2/10] Batch 135/510               Loss D: 0.1347, loss G: 1.8885\n",
      "Epoch [2/10] Batch 136/510               Loss D: 0.1096, loss G: 1.8687\n",
      "Epoch [2/10] Batch 137/510               Loss D: 0.2133, loss G: 1.8357\n",
      "Epoch [2/10] Batch 138/510               Loss D: 0.2949, loss G: 1.7978\n",
      "Epoch [2/10] Batch 139/510               Loss D: 0.5393, loss G: 1.7065\n",
      "Epoch [2/10] Batch 140/510               Loss D: 0.1492, loss G: 1.6272\n",
      "Epoch [2/10] Batch 141/510               Loss D: 0.9383, loss G: 1.4694\n",
      "Epoch [2/10] Batch 142/510               Loss D: 0.2031, loss G: 1.3539\n",
      "Epoch [2/10] Batch 143/510               Loss D: 0.1853, loss G: 1.2885\n",
      "Epoch [2/10] Batch 144/510               Loss D: 0.2058, loss G: 1.2466\n",
      "Epoch [2/10] Batch 145/510               Loss D: 0.2054, loss G: 1.2559\n",
      "Epoch [2/10] Batch 146/510               Loss D: 0.2087, loss G: 1.2679\n",
      "Epoch [2/10] Batch 147/510               Loss D: 0.2137, loss G: 1.3120\n",
      "Epoch [2/10] Batch 148/510               Loss D: 0.1959, loss G: 1.3768\n",
      "Epoch [2/10] Batch 149/510               Loss D: 0.1877, loss G: 1.4356\n",
      "Epoch [2/10] Batch 150/510               Loss D: 0.1696, loss G: 1.5412\n",
      "Epoch [2/10] Batch 151/510               Loss D: 0.1544, loss G: 1.6451\n",
      "Epoch [2/10] Batch 152/510               Loss D: 0.1433, loss G: 1.7388\n",
      "Epoch [2/10] Batch 153/510               Loss D: 0.1272, loss G: 1.8476\n",
      "Epoch [2/10] Batch 154/510               Loss D: 0.1182, loss G: 1.9387\n",
      "Epoch [2/10] Batch 155/510               Loss D: 0.3537, loss G: 1.9807\n",
      "Epoch [2/10] Batch 156/510               Loss D: 0.1078, loss G: 2.0113\n",
      "Epoch [2/10] Batch 157/510               Loss D: 0.1168, loss G: 2.0423\n",
      "Epoch [2/10] Batch 158/510               Loss D: 0.0992, loss G: 2.0409\n",
      "Epoch [2/10] Batch 159/510               Loss D: 0.5524, loss G: 1.9884\n",
      "Epoch [2/10] Batch 160/510               Loss D: 0.1055, loss G: 1.9078\n",
      "Epoch [2/10] Batch 161/510               Loss D: 0.1116, loss G: 1.8578\n",
      "Epoch [2/10] Batch 162/510               Loss D: 0.1967, loss G: 1.8042\n",
      "Epoch [2/10] Batch 163/510               Loss D: 0.1234, loss G: 1.7768\n",
      "Epoch [2/10] Batch 164/510               Loss D: 0.1331, loss G: 1.7322\n",
      "Epoch [2/10] Batch 165/510               Loss D: 0.1359, loss G: 1.7193\n",
      "Epoch [2/10] Batch 166/510               Loss D: 0.1378, loss G: 1.7126\n",
      "Epoch [2/10] Batch 167/510               Loss D: 0.1405, loss G: 1.7098\n",
      "Epoch [2/10] Batch 168/510               Loss D: 0.1423, loss G: 1.7239\n",
      "Epoch [2/10] Batch 169/510               Loss D: 0.1375, loss G: 1.7586\n",
      "Epoch [2/10] Batch 170/510               Loss D: 0.1337, loss G: 1.7896\n",
      "Epoch [2/10] Batch 171/510               Loss D: 0.1348, loss G: 1.8044\n",
      "Epoch [2/10] Batch 172/510               Loss D: 0.3387, loss G: 1.8063\n",
      "Epoch [2/10] Batch 173/510               Loss D: 0.1304, loss G: 1.8435\n",
      "Epoch [2/10] Batch 174/510               Loss D: 0.1281, loss G: 1.8562\n",
      "Epoch [2/10] Batch 175/510               Loss D: 0.2129, loss G: 1.8789\n",
      "Epoch [2/10] Batch 176/510               Loss D: 0.1170, loss G: 1.9313\n",
      "Epoch [2/10] Batch 177/510               Loss D: 0.2334, loss G: 1.9046\n",
      "Epoch [2/10] Batch 178/510               Loss D: 0.1332, loss G: 1.9416\n",
      "Epoch [2/10] Batch 179/510               Loss D: 0.1393, loss G: 1.9393\n",
      "Epoch [2/10] Batch 180/510               Loss D: 0.1254, loss G: 1.9401\n",
      "Epoch [2/10] Batch 181/510               Loss D: 0.1163, loss G: 1.9862\n",
      "Epoch [2/10] Batch 182/510               Loss D: 0.6687, loss G: 1.9640\n",
      "Epoch [2/10] Batch 183/510               Loss D: 0.1403, loss G: 1.9093\n",
      "Epoch [2/10] Batch 184/510               Loss D: 0.1357, loss G: 1.8923\n",
      "Epoch [2/10] Batch 185/510               Loss D: 0.1370, loss G: 1.9114\n",
      "Epoch [2/10] Batch 186/510               Loss D: 0.1324, loss G: 1.8907\n",
      "Epoch [2/10] Batch 187/510               Loss D: 0.1205, loss G: 1.9282\n",
      "Epoch [2/10] Batch 188/510               Loss D: 0.1265, loss G: 1.8946\n",
      "Epoch [2/10] Batch 189/510               Loss D: 0.1252, loss G: 1.9447\n",
      "Epoch [2/10] Batch 190/510               Loss D: 0.1293, loss G: 1.9351\n",
      "Epoch [2/10] Batch 191/510               Loss D: 0.1263, loss G: 1.9778\n",
      "Epoch [2/10] Batch 192/510               Loss D: 0.2422, loss G: 2.0205\n",
      "Epoch [2/10] Batch 193/510               Loss D: 0.1189, loss G: 2.0186\n",
      "Epoch [2/10] Batch 194/510               Loss D: 0.1278, loss G: 2.0750\n",
      "Epoch [2/10] Batch 195/510               Loss D: 0.1101, loss G: 2.1026\n",
      "Epoch [2/10] Batch 196/510               Loss D: 0.2993, loss G: 2.0963\n",
      "Epoch [2/10] Batch 197/510               Loss D: 0.1077, loss G: 2.0866\n",
      "Epoch [2/10] Batch 198/510               Loss D: 0.1181, loss G: 2.1169\n",
      "Epoch [2/10] Batch 199/510               Loss D: 0.1449, loss G: 2.1048\n",
      "Epoch [2/10] Batch 200/510               Loss D: 0.1196, loss G: 2.1055\n",
      "Epoch [2/10] Batch 201/510               Loss D: 0.1076, loss G: 2.0812\n",
      "Epoch [2/10] Batch 202/510               Loss D: 0.1019, loss G: 2.1577\n",
      "Epoch [2/10] Batch 203/510               Loss D: 0.1050, loss G: 2.0903\n",
      "Epoch [2/10] Batch 204/510               Loss D: 0.1183, loss G: 2.1556\n",
      "Epoch [2/10] Batch 205/510               Loss D: 0.0953, loss G: 2.1793\n",
      "Epoch [2/10] Batch 206/510               Loss D: 0.0945, loss G: 2.1921\n",
      "Epoch [2/10] Batch 207/510               Loss D: 0.1030, loss G: 2.2087\n",
      "Epoch [2/10] Batch 208/510               Loss D: 0.0947, loss G: 2.2035\n",
      "Epoch [2/10] Batch 209/510               Loss D: 0.0895, loss G: 2.2518\n",
      "Epoch [2/10] Batch 210/510               Loss D: 0.0875, loss G: 2.2407\n",
      "Epoch [2/10] Batch 211/510               Loss D: 0.1476, loss G: 2.2424\n",
      "Epoch [2/10] Batch 212/510               Loss D: 0.0835, loss G: 2.2843\n",
      "Epoch [2/10] Batch 213/510               Loss D: 0.0965, loss G: 2.2617\n",
      "Epoch [2/10] Batch 214/510               Loss D: 0.0873, loss G: 2.2959\n",
      "Epoch [2/10] Batch 215/510               Loss D: 0.1172, loss G: 2.2586\n",
      "Epoch [2/10] Batch 216/510               Loss D: 0.0825, loss G: 2.2935\n",
      "Epoch [2/10] Batch 217/510               Loss D: 0.1046, loss G: 2.2557\n",
      "Epoch [2/10] Batch 218/510               Loss D: 0.2950, loss G: 2.2391\n",
      "Epoch [2/10] Batch 219/510               Loss D: 0.1111, loss G: 2.1881\n",
      "Epoch [2/10] Batch 220/510               Loss D: 0.1815, loss G: 2.1164\n",
      "Epoch [2/10] Batch 221/510               Loss D: 0.0998, loss G: 2.0668\n",
      "Epoch [2/10] Batch 222/510               Loss D: 0.1031, loss G: 2.0464\n",
      "Epoch [2/10] Batch 223/510               Loss D: 0.2928, loss G: 1.9875\n",
      "Epoch [2/10] Batch 224/510               Loss D: 0.2380, loss G: 1.9589\n",
      "Epoch [2/10] Batch 225/510               Loss D: 0.1240, loss G: 1.8877\n",
      "Epoch [2/10] Batch 226/510               Loss D: 0.1314, loss G: 1.8363\n",
      "Epoch [2/10] Batch 227/510               Loss D: 0.1846, loss G: 1.8202\n",
      "Epoch [2/10] Batch 228/510               Loss D: 0.1363, loss G: 1.8162\n",
      "Epoch [2/10] Batch 229/510               Loss D: 0.1596, loss G: 1.8000\n",
      "Epoch [2/10] Batch 230/510               Loss D: 0.1322, loss G: 1.8668\n",
      "Epoch [2/10] Batch 231/510               Loss D: 0.1308, loss G: 1.9003\n",
      "Epoch [2/10] Batch 232/510               Loss D: 0.1295, loss G: 1.9391\n",
      "Epoch [2/10] Batch 233/510               Loss D: 0.1251, loss G: 1.9548\n",
      "Epoch [2/10] Batch 234/510               Loss D: 0.1138, loss G: 2.0596\n",
      "Epoch [2/10] Batch 235/510               Loss D: 0.2060, loss G: 2.0973\n",
      "Epoch [2/10] Batch 236/510               Loss D: 0.0998, loss G: 2.1799\n",
      "Epoch [2/10] Batch 237/510               Loss D: 0.0965, loss G: 2.2165\n",
      "Epoch [2/10] Batch 238/510               Loss D: 0.0936, loss G: 2.3133\n",
      "Epoch [2/10] Batch 239/510               Loss D: 0.1032, loss G: 2.2986\n",
      "Epoch [2/10] Batch 240/510               Loss D: 0.0912, loss G: 2.3844\n",
      "Epoch [2/10] Batch 241/510               Loss D: 0.0813, loss G: 2.4053\n",
      "Epoch [2/10] Batch 242/510               Loss D: 0.0780, loss G: 2.4486\n",
      "Epoch [2/10] Batch 243/510               Loss D: 0.1117, loss G: 2.4265\n",
      "Epoch [2/10] Batch 244/510               Loss D: 0.0802, loss G: 2.4157\n",
      "Epoch [2/10] Batch 245/510               Loss D: 0.1328, loss G: 2.4126\n",
      "Epoch [2/10] Batch 246/510               Loss D: 0.0883, loss G: 2.4833\n",
      "Epoch [2/10] Batch 247/510               Loss D: 0.0790, loss G: 2.4497\n",
      "Epoch [2/10] Batch 248/510               Loss D: 0.1823, loss G: 2.4256\n",
      "Epoch [2/10] Batch 249/510               Loss D: 0.1027, loss G: 2.3724\n",
      "Epoch [2/10] Batch 250/510               Loss D: 0.1095, loss G: 2.3734\n",
      "Epoch [2/10] Batch 251/510               Loss D: 0.0842, loss G: 2.3432\n",
      "Epoch [2/10] Batch 252/510               Loss D: 0.0873, loss G: 2.3372\n",
      "Epoch [2/10] Batch 253/510               Loss D: 0.0916, loss G: 2.3044\n",
      "Epoch [2/10] Batch 254/510               Loss D: 0.1745, loss G: 2.2481\n",
      "Epoch [2/10] Batch 255/510               Loss D: 0.1199, loss G: 2.2314\n",
      "Epoch [2/10] Batch 256/510               Loss D: 0.0951, loss G: 2.2221\n",
      "Epoch [2/10] Batch 257/510               Loss D: 0.8311, loss G: 2.0618\n",
      "Epoch [2/10] Batch 258/510               Loss D: 0.1900, loss G: 1.9855\n",
      "Epoch [2/10] Batch 259/510               Loss D: 0.1520, loss G: 1.8718\n",
      "Epoch [2/10] Batch 260/510               Loss D: 0.1424, loss G: 1.7821\n",
      "Epoch [2/10] Batch 261/510               Loss D: 0.1454, loss G: 1.7728\n",
      "Epoch [2/10] Batch 262/510               Loss D: 0.1492, loss G: 1.7670\n",
      "Epoch [2/10] Batch 263/510               Loss D: 0.1428, loss G: 1.8465\n",
      "Epoch [2/10] Batch 264/510               Loss D: 1.1269, loss G: 1.7133\n",
      "Epoch [2/10] Batch 265/510               Loss D: 0.1505, loss G: 1.7346\n",
      "Epoch [2/10] Batch 266/510               Loss D: 0.1526, loss G: 1.7044\n",
      "Epoch [2/10] Batch 267/510               Loss D: 0.1498, loss G: 1.7587\n",
      "Epoch [2/10] Batch 268/510               Loss D: 0.1470, loss G: 1.7907\n",
      "Epoch [2/10] Batch 269/510               Loss D: 0.1964, loss G: 1.8280\n",
      "Epoch [2/10] Batch 270/510               Loss D: 0.1366, loss G: 1.8856\n",
      "Epoch [2/10] Batch 271/510               Loss D: 0.1267, loss G: 1.9663\n",
      "Epoch [2/10] Batch 272/510               Loss D: 0.1133, loss G: 2.0820\n",
      "Epoch [2/10] Batch 273/510               Loss D: 0.1097, loss G: 2.1375\n",
      "Epoch [2/10] Batch 274/510               Loss D: 0.1040, loss G: 2.2083\n",
      "Epoch [2/10] Batch 275/510               Loss D: 0.0980, loss G: 2.2594\n",
      "Epoch [2/10] Batch 276/510               Loss D: 0.0944, loss G: 2.3948\n",
      "Epoch [2/10] Batch 277/510               Loss D: 0.0788, loss G: 2.4474\n",
      "Epoch [2/10] Batch 278/510               Loss D: 0.0996, loss G: 2.5281\n",
      "Epoch [2/10] Batch 279/510               Loss D: 0.0875, loss G: 2.5339\n",
      "Epoch [2/10] Batch 280/510               Loss D: 0.0685, loss G: 2.5897\n",
      "Epoch [2/10] Batch 281/510               Loss D: 0.0949, loss G: 2.6377\n",
      "Epoch [2/10] Batch 282/510               Loss D: 0.0844, loss G: 2.6371\n",
      "Epoch [2/10] Batch 283/510               Loss D: 0.0643, loss G: 2.6584\n",
      "Epoch [2/10] Batch 284/510               Loss D: 0.0620, loss G: 2.6604\n",
      "Epoch [2/10] Batch 285/510               Loss D: 0.0598, loss G: 2.6836\n",
      "Epoch [2/10] Batch 286/510               Loss D: 0.0758, loss G: 2.6274\n",
      "Epoch [2/10] Batch 287/510               Loss D: 0.0646, loss G: 2.6456\n",
      "Epoch [2/10] Batch 288/510               Loss D: 0.0768, loss G: 2.5835\n",
      "Epoch [2/10] Batch 289/510               Loss D: 0.0871, loss G: 2.5490\n",
      "Epoch [2/10] Batch 290/510               Loss D: 0.0674, loss G: 2.5406\n",
      "Epoch [2/10] Batch 291/510               Loss D: 0.0732, loss G: 2.5249\n",
      "Epoch [2/10] Batch 292/510               Loss D: 0.0722, loss G: 2.5140\n",
      "Epoch [2/10] Batch 293/510               Loss D: 0.0770, loss G: 2.4654\n",
      "Epoch [2/10] Batch 294/510               Loss D: 0.4119, loss G: 2.4031\n",
      "Epoch [2/10] Batch 295/510               Loss D: 0.0855, loss G: 2.2806\n",
      "Epoch [2/10] Batch 296/510               Loss D: 0.0893, loss G: 2.2733\n",
      "Epoch [2/10] Batch 297/510               Loss D: 0.0915, loss G: 2.1730\n",
      "Epoch [2/10] Batch 298/510               Loss D: 0.2535, loss G: 2.1261\n",
      "Epoch [2/10] Batch 299/510               Loss D: 0.1004, loss G: 2.0425\n",
      "Epoch [2/10] Batch 300/510               Loss D: 0.1253, loss G: 2.0074\n",
      "Epoch [2/10] Batch 301/510               Loss D: 0.1112, loss G: 1.9812\n",
      "Epoch [2/10] Batch 302/510               Loss D: 0.1131, loss G: 2.0108\n",
      "Epoch [2/10] Batch 303/510               Loss D: 0.1116, loss G: 2.0136\n",
      "Epoch [2/10] Batch 304/510               Loss D: 0.1053, loss G: 2.0960\n",
      "Epoch [2/10] Batch 305/510               Loss D: 0.1293, loss G: 2.1067\n",
      "Epoch [2/10] Batch 306/510               Loss D: 0.1028, loss G: 2.1508\n",
      "Epoch [2/10] Batch 307/510               Loss D: 0.1071, loss G: 2.2070\n",
      "Epoch [2/10] Batch 308/510               Loss D: 0.0993, loss G: 2.2929\n",
      "Epoch [2/10] Batch 309/510               Loss D: 0.1831, loss G: 2.2984\n",
      "Epoch [2/10] Batch 310/510               Loss D: 0.1047, loss G: 2.3225\n",
      "Epoch [2/10] Batch 311/510               Loss D: 0.4291, loss G: 2.3555\n",
      "Epoch [2/10] Batch 312/510               Loss D: 0.0835, loss G: 2.3023\n",
      "Epoch [2/10] Batch 313/510               Loss D: 0.1110, loss G: 2.3327\n",
      "Epoch [2/10] Batch 314/510               Loss D: 0.0861, loss G: 2.2976\n",
      "Epoch [2/10] Batch 315/510               Loss D: 0.0954, loss G: 2.2814\n",
      "Epoch [2/10] Batch 316/510               Loss D: 0.1669, loss G: 2.3366\n",
      "Epoch [2/10] Batch 317/510               Loss D: 0.0859, loss G: 2.3344\n",
      "Epoch [2/10] Batch 318/510               Loss D: 0.1040, loss G: 2.3541\n",
      "Epoch [2/10] Batch 319/510               Loss D: 0.0770, loss G: 2.4460\n",
      "Epoch [2/10] Batch 320/510               Loss D: 0.1117, loss G: 2.4125\n",
      "Epoch [2/10] Batch 321/510               Loss D: 0.1052, loss G: 2.3775\n",
      "Epoch [2/10] Batch 322/510               Loss D: 0.0730, loss G: 2.4526\n",
      "Epoch [2/10] Batch 323/510               Loss D: 0.0735, loss G: 2.4538\n",
      "Epoch [2/10] Batch 324/510               Loss D: 0.0715, loss G: 2.4621\n",
      "Epoch [2/10] Batch 325/510               Loss D: 0.1139, loss G: 2.5047\n",
      "Epoch [2/10] Batch 326/510               Loss D: 0.0832, loss G: 2.5379\n",
      "Epoch [2/10] Batch 327/510               Loss D: 0.2200, loss G: 2.4863\n",
      "Epoch [2/10] Batch 328/510               Loss D: 0.0783, loss G: 2.4590\n",
      "Epoch [2/10] Batch 329/510               Loss D: 0.0856, loss G: 2.4679\n",
      "Epoch [2/10] Batch 330/510               Loss D: 0.0724, loss G: 2.4412\n",
      "Epoch [2/10] Batch 331/510               Loss D: 0.0752, loss G: 2.4197\n",
      "Epoch [2/10] Batch 332/510               Loss D: 0.1437, loss G: 2.4632\n",
      "Epoch [2/10] Batch 333/510               Loss D: 0.1051, loss G: 2.4331\n",
      "Epoch [2/10] Batch 334/510               Loss D: 0.0695, loss G: 2.4644\n",
      "Epoch [2/10] Batch 335/510               Loss D: 0.0825, loss G: 2.4648\n",
      "Epoch [2/10] Batch 336/510               Loss D: 0.4140, loss G: 2.4307\n",
      "Epoch [2/10] Batch 337/510               Loss D: 0.1029, loss G: 2.3783\n",
      "Epoch [2/10] Batch 338/510               Loss D: 0.2394, loss G: 2.2846\n",
      "Epoch [2/10] Batch 339/510               Loss D: 0.0791, loss G: 2.2748\n",
      "Epoch [2/10] Batch 340/510               Loss D: 0.1810, loss G: 2.2093\n",
      "Epoch [2/10] Batch 341/510               Loss D: 0.0821, loss G: 2.1662\n",
      "Epoch [2/10] Batch 342/510               Loss D: 0.1072, loss G: 2.1553\n",
      "Epoch [2/10] Batch 343/510               Loss D: 0.0873, loss G: 2.1584\n",
      "Epoch [2/10] Batch 344/510               Loss D: 0.0833, loss G: 2.1879\n",
      "Epoch [2/10] Batch 345/510               Loss D: 0.0812, loss G: 2.2310\n",
      "Epoch [2/10] Batch 346/510               Loss D: 0.0895, loss G: 2.3039\n",
      "Epoch [2/10] Batch 347/510               Loss D: 0.1258, loss G: 2.3827\n",
      "Epoch [2/10] Batch 348/510               Loss D: 0.0689, loss G: 2.4285\n",
      "Epoch [2/10] Batch 349/510               Loss D: 0.0763, loss G: 2.4905\n",
      "Epoch [2/10] Batch 350/510               Loss D: 0.0630, loss G: 2.5547\n",
      "Epoch [2/10] Batch 351/510               Loss D: 0.1426, loss G: 2.5966\n",
      "Epoch [2/10] Batch 352/510               Loss D: 0.0568, loss G: 2.6532\n",
      "Epoch [2/10] Batch 353/510               Loss D: 0.0541, loss G: 2.7015\n",
      "Epoch [2/10] Batch 354/510               Loss D: 0.0686, loss G: 2.7670\n",
      "Epoch [2/10] Batch 355/510               Loss D: 0.0940, loss G: 2.7325\n",
      "Epoch [2/10] Batch 356/510               Loss D: 0.0474, loss G: 2.7673\n",
      "Epoch [2/10] Batch 357/510               Loss D: 0.0460, loss G: 2.8152\n",
      "Epoch [2/10] Batch 358/510               Loss D: 0.0602, loss G: 2.8444\n",
      "Epoch [2/10] Batch 359/510               Loss D: 0.0428, loss G: 2.8855\n",
      "Epoch [2/10] Batch 360/510               Loss D: 0.0846, loss G: 2.8519\n",
      "Epoch [2/10] Batch 361/510               Loss D: 0.0631, loss G: 2.8512\n",
      "Epoch [2/10] Batch 362/510               Loss D: 0.0585, loss G: 2.8531\n",
      "Epoch [2/10] Batch 363/510               Loss D: 0.0564, loss G: 2.8063\n",
      "Epoch [2/10] Batch 364/510               Loss D: 0.0602, loss G: 2.8360\n",
      "Epoch [2/10] Batch 365/510               Loss D: 0.0419, loss G: 2.8612\n",
      "Epoch [2/10] Batch 366/510               Loss D: 0.0500, loss G: 2.8198\n",
      "Epoch [2/10] Batch 367/510               Loss D: 0.0431, loss G: 2.8244\n",
      "Epoch [2/10] Batch 368/510               Loss D: 0.0526, loss G: 2.8610\n",
      "Epoch [2/10] Batch 369/510               Loss D: 0.0815, loss G: 2.8563\n",
      "Epoch [2/10] Batch 370/510               Loss D: 0.0417, loss G: 2.8456\n",
      "Epoch [2/10] Batch 371/510               Loss D: 0.0455, loss G: 2.8259\n",
      "Epoch [2/10] Batch 372/510               Loss D: 0.0457, loss G: 2.8493\n",
      "Epoch [2/10] Batch 373/510               Loss D: 0.0491, loss G: 2.8228\n",
      "Epoch [2/10] Batch 374/510               Loss D: 0.3742, loss G: 2.7484\n",
      "Epoch [2/10] Batch 375/510               Loss D: 0.0472, loss G: 2.6771\n",
      "Epoch [2/10] Batch 376/510               Loss D: 0.0519, loss G: 2.5920\n",
      "Epoch [2/10] Batch 377/510               Loss D: 0.0614, loss G: 2.5462\n",
      "Epoch [2/10] Batch 378/510               Loss D: 0.0605, loss G: 2.5186\n",
      "Epoch [2/10] Batch 379/510               Loss D: 0.0598, loss G: 2.5122\n",
      "Epoch [2/10] Batch 380/510               Loss D: 0.3720, loss G: 2.4205\n",
      "Epoch [2/10] Batch 381/510               Loss D: 0.0695, loss G: 2.3243\n",
      "Epoch [2/10] Batch 382/510               Loss D: 0.1749, loss G: 2.2724\n",
      "Epoch [2/10] Batch 383/510               Loss D: 0.1534, loss G: 2.1915\n",
      "Epoch [2/10] Batch 384/510               Loss D: 0.0854, loss G: 2.1471\n",
      "Epoch [2/10] Batch 385/510               Loss D: 0.1159, loss G: 2.0762\n",
      "Epoch [2/10] Batch 386/510               Loss D: 0.1584, loss G: 2.1090\n",
      "Epoch [2/10] Batch 387/510               Loss D: 0.1046, loss G: 2.0581\n",
      "Epoch [2/10] Batch 388/510               Loss D: 0.0966, loss G: 2.0540\n",
      "Epoch [2/10] Batch 389/510               Loss D: 0.1125, loss G: 2.1016\n",
      "Epoch [2/10] Batch 390/510               Loss D: 0.1019, loss G: 2.1364\n",
      "Epoch [2/10] Batch 391/510               Loss D: 0.0893, loss G: 2.2182\n",
      "Epoch [2/10] Batch 392/510               Loss D: 0.0870, loss G: 2.2878\n",
      "Epoch [2/10] Batch 393/510               Loss D: 0.0815, loss G: 2.3441\n",
      "Epoch [2/10] Batch 394/510               Loss D: 0.1125, loss G: 2.4285\n",
      "Epoch [2/10] Batch 395/510               Loss D: 0.0629, loss G: 2.5401\n",
      "Epoch [2/10] Batch 396/510               Loss D: 0.0610, loss G: 2.5600\n",
      "Epoch [2/10] Batch 397/510               Loss D: 0.0631, loss G: 2.6681\n",
      "Epoch [2/10] Batch 398/510               Loss D: 0.0532, loss G: 2.7144\n",
      "Epoch [2/10] Batch 399/510               Loss D: 0.0627, loss G: 2.8007\n",
      "Epoch [2/10] Batch 400/510               Loss D: 0.0479, loss G: 2.8090\n",
      "Epoch [2/10] Batch 401/510               Loss D: 0.0435, loss G: 2.8754\n",
      "Epoch [2/10] Batch 402/510               Loss D: 0.0438, loss G: 2.9312\n",
      "Epoch [2/10] Batch 403/510               Loss D: 0.0436, loss G: 2.9719\n",
      "Epoch [2/10] Batch 404/510               Loss D: 0.0434, loss G: 2.9781\n",
      "Epoch [2/10] Batch 405/510               Loss D: 0.0365, loss G: 3.0301\n",
      "Epoch [2/10] Batch 406/510               Loss D: 0.0357, loss G: 3.0318\n",
      "Epoch [2/10] Batch 407/510               Loss D: 0.0556, loss G: 3.0675\n",
      "Epoch [2/10] Batch 408/510               Loss D: 0.0364, loss G: 3.0835\n",
      "Epoch [2/10] Batch 409/510               Loss D: 0.0428, loss G: 3.0455\n",
      "Epoch [2/10] Batch 410/510               Loss D: 0.0366, loss G: 3.0534\n",
      "Epoch [2/10] Batch 411/510               Loss D: 0.0657, loss G: 3.0996\n",
      "Epoch [2/10] Batch 412/510               Loss D: 0.0328, loss G: 3.0735\n",
      "Epoch [2/10] Batch 413/510               Loss D: 0.2274, loss G: 3.0170\n",
      "Epoch [2/10] Batch 414/510               Loss D: 0.0342, loss G: 2.9693\n",
      "Epoch [2/10] Batch 415/510               Loss D: 0.1513, loss G: 2.8907\n",
      "Epoch [2/10] Batch 416/510               Loss D: 0.0428, loss G: 2.8184\n",
      "Epoch [2/10] Batch 417/510               Loss D: 0.0438, loss G: 2.7670\n",
      "Epoch [2/10] Batch 418/510               Loss D: 0.0453, loss G: 2.7591\n",
      "Epoch [2/10] Batch 419/510               Loss D: 0.0925, loss G: 2.7105\n",
      "Epoch [2/10] Batch 420/510               Loss D: 0.0947, loss G: 2.6834\n",
      "Epoch [2/10] Batch 421/510               Loss D: 0.0623, loss G: 2.6100\n",
      "Epoch [2/10] Batch 422/510               Loss D: 0.2474, loss G: 2.5389\n",
      "Epoch [2/10] Batch 423/510               Loss D: 0.1010, loss G: 2.4743\n",
      "Epoch [2/10] Batch 424/510               Loss D: 0.0588, loss G: 2.4079\n",
      "Epoch [2/10] Batch 425/510               Loss D: 0.2226, loss G: 2.3388\n",
      "Epoch [2/10] Batch 426/510               Loss D: 0.1896, loss G: 2.2566\n",
      "Epoch [2/10] Batch 427/510               Loss D: 0.0852, loss G: 2.1977\n",
      "Epoch [2/10] Batch 428/510               Loss D: 0.0754, loss G: 2.1791\n",
      "Epoch [2/10] Batch 429/510               Loss D: 0.0781, loss G: 2.1579\n",
      "Epoch [2/10] Batch 430/510               Loss D: 0.1296, loss G: 2.1385\n",
      "Epoch [2/10] Batch 431/510               Loss D: 0.0776, loss G: 2.1269\n",
      "Epoch [2/10] Batch 432/510               Loss D: 0.0806, loss G: 2.1021\n",
      "Epoch [2/10] Batch 433/510               Loss D: 0.2100, loss G: 2.0863\n",
      "Epoch [2/10] Batch 434/510               Loss D: 0.0799, loss G: 2.1059\n",
      "Epoch [2/10] Batch 435/510               Loss D: 0.0813, loss G: 2.1290\n",
      "Epoch [2/10] Batch 436/510               Loss D: 0.0898, loss G: 2.1412\n",
      "Epoch [2/10] Batch 437/510               Loss D: 0.0821, loss G: 2.1772\n",
      "Epoch [2/10] Batch 438/510               Loss D: 0.0977, loss G: 2.1892\n",
      "Epoch [2/10] Batch 439/510               Loss D: 0.0733, loss G: 2.2560\n",
      "Epoch [2/10] Batch 440/510               Loss D: 0.0808, loss G: 2.2642\n",
      "Epoch [2/10] Batch 441/510               Loss D: 0.0699, loss G: 2.3296\n",
      "Epoch [2/10] Batch 442/510               Loss D: 0.0794, loss G: 2.3768\n",
      "Epoch [2/10] Batch 443/510               Loss D: 0.0648, loss G: 2.4283\n",
      "Epoch [2/10] Batch 444/510               Loss D: 0.0640, loss G: 2.4716\n",
      "Epoch [2/10] Batch 445/510               Loss D: 0.0590, loss G: 2.5412\n",
      "Epoch [2/10] Batch 446/510               Loss D: 0.0957, loss G: 2.5635\n",
      "Epoch [2/10] Batch 447/510               Loss D: 0.0698, loss G: 2.6052\n",
      "Epoch [2/10] Batch 448/510               Loss D: 0.0570, loss G: 2.6401\n",
      "Epoch [2/10] Batch 449/510               Loss D: 0.0546, loss G: 2.6660\n",
      "Epoch [2/10] Batch 450/510               Loss D: 0.0534, loss G: 2.6625\n",
      "Epoch [2/10] Batch 451/510               Loss D: 0.0550, loss G: 2.6697\n",
      "Epoch [2/10] Batch 452/510               Loss D: 0.0581, loss G: 2.6934\n",
      "Epoch [2/10] Batch 453/510               Loss D: 0.0788, loss G: 2.7497\n",
      "Epoch [2/10] Batch 454/510               Loss D: 0.0488, loss G: 2.7318\n",
      "Epoch [2/10] Batch 455/510               Loss D: 0.0585, loss G: 2.7297\n",
      "Epoch [2/10] Batch 456/510               Loss D: 0.0535, loss G: 2.7284\n",
      "Epoch [2/10] Batch 457/510               Loss D: 0.0591, loss G: 2.7063\n",
      "Epoch [2/10] Batch 458/510               Loss D: 0.0694, loss G: 2.6795\n",
      "Epoch [2/10] Batch 459/510               Loss D: 0.0589, loss G: 2.6833\n",
      "Epoch [2/10] Batch 460/510               Loss D: 0.3225, loss G: 2.6168\n",
      "Epoch [2/10] Batch 461/510               Loss D: 0.0596, loss G: 2.5482\n",
      "Epoch [2/10] Batch 462/510               Loss D: 0.0628, loss G: 2.4897\n",
      "Epoch [2/10] Batch 463/510               Loss D: 0.0751, loss G: 2.4739\n",
      "Epoch [2/10] Batch 464/510               Loss D: 0.0719, loss G: 2.3609\n",
      "Epoch [2/10] Batch 465/510               Loss D: 0.0841, loss G: 2.3411\n",
      "Epoch [2/10] Batch 466/510               Loss D: 0.1211, loss G: 2.3021\n",
      "Epoch [2/10] Batch 467/510               Loss D: 0.1358, loss G: 2.2819\n",
      "Epoch [2/10] Batch 468/510               Loss D: 0.0861, loss G: 2.2837\n",
      "Epoch [2/10] Batch 469/510               Loss D: 0.0955, loss G: 2.2518\n",
      "Epoch [2/10] Batch 470/510               Loss D: 0.0884, loss G: 2.2502\n",
      "Epoch [2/10] Batch 471/510               Loss D: 0.0869, loss G: 2.2228\n",
      "Epoch [2/10] Batch 472/510               Loss D: 0.0925, loss G: 2.2778\n",
      "Epoch [2/10] Batch 473/510               Loss D: 0.1087, loss G: 2.2606\n",
      "Epoch [2/10] Batch 474/510               Loss D: 0.1264, loss G: 2.2548\n",
      "Epoch [2/10] Batch 475/510               Loss D: 0.1013, loss G: 2.2932\n",
      "Epoch [2/10] Batch 476/510               Loss D: 0.0836, loss G: 2.3032\n",
      "Epoch [2/10] Batch 477/510               Loss D: 0.0783, loss G: 2.3509\n",
      "Epoch [2/10] Batch 478/510               Loss D: 0.0780, loss G: 2.3498\n",
      "Epoch [2/10] Batch 479/510               Loss D: 0.0838, loss G: 2.4127\n",
      "Epoch [2/10] Batch 480/510               Loss D: 0.0733, loss G: 2.4089\n",
      "Epoch [2/10] Batch 481/510               Loss D: 0.0686, loss G: 2.4712\n",
      "Epoch [2/10] Batch 482/510               Loss D: 0.0684, loss G: 2.4722\n",
      "Epoch [2/10] Batch 483/510               Loss D: 0.0729, loss G: 2.4763\n",
      "Epoch [2/10] Batch 484/510               Loss D: 0.0814, loss G: 2.5144\n",
      "Epoch [2/10] Batch 485/510               Loss D: 0.0660, loss G: 2.4896\n",
      "Epoch [2/10] Batch 486/510               Loss D: 0.0756, loss G: 2.5475\n",
      "Epoch [2/10] Batch 487/510               Loss D: 0.0614, loss G: 2.5427\n",
      "Epoch [2/10] Batch 488/510               Loss D: 0.0635, loss G: 2.5347\n",
      "Epoch [2/10] Batch 489/510               Loss D: 0.3674, loss G: 2.5125\n",
      "Epoch [2/10] Batch 490/510               Loss D: 0.0722, loss G: 2.4817\n",
      "Epoch [2/10] Batch 491/510               Loss D: 0.0651, loss G: 2.4269\n",
      "Epoch [2/10] Batch 492/510               Loss D: 0.0666, loss G: 2.4229\n",
      "Epoch [2/10] Batch 493/510               Loss D: 0.0751, loss G: 2.3802\n",
      "Epoch [2/10] Batch 494/510               Loss D: 0.0714, loss G: 2.3366\n",
      "Epoch [2/10] Batch 495/510               Loss D: 0.0721, loss G: 2.3217\n",
      "Epoch [2/10] Batch 496/510               Loss D: 0.0781, loss G: 2.3490\n",
      "Epoch [2/10] Batch 497/510               Loss D: 0.0723, loss G: 2.3235\n",
      "Epoch [2/10] Batch 498/510               Loss D: 0.0731, loss G: 2.3393\n",
      "Epoch [2/10] Batch 499/510               Loss D: 0.0959, loss G: 2.3134\n",
      "Epoch [2/10] Batch 500/510               Loss D: 0.0878, loss G: 2.3342\n",
      "Epoch [2/10] Batch 501/510               Loss D: 0.0736, loss G: 2.3058\n",
      "Epoch [2/10] Batch 502/510               Loss D: 0.0748, loss G: 2.2905\n",
      "Epoch [2/10] Batch 503/510               Loss D: 0.0854, loss G: 2.3183\n",
      "Epoch [2/10] Batch 504/510               Loss D: 0.0782, loss G: 2.3486\n",
      "Epoch [2/10] Batch 505/510               Loss D: 0.5560, loss G: 2.3397\n",
      "Epoch [2/10] Batch 506/510               Loss D: 0.0701, loss G: 2.3244\n",
      "Epoch [2/10] Batch 507/510               Loss D: 0.0762, loss G: 2.2796\n",
      "Epoch [2/10] Batch 508/510               Loss D: 0.0777, loss G: 2.2747\n",
      "Epoch [2/10] Batch 509/510               Loss D: 0.0800, loss G: 2.2280\n",
      "Epoch [3/10] Batch 0/510               Loss D: 0.0834, loss G: 2.1949\n",
      "Epoch [3/10] Batch 1/510               Loss D: 0.1093, loss G: 2.2078\n",
      "Epoch [3/10] Batch 2/510               Loss D: 0.0767, loss G: 2.2664\n",
      "Epoch [3/10] Batch 3/510               Loss D: 0.0799, loss G: 2.2546\n",
      "Epoch [3/10] Batch 4/510               Loss D: 0.0810, loss G: 2.2439\n",
      "Epoch [3/10] Batch 5/510               Loss D: 0.0811, loss G: 2.2312\n",
      "Epoch [3/10] Batch 6/510               Loss D: 0.0831, loss G: 2.2813\n",
      "Epoch [3/10] Batch 7/510               Loss D: 0.0823, loss G: 2.2434\n",
      "Epoch [3/10] Batch 8/510               Loss D: 0.0848, loss G: 2.2568\n",
      "Epoch [3/10] Batch 9/510               Loss D: 0.1096, loss G: 2.2915\n",
      "Epoch [3/10] Batch 10/510               Loss D: 0.0823, loss G: 2.2784\n",
      "Epoch [3/10] Batch 11/510               Loss D: 0.0834, loss G: 2.2650\n",
      "Epoch [3/10] Batch 12/510               Loss D: 0.0807, loss G: 2.3341\n",
      "Epoch [3/10] Batch 13/510               Loss D: 0.1047, loss G: 2.3120\n",
      "Epoch [3/10] Batch 14/510               Loss D: 0.0766, loss G: 2.3582\n",
      "Epoch [3/10] Batch 15/510               Loss D: 0.0805, loss G: 2.3372\n",
      "Epoch [3/10] Batch 16/510               Loss D: 0.0883, loss G: 2.3368\n",
      "Epoch [3/10] Batch 17/510               Loss D: 0.0940, loss G: 2.4049\n",
      "Epoch [3/10] Batch 18/510               Loss D: 0.0770, loss G: 2.3764\n",
      "Epoch [3/10] Batch 19/510               Loss D: 0.0792, loss G: 2.3918\n",
      "Epoch [3/10] Batch 20/510               Loss D: 0.0773, loss G: 2.3808\n",
      "Epoch [3/10] Batch 21/510               Loss D: 0.0916, loss G: 2.3651\n",
      "Epoch [3/10] Batch 22/510               Loss D: 0.0911, loss G: 2.4057\n",
      "Epoch [3/10] Batch 23/510               Loss D: 0.0894, loss G: 2.3961\n",
      "Epoch [3/10] Batch 24/510               Loss D: 0.0787, loss G: 2.4226\n",
      "Epoch [3/10] Batch 25/510               Loss D: 0.0802, loss G: 2.4586\n",
      "Epoch [3/10] Batch 26/510               Loss D: 0.0821, loss G: 2.3869\n",
      "Epoch [3/10] Batch 27/510               Loss D: 0.0790, loss G: 2.4207\n",
      "Epoch [3/10] Batch 28/510               Loss D: 0.0978, loss G: 2.4124\n",
      "Epoch [3/10] Batch 29/510               Loss D: 0.1128, loss G: 2.4608\n",
      "Epoch [3/10] Batch 30/510               Loss D: 0.0885, loss G: 2.3894\n",
      "Epoch [3/10] Batch 31/510               Loss D: 0.0808, loss G: 2.3938\n",
      "Epoch [3/10] Batch 32/510               Loss D: 0.0816, loss G: 2.3616\n",
      "Epoch [3/10] Batch 33/510               Loss D: 0.0861, loss G: 2.3636\n",
      "Epoch [3/10] Batch 34/510               Loss D: 0.0824, loss G: 2.3845\n",
      "Epoch [3/10] Batch 35/510               Loss D: 0.0851, loss G: 2.3796\n",
      "Epoch [3/10] Batch 36/510               Loss D: 0.0939, loss G: 2.3918\n",
      "Epoch [3/10] Batch 37/510               Loss D: 0.0825, loss G: 2.3971\n",
      "Epoch [3/10] Batch 38/510               Loss D: 0.0824, loss G: 2.3885\n",
      "Epoch [3/10] Batch 39/510               Loss D: 0.0826, loss G: 2.3873\n",
      "Epoch [3/10] Batch 40/510               Loss D: 0.0860, loss G: 2.3888\n",
      "Epoch [3/10] Batch 41/510               Loss D: 0.0922, loss G: 2.4380\n",
      "Epoch [3/10] Batch 42/510               Loss D: 0.0802, loss G: 2.4107\n",
      "Epoch [3/10] Batch 43/510               Loss D: 0.0867, loss G: 2.3872\n",
      "Epoch [3/10] Batch 44/510               Loss D: 0.0838, loss G: 2.4175\n",
      "Epoch [3/10] Batch 45/510               Loss D: 0.1688, loss G: 2.4135\n",
      "Epoch [3/10] Batch 46/510               Loss D: 0.0760, loss G: 2.4301\n",
      "Epoch [3/10] Batch 47/510               Loss D: 0.0785, loss G: 2.4185\n",
      "Epoch [3/10] Batch 48/510               Loss D: 0.1240, loss G: 2.3749\n",
      "Epoch [3/10] Batch 49/510               Loss D: 0.0842, loss G: 2.4186\n",
      "Epoch [3/10] Batch 50/510               Loss D: 0.0793, loss G: 2.3658\n",
      "Epoch [3/10] Batch 51/510               Loss D: 0.0765, loss G: 2.3939\n",
      "Epoch [3/10] Batch 52/510               Loss D: 0.0982, loss G: 2.3232\n",
      "Epoch [3/10] Batch 53/510               Loss D: 0.0881, loss G: 2.3856\n",
      "Epoch [3/10] Batch 54/510               Loss D: 0.0828, loss G: 2.3420\n",
      "Epoch [3/10] Batch 55/510               Loss D: 0.0805, loss G: 2.3674\n",
      "Epoch [3/10] Batch 56/510               Loss D: 0.0801, loss G: 2.3599\n",
      "Epoch [3/10] Batch 57/510               Loss D: 0.0832, loss G: 2.3129\n",
      "Epoch [3/10] Batch 58/510               Loss D: 0.6981, loss G: 2.2993\n",
      "Epoch [3/10] Batch 59/510               Loss D: 0.6364, loss G: 2.1206\n",
      "Epoch [3/10] Batch 60/510               Loss D: 0.0984, loss G: 2.0922\n",
      "Epoch [3/10] Batch 61/510               Loss D: 0.1076, loss G: 1.9868\n",
      "Epoch [3/10] Batch 62/510               Loss D: 0.1248, loss G: 1.9430\n",
      "Epoch [3/10] Batch 63/510               Loss D: 0.1259, loss G: 1.8680\n",
      "Epoch [3/10] Batch 64/510               Loss D: 0.1267, loss G: 1.8701\n",
      "Epoch [3/10] Batch 65/510               Loss D: 0.1280, loss G: 1.8721\n",
      "Epoch [3/10] Batch 66/510               Loss D: 0.1308, loss G: 1.8298\n",
      "Epoch [3/10] Batch 67/510               Loss D: 0.1340, loss G: 1.8154\n",
      "Epoch [3/10] Batch 68/510               Loss D: 0.1272, loss G: 1.8718\n",
      "Epoch [3/10] Batch 69/510               Loss D: 0.1343, loss G: 1.8605\n",
      "Epoch [3/10] Batch 70/510               Loss D: 0.1345, loss G: 1.8396\n",
      "Epoch [3/10] Batch 71/510               Loss D: 0.1435, loss G: 1.9091\n",
      "Epoch [3/10] Batch 72/510               Loss D: 0.1269, loss G: 1.9257\n",
      "Epoch [3/10] Batch 73/510               Loss D: 0.1204, loss G: 1.9497\n",
      "Epoch [3/10] Batch 74/510               Loss D: 0.1221, loss G: 1.9346\n",
      "Epoch [3/10] Batch 75/510               Loss D: 0.1162, loss G: 1.9938\n",
      "Epoch [3/10] Batch 76/510               Loss D: 0.1205, loss G: 1.9565\n",
      "Epoch [3/10] Batch 77/510               Loss D: 0.1053, loss G: 2.1035\n",
      "Epoch [3/10] Batch 78/510               Loss D: 0.1025, loss G: 2.1287\n",
      "Epoch [3/10] Batch 79/510               Loss D: 0.1012, loss G: 2.1535\n",
      "Epoch [3/10] Batch 80/510               Loss D: 0.1024, loss G: 2.1524\n",
      "Epoch [3/10] Batch 81/510               Loss D: 0.0965, loss G: 2.1933\n",
      "Epoch [3/10] Batch 82/510               Loss D: 0.0915, loss G: 2.2329\n",
      "Epoch [3/10] Batch 83/510               Loss D: 0.0913, loss G: 2.2362\n",
      "Epoch [3/10] Batch 84/510               Loss D: 0.1108, loss G: 2.2652\n",
      "Epoch [3/10] Batch 85/510               Loss D: 0.0893, loss G: 2.2373\n",
      "Epoch [3/10] Batch 86/510               Loss D: 0.0872, loss G: 2.2698\n",
      "Epoch [3/10] Batch 87/510               Loss D: 0.0869, loss G: 2.3021\n",
      "Epoch [3/10] Batch 88/510               Loss D: 0.0811, loss G: 2.3043\n",
      "Epoch [3/10] Batch 89/510               Loss D: 0.0785, loss G: 2.3497\n",
      "Epoch [3/10] Batch 90/510               Loss D: 0.0788, loss G: 2.3789\n",
      "Epoch [3/10] Batch 91/510               Loss D: 0.0814, loss G: 2.3603\n",
      "Epoch [3/10] Batch 92/510               Loss D: 0.0957, loss G: 2.3478\n",
      "Epoch [3/10] Batch 93/510               Loss D: 0.0804, loss G: 2.3488\n",
      "Epoch [3/10] Batch 94/510               Loss D: 0.0748, loss G: 2.4226\n",
      "Epoch [3/10] Batch 95/510               Loss D: 0.0761, loss G: 2.4003\n",
      "Epoch [3/10] Batch 96/510               Loss D: 0.0738, loss G: 2.3703\n",
      "Epoch [3/10] Batch 97/510               Loss D: 0.0730, loss G: 2.3752\n",
      "Epoch [3/10] Batch 98/510               Loss D: 0.0843, loss G: 2.3864\n",
      "Epoch [3/10] Batch 99/510               Loss D: 0.0717, loss G: 2.4009\n",
      "Epoch [3/10] Batch 100/510               Loss D: 0.0716, loss G: 2.4041\n",
      "Epoch [3/10] Batch 101/510               Loss D: 0.0711, loss G: 2.3835\n",
      "Epoch [3/10] Batch 102/510               Loss D: 0.0774, loss G: 2.4284\n",
      "Epoch [3/10] Batch 103/510               Loss D: 0.0733, loss G: 2.4112\n",
      "Epoch [3/10] Batch 104/510               Loss D: 0.0832, loss G: 2.4616\n",
      "Epoch [3/10] Batch 105/510               Loss D: 0.1093, loss G: 2.4536\n",
      "Epoch [3/10] Batch 106/510               Loss D: 0.0707, loss G: 2.4200\n",
      "Epoch [3/10] Batch 107/510               Loss D: 0.0689, loss G: 2.4720\n",
      "Epoch [3/10] Batch 108/510               Loss D: 0.6267, loss G: 2.3353\n",
      "Epoch [3/10] Batch 109/510               Loss D: 0.4947, loss G: 2.1713\n",
      "Epoch [3/10] Batch 110/510               Loss D: 0.0929, loss G: 2.0289\n",
      "Epoch [3/10] Batch 111/510               Loss D: 0.1142, loss G: 1.9041\n",
      "Epoch [3/10] Batch 112/510               Loss D: 0.1110, loss G: 1.8581\n",
      "Epoch [3/10] Batch 113/510               Loss D: 0.1426, loss G: 1.7344\n",
      "Epoch [3/10] Batch 114/510               Loss D: 0.1316, loss G: 1.7427\n",
      "Epoch [3/10] Batch 115/510               Loss D: 0.1401, loss G: 1.7093\n",
      "Epoch [3/10] Batch 116/510               Loss D: 0.1413, loss G: 1.7306\n",
      "Epoch [3/10] Batch 117/510               Loss D: 0.1422, loss G: 1.7725\n",
      "Epoch [3/10] Batch 118/510               Loss D: 0.1320, loss G: 1.8243\n",
      "Epoch [3/10] Batch 119/510               Loss D: 0.1268, loss G: 1.9090\n",
      "Epoch [3/10] Batch 120/510               Loss D: 0.1238, loss G: 1.9539\n",
      "Epoch [3/10] Batch 121/510               Loss D: 0.1472, loss G: 2.0435\n",
      "Epoch [3/10] Batch 122/510               Loss D: 0.1046, loss G: 2.1319\n",
      "Epoch [3/10] Batch 123/510               Loss D: 0.3428, loss G: 2.1340\n",
      "Epoch [3/10] Batch 124/510               Loss D: 0.0923, loss G: 2.2019\n",
      "Epoch [3/10] Batch 125/510               Loss D: 0.2749, loss G: 2.1949\n",
      "Epoch [3/10] Batch 126/510               Loss D: 0.1125, loss G: 2.1760\n",
      "Epoch [3/10] Batch 127/510               Loss D: 0.0888, loss G: 2.2265\n",
      "Epoch [3/10] Batch 128/510               Loss D: 0.1371, loss G: 2.1817\n",
      "Epoch [3/10] Batch 129/510               Loss D: 0.3172, loss G: 2.2642\n",
      "Epoch [3/10] Batch 130/510               Loss D: 0.1988, loss G: 2.1602\n",
      "Epoch [3/10] Batch 131/510               Loss D: 0.0917, loss G: 2.1855\n",
      "Epoch [3/10] Batch 132/510               Loss D: 0.0929, loss G: 2.1601\n",
      "Epoch [3/10] Batch 133/510               Loss D: 0.0925, loss G: 2.1878\n",
      "Epoch [3/10] Batch 134/510               Loss D: 0.5871, loss G: 2.1059\n",
      "Epoch [3/10] Batch 135/510               Loss D: 0.1549, loss G: 2.1137\n",
      "Epoch [3/10] Batch 136/510               Loss D: 0.1077, loss G: 2.0129\n",
      "Epoch [3/10] Batch 137/510               Loss D: 0.1099, loss G: 2.0180\n",
      "Epoch [3/10] Batch 138/510               Loss D: 0.1080, loss G: 2.0447\n",
      "Epoch [3/10] Batch 139/510               Loss D: 0.1067, loss G: 2.0776\n",
      "Epoch [3/10] Batch 140/510               Loss D: 0.1059, loss G: 2.1495\n",
      "Epoch [3/10] Batch 141/510               Loss D: 0.0979, loss G: 2.2028\n",
      "Epoch [3/10] Batch 142/510               Loss D: 0.1161, loss G: 2.2710\n",
      "Epoch [3/10] Batch 143/510               Loss D: 0.0874, loss G: 2.3363\n",
      "Epoch [3/10] Batch 144/510               Loss D: 0.0893, loss G: 2.4259\n",
      "Epoch [3/10] Batch 145/510               Loss D: 0.0751, loss G: 2.5326\n",
      "Epoch [3/10] Batch 146/510               Loss D: 0.0807, loss G: 2.5590\n",
      "Epoch [3/10] Batch 147/510               Loss D: 0.0792, loss G: 2.6031\n",
      "Epoch [3/10] Batch 148/510               Loss D: 0.0732, loss G: 2.6834\n",
      "Epoch [3/10] Batch 149/510               Loss D: 0.0921, loss G: 2.7181\n",
      "Epoch [3/10] Batch 150/510               Loss D: 0.0688, loss G: 2.7945\n",
      "Epoch [3/10] Batch 151/510               Loss D: 0.0571, loss G: 2.7998\n",
      "Epoch [3/10] Batch 152/510               Loss D: 0.0739, loss G: 2.7896\n",
      "Epoch [3/10] Batch 153/510               Loss D: 0.1223, loss G: 2.8454\n",
      "Epoch [3/10] Batch 154/510               Loss D: 0.0562, loss G: 2.8183\n",
      "Epoch [3/10] Batch 155/510               Loss D: 0.0545, loss G: 2.8268\n",
      "Epoch [3/10] Batch 156/510               Loss D: 0.0791, loss G: 2.7461\n",
      "Epoch [3/10] Batch 157/510               Loss D: 0.0562, loss G: 2.7719\n",
      "Epoch [3/10] Batch 158/510               Loss D: 0.0549, loss G: 2.8051\n",
      "Epoch [3/10] Batch 159/510               Loss D: 0.0574, loss G: 2.7560\n",
      "Epoch [3/10] Batch 160/510               Loss D: 0.1101, loss G: 2.7305\n",
      "Epoch [3/10] Batch 161/510               Loss D: 0.0860, loss G: 2.7626\n",
      "Epoch [3/10] Batch 162/510               Loss D: 0.0584, loss G: 2.6950\n",
      "Epoch [3/10] Batch 163/510               Loss D: 0.0651, loss G: 2.6688\n",
      "Epoch [3/10] Batch 164/510               Loss D: 0.0628, loss G: 2.6916\n",
      "Epoch [3/10] Batch 165/510               Loss D: 0.0670, loss G: 2.6869\n",
      "Epoch [3/10] Batch 166/510               Loss D: 0.0611, loss G: 2.6701\n",
      "Epoch [3/10] Batch 167/510               Loss D: 0.0652, loss G: 2.6713\n",
      "Epoch [3/10] Batch 168/510               Loss D: 0.0657, loss G: 2.6404\n",
      "Epoch [3/10] Batch 169/510               Loss D: 0.0618, loss G: 2.7196\n",
      "Epoch [3/10] Batch 170/510               Loss D: 0.1054, loss G: 2.6812\n",
      "Epoch [3/10] Batch 171/510               Loss D: 0.3339, loss G: 2.6395\n",
      "Epoch [3/10] Batch 172/510               Loss D: 0.1036, loss G: 2.5533\n",
      "Epoch [3/10] Batch 173/510               Loss D: 0.0720, loss G: 2.4661\n",
      "Epoch [3/10] Batch 174/510               Loss D: 0.0745, loss G: 2.3963\n",
      "Epoch [3/10] Batch 175/510               Loss D: 0.0864, loss G: 2.4060\n",
      "Epoch [3/10] Batch 176/510               Loss D: 0.1142, loss G: 2.3872\n",
      "Epoch [3/10] Batch 177/510               Loss D: 0.0796, loss G: 2.3284\n",
      "Epoch [3/10] Batch 178/510               Loss D: 0.1629, loss G: 2.3861\n",
      "Epoch [3/10] Batch 179/510               Loss D: 0.1118, loss G: 2.3792\n",
      "Epoch [3/10] Batch 180/510               Loss D: 0.0764, loss G: 2.3550\n",
      "Epoch [3/10] Batch 181/510               Loss D: 0.1461, loss G: 2.4105\n",
      "Epoch [3/10] Batch 182/510               Loss D: 0.0685, loss G: 2.4383\n",
      "Epoch [3/10] Batch 183/510               Loss D: 0.0700, loss G: 2.4674\n",
      "Epoch [3/10] Batch 184/510               Loss D: 0.0643, loss G: 2.5380\n",
      "Epoch [3/10] Batch 185/510               Loss D: 0.0658, loss G: 2.5593\n",
      "Epoch [3/10] Batch 186/510               Loss D: 0.2609, loss G: 2.5865\n",
      "Epoch [3/10] Batch 187/510               Loss D: 0.0570, loss G: 2.5722\n",
      "Epoch [3/10] Batch 188/510               Loss D: 0.0540, loss G: 2.6359\n",
      "Epoch [3/10] Batch 189/510               Loss D: 0.0517, loss G: 2.6711\n",
      "Epoch [3/10] Batch 190/510               Loss D: 0.7910, loss G: 2.6466\n",
      "Epoch [3/10] Batch 191/510               Loss D: 0.0556, loss G: 2.6157\n",
      "Epoch [3/10] Batch 192/510               Loss D: 0.0620, loss G: 2.6059\n",
      "Epoch [3/10] Batch 193/510               Loss D: 0.0679, loss G: 2.5412\n",
      "Epoch [3/10] Batch 194/510               Loss D: 0.0538, loss G: 2.5835\n",
      "Epoch [3/10] Batch 195/510               Loss D: 0.0567, loss G: 2.5418\n",
      "Epoch [3/10] Batch 196/510               Loss D: 0.0570, loss G: 2.5846\n",
      "Epoch [3/10] Batch 197/510               Loss D: 0.0609, loss G: 2.5660\n",
      "Epoch [3/10] Batch 198/510               Loss D: 0.0644, loss G: 2.6042\n",
      "Epoch [3/10] Batch 199/510               Loss D: 0.0553, loss G: 2.6207\n",
      "Epoch [3/10] Batch 200/510               Loss D: 0.0544, loss G: 2.6669\n",
      "Epoch [3/10] Batch 201/510               Loss D: 0.0531, loss G: 2.6369\n",
      "Epoch [3/10] Batch 202/510               Loss D: 0.0493, loss G: 2.7281\n",
      "Epoch [3/10] Batch 203/510               Loss D: 0.0777, loss G: 2.7302\n",
      "Epoch [3/10] Batch 204/510               Loss D: 0.0930, loss G: 2.7176\n",
      "Epoch [3/10] Batch 205/510               Loss D: 0.0680, loss G: 2.7571\n",
      "Epoch [3/10] Batch 206/510               Loss D: 0.0482, loss G: 2.7468\n",
      "Epoch [3/10] Batch 207/510               Loss D: 0.0496, loss G: 2.7340\n",
      "Epoch [3/10] Batch 208/510               Loss D: 0.0472, loss G: 2.7772\n",
      "Epoch [3/10] Batch 209/510               Loss D: 0.0480, loss G: 2.7509\n",
      "Epoch [3/10] Batch 210/510               Loss D: 0.0550, loss G: 2.7384\n",
      "Epoch [3/10] Batch 211/510               Loss D: 0.0606, loss G: 2.7813\n",
      "Epoch [3/10] Batch 212/510               Loss D: 0.0557, loss G: 2.7749\n",
      "Epoch [3/10] Batch 213/510               Loss D: 0.0439, loss G: 2.8235\n",
      "Epoch [3/10] Batch 214/510               Loss D: 0.0620, loss G: 2.7801\n",
      "Epoch [3/10] Batch 215/510               Loss D: 0.0473, loss G: 2.7713\n",
      "Epoch [3/10] Batch 216/510               Loss D: 0.1044, loss G: 2.7792\n",
      "Epoch [3/10] Batch 217/510               Loss D: 0.0456, loss G: 2.7712\n",
      "Epoch [3/10] Batch 218/510               Loss D: 0.0455, loss G: 2.7671\n",
      "Epoch [3/10] Batch 219/510               Loss D: 0.1197, loss G: 2.6817\n",
      "Epoch [3/10] Batch 220/510               Loss D: 0.0473, loss G: 2.6956\n",
      "Epoch [3/10] Batch 221/510               Loss D: 0.0667, loss G: 2.6800\n",
      "Epoch [3/10] Batch 222/510               Loss D: 0.0518, loss G: 2.6136\n",
      "Epoch [3/10] Batch 223/510               Loss D: 0.0540, loss G: 2.6161\n",
      "Epoch [3/10] Batch 224/510               Loss D: 0.0568, loss G: 2.6017\n",
      "Epoch [3/10] Batch 225/510               Loss D: 0.0492, loss G: 2.6777\n",
      "Epoch [3/10] Batch 226/510               Loss D: 0.1262, loss G: 2.6731\n",
      "Epoch [3/10] Batch 227/510               Loss D: 0.4130, loss G: 2.5646\n",
      "Epoch [3/10] Batch 228/510               Loss D: 0.0571, loss G: 2.4728\n",
      "Epoch [3/10] Batch 229/510               Loss D: 0.0604, loss G: 2.4233\n",
      "Epoch [3/10] Batch 230/510               Loss D: 0.0621, loss G: 2.3893\n",
      "Epoch [3/10] Batch 231/510               Loss D: 0.0639, loss G: 2.3807\n",
      "Epoch [3/10] Batch 232/510               Loss D: 0.0656, loss G: 2.3618\n",
      "Epoch [3/10] Batch 233/510               Loss D: 0.0664, loss G: 2.3605\n",
      "Epoch [3/10] Batch 234/510               Loss D: 0.0639, loss G: 2.4310\n",
      "Epoch [3/10] Batch 235/510               Loss D: 0.0652, loss G: 2.4243\n",
      "Epoch [3/10] Batch 236/510               Loss D: 0.0871, loss G: 2.4694\n",
      "Epoch [3/10] Batch 237/510               Loss D: 0.0820, loss G: 2.4992\n",
      "Epoch [3/10] Batch 238/510               Loss D: 0.0641, loss G: 2.5373\n",
      "Epoch [3/10] Batch 239/510               Loss D: 0.0745, loss G: 2.5822\n",
      "Epoch [3/10] Batch 240/510               Loss D: 0.0567, loss G: 2.6038\n",
      "Epoch [3/10] Batch 241/510               Loss D: 0.0562, loss G: 2.6773\n",
      "Epoch [3/10] Batch 242/510               Loss D: 0.0500, loss G: 2.7339\n",
      "Epoch [3/10] Batch 243/510               Loss D: 0.0492, loss G: 2.7681\n",
      "Epoch [3/10] Batch 244/510               Loss D: 0.0505, loss G: 2.7721\n",
      "Epoch [3/10] Batch 245/510               Loss D: 0.0460, loss G: 2.8446\n",
      "Epoch [3/10] Batch 246/510               Loss D: 0.0462, loss G: 2.8909\n",
      "Epoch [3/10] Batch 247/510               Loss D: 0.0445, loss G: 2.9038\n",
      "Epoch [3/10] Batch 248/510               Loss D: 0.0428, loss G: 2.9290\n",
      "Epoch [3/10] Batch 249/510               Loss D: 0.0503, loss G: 2.9846\n",
      "Epoch [3/10] Batch 250/510               Loss D: 0.0408, loss G: 2.9970\n",
      "Epoch [3/10] Batch 251/510               Loss D: 1.6942, loss G: 2.7331\n",
      "Epoch [3/10] Batch 252/510               Loss D: 0.0485, loss G: 2.6431\n",
      "Epoch [3/10] Batch 253/510               Loss D: 0.0622, loss G: 2.4892\n",
      "Epoch [3/10] Batch 254/510               Loss D: 0.0668, loss G: 2.3975\n",
      "Epoch [3/10] Batch 255/510               Loss D: 0.0664, loss G: 2.3417\n",
      "Epoch [3/10] Batch 256/510               Loss D: 0.0772, loss G: 2.2865\n",
      "Epoch [3/10] Batch 257/510               Loss D: 0.0772, loss G: 2.2518\n",
      "Epoch [3/10] Batch 258/510               Loss D: 0.0790, loss G: 2.2626\n",
      "Epoch [3/10] Batch 259/510               Loss D: 0.0776, loss G: 2.2847\n",
      "Epoch [3/10] Batch 260/510               Loss D: 0.0782, loss G: 2.3134\n",
      "Epoch [3/10] Batch 261/510               Loss D: 0.0766, loss G: 2.3429\n",
      "Epoch [3/10] Batch 262/510               Loss D: 0.0745, loss G: 2.3999\n",
      "Epoch [3/10] Batch 263/510               Loss D: 0.0713, loss G: 2.5152\n",
      "Epoch [3/10] Batch 264/510               Loss D: 0.0643, loss G: 2.5484\n",
      "Epoch [3/10] Batch 265/510               Loss D: 0.0596, loss G: 2.6090\n",
      "Epoch [3/10] Batch 266/510               Loss D: 0.0586, loss G: 2.6935\n",
      "Epoch [3/10] Batch 267/510               Loss D: 0.0626, loss G: 2.7833\n",
      "Epoch [3/10] Batch 268/510               Loss D: 0.3762, loss G: 2.7189\n",
      "Epoch [3/10] Batch 269/510               Loss D: 0.2453, loss G: 2.6977\n",
      "Epoch [3/10] Batch 270/510               Loss D: 0.1595, loss G: 2.5951\n",
      "Epoch [3/10] Batch 271/510               Loss D: 0.0544, loss G: 2.5754\n",
      "Epoch [3/10] Batch 272/510               Loss D: 0.0594, loss G: 2.5259\n",
      "Epoch [3/10] Batch 273/510               Loss D: 0.0997, loss G: 2.5078\n",
      "Epoch [3/10] Batch 274/510               Loss D: 0.0672, loss G: 2.4454\n",
      "Epoch [3/10] Batch 275/510               Loss D: 0.0670, loss G: 2.3863\n",
      "Epoch [3/10] Batch 276/510               Loss D: 0.5346, loss G: 2.2848\n",
      "Epoch [3/10] Batch 277/510               Loss D: 0.0757, loss G: 2.1792\n",
      "Epoch [3/10] Batch 278/510               Loss D: 0.0853, loss G: 2.1445\n",
      "Epoch [3/10] Batch 279/510               Loss D: 0.0866, loss G: 2.1115\n",
      "Epoch [3/10] Batch 280/510               Loss D: 0.0874, loss G: 2.1070\n",
      "Epoch [3/10] Batch 281/510               Loss D: 0.0899, loss G: 2.0937\n",
      "Epoch [3/10] Batch 282/510               Loss D: 0.1264, loss G: 2.1232\n",
      "Epoch [3/10] Batch 283/510               Loss D: 0.0869, loss G: 2.1591\n",
      "Epoch [3/10] Batch 284/510               Loss D: 0.0837, loss G: 2.2137\n",
      "Epoch [3/10] Batch 285/510               Loss D: 0.0819, loss G: 2.2851\n",
      "Epoch [3/10] Batch 286/510               Loss D: 0.0728, loss G: 2.3747\n",
      "Epoch [3/10] Batch 287/510               Loss D: 0.0657, loss G: 2.4901\n",
      "Epoch [3/10] Batch 288/510               Loss D: 0.0825, loss G: 2.5566\n",
      "Epoch [3/10] Batch 289/510               Loss D: 0.0587, loss G: 2.5978\n",
      "Epoch [3/10] Batch 290/510               Loss D: 0.0533, loss G: 2.6980\n",
      "Epoch [3/10] Batch 291/510               Loss D: 0.0600, loss G: 2.7677\n",
      "Epoch [3/10] Batch 292/510               Loss D: 0.0475, loss G: 2.8041\n",
      "Epoch [3/10] Batch 293/510               Loss D: 0.0431, loss G: 2.9085\n",
      "Epoch [3/10] Batch 294/510               Loss D: 0.0643, loss G: 2.9415\n",
      "Epoch [3/10] Batch 295/510               Loss D: 0.0423, loss G: 2.9583\n",
      "Epoch [3/10] Batch 296/510               Loss D: 0.0613, loss G: 2.9778\n",
      "Epoch [3/10] Batch 297/510               Loss D: 0.0402, loss G: 2.9744\n",
      "Epoch [3/10] Batch 298/510               Loss D: 0.0698, loss G: 2.9645\n",
      "Epoch [3/10] Batch 299/510               Loss D: 0.0510, loss G: 2.9772\n",
      "Epoch [3/10] Batch 300/510               Loss D: 0.0420, loss G: 2.9232\n",
      "Epoch [3/10] Batch 301/510               Loss D: 0.0618, loss G: 2.8879\n",
      "Epoch [3/10] Batch 302/510               Loss D: 0.0613, loss G: 2.8668\n",
      "Epoch [3/10] Batch 303/510               Loss D: 0.0506, loss G: 2.7936\n",
      "Epoch [3/10] Batch 304/510               Loss D: 0.0495, loss G: 2.7500\n",
      "Epoch [3/10] Batch 305/510               Loss D: 0.0549, loss G: 2.6870\n",
      "Epoch [3/10] Batch 306/510               Loss D: 0.0544, loss G: 2.6932\n",
      "Epoch [3/10] Batch 307/510               Loss D: 0.0561, loss G: 2.6498\n",
      "Epoch [3/10] Batch 308/510               Loss D: 0.0578, loss G: 2.6589\n",
      "Epoch [3/10] Batch 309/510               Loss D: 0.2602, loss G: 2.6263\n",
      "Epoch [3/10] Batch 310/510               Loss D: 0.1153, loss G: 2.5112\n",
      "Epoch [3/10] Batch 311/510               Loss D: 0.0960, loss G: 2.5154\n",
      "Epoch [3/10] Batch 312/510               Loss D: 0.1196, loss G: 2.4205\n",
      "Epoch [3/10] Batch 313/510               Loss D: 0.0875, loss G: 2.3785\n",
      "Epoch [3/10] Batch 314/510               Loss D: 0.1619, loss G: 2.2904\n",
      "Epoch [3/10] Batch 315/510               Loss D: 0.1367, loss G: 2.2989\n",
      "Epoch [3/10] Batch 316/510               Loss D: 0.4058, loss G: 2.2601\n",
      "Epoch [3/10] Batch 317/510               Loss D: 0.1793, loss G: 2.1763\n",
      "Epoch [3/10] Batch 318/510               Loss D: 0.1025, loss G: 2.1673\n",
      "Epoch [3/10] Batch 319/510               Loss D: 0.1158, loss G: 2.1437\n",
      "Epoch [3/10] Batch 320/510               Loss D: 0.1082, loss G: 2.1662\n",
      "Epoch [3/10] Batch 321/510               Loss D: 0.1061, loss G: 2.2043\n",
      "Epoch [3/10] Batch 322/510               Loss D: 0.1037, loss G: 2.2673\n",
      "Epoch [3/10] Batch 323/510               Loss D: 0.1020, loss G: 2.3036\n",
      "Epoch [3/10] Batch 324/510               Loss D: 0.0879, loss G: 2.4631\n",
      "Epoch [3/10] Batch 325/510               Loss D: 0.0935, loss G: 2.5264\n",
      "Epoch [3/10] Batch 326/510               Loss D: 0.0777, loss G: 2.6265\n",
      "Epoch [3/10] Batch 327/510               Loss D: 0.0804, loss G: 2.6884\n",
      "Epoch [3/10] Batch 328/510               Loss D: 0.0685, loss G: 2.7837\n",
      "Epoch [3/10] Batch 329/510               Loss D: 0.0640, loss G: 2.8910\n",
      "Epoch [3/10] Batch 330/510               Loss D: 0.0629, loss G: 2.9957\n",
      "Epoch [3/10] Batch 331/510               Loss D: 0.0823, loss G: 2.9869\n",
      "Epoch [3/10] Batch 332/510               Loss D: 0.0569, loss G: 2.9981\n",
      "Epoch [3/10] Batch 333/510               Loss D: 0.1273, loss G: 3.0675\n",
      "Epoch [3/10] Batch 334/510               Loss D: 0.1166, loss G: 3.0243\n",
      "Epoch [3/10] Batch 335/510               Loss D: 0.2868, loss G: 2.9073\n",
      "Epoch [3/10] Batch 336/510               Loss D: 0.1400, loss G: 2.8608\n",
      "Epoch [3/10] Batch 337/510               Loss D: 0.3897, loss G: 2.6147\n",
      "Epoch [3/10] Batch 338/510               Loss D: 0.0857, loss G: 2.4735\n",
      "Epoch [3/10] Batch 339/510               Loss D: 0.0925, loss G: 2.3600\n",
      "Epoch [3/10] Batch 340/510               Loss D: 0.1097, loss G: 2.2067\n",
      "Epoch [3/10] Batch 341/510               Loss D: 0.1219, loss G: 2.1538\n",
      "Epoch [3/10] Batch 342/510               Loss D: 0.1134, loss G: 2.1967\n",
      "Epoch [3/10] Batch 343/510               Loss D: 0.1249, loss G: 2.1422\n",
      "Epoch [3/10] Batch 344/510               Loss D: 0.1160, loss G: 2.1831\n",
      "Epoch [3/10] Batch 345/510               Loss D: 0.1157, loss G: 2.2188\n",
      "Epoch [3/10] Batch 346/510               Loss D: 0.1068, loss G: 2.2971\n",
      "Epoch [3/10] Batch 347/510               Loss D: 0.1060, loss G: 2.3473\n",
      "Epoch [3/10] Batch 348/510               Loss D: 0.0913, loss G: 2.4549\n",
      "Epoch [3/10] Batch 349/510               Loss D: 0.0824, loss G: 2.5388\n",
      "Epoch [3/10] Batch 350/510               Loss D: 0.0789, loss G: 2.6248\n",
      "Epoch [3/10] Batch 351/510               Loss D: 0.0782, loss G: 2.6606\n",
      "Epoch [3/10] Batch 352/510               Loss D: 0.5917, loss G: 2.6871\n",
      "Epoch [3/10] Batch 353/510               Loss D: 0.0673, loss G: 2.6479\n",
      "Epoch [3/10] Batch 354/510               Loss D: 0.0668, loss G: 2.6253\n",
      "Epoch [3/10] Batch 355/510               Loss D: 0.0719, loss G: 2.6125\n",
      "Epoch [3/10] Batch 356/510               Loss D: 0.0706, loss G: 2.5854\n",
      "Epoch [3/10] Batch 357/510               Loss D: 0.0686, loss G: 2.6237\n",
      "Epoch [3/10] Batch 358/510               Loss D: 0.0713, loss G: 2.5911\n",
      "Epoch [3/10] Batch 359/510               Loss D: 0.0666, loss G: 2.6320\n",
      "Epoch [3/10] Batch 360/510               Loss D: 0.0694, loss G: 2.6853\n",
      "Epoch [3/10] Batch 361/510               Loss D: 0.0868, loss G: 2.5956\n",
      "Epoch [3/10] Batch 362/510               Loss D: 0.0620, loss G: 2.7398\n",
      "Epoch [3/10] Batch 363/510               Loss D: 0.0666, loss G: 2.6237\n",
      "Epoch [3/10] Batch 364/510               Loss D: 0.0640, loss G: 2.6557\n",
      "Epoch [3/10] Batch 365/510               Loss D: 0.0611, loss G: 2.6970\n",
      "Epoch [3/10] Batch 366/510               Loss D: 0.0604, loss G: 2.7140\n",
      "Epoch [3/10] Batch 367/510               Loss D: 0.0619, loss G: 2.6902\n",
      "Epoch [3/10] Batch 368/510               Loss D: 0.0629, loss G: 2.7029\n",
      "Epoch [3/10] Batch 369/510               Loss D: 0.0612, loss G: 2.7056\n",
      "Epoch [3/10] Batch 370/510               Loss D: 0.0583, loss G: 2.7431\n",
      "Epoch [3/10] Batch 371/510               Loss D: 0.0569, loss G: 2.7675\n",
      "Epoch [3/10] Batch 372/510               Loss D: 0.0727, loss G: 2.6744\n",
      "Epoch [3/10] Batch 373/510               Loss D: 0.0600, loss G: 2.7181\n",
      "Epoch [3/10] Batch 374/510               Loss D: 0.0599, loss G: 2.7223\n",
      "Epoch [3/10] Batch 375/510               Loss D: 0.0627, loss G: 2.6825\n",
      "Epoch [3/10] Batch 376/510               Loss D: 0.0626, loss G: 2.6886\n",
      "Epoch [3/10] Batch 377/510               Loss D: 0.0615, loss G: 2.6679\n",
      "Epoch [3/10] Batch 378/510               Loss D: 0.0693, loss G: 2.7103\n",
      "Epoch [3/10] Batch 379/510               Loss D: 0.0626, loss G: 2.6828\n",
      "Epoch [3/10] Batch 380/510               Loss D: 0.3654, loss G: 2.6549\n",
      "Epoch [3/10] Batch 381/510               Loss D: 0.0670, loss G: 2.5910\n",
      "Epoch [3/10] Batch 382/510               Loss D: 0.0983, loss G: 2.5628\n",
      "Epoch [3/10] Batch 383/510               Loss D: 0.0691, loss G: 2.5687\n",
      "Epoch [3/10] Batch 384/510               Loss D: 0.2581, loss G: 2.5388\n",
      "Epoch [3/10] Batch 385/510               Loss D: 0.0731, loss G: 2.4738\n",
      "Epoch [3/10] Batch 386/510               Loss D: 0.0787, loss G: 2.4016\n",
      "Epoch [3/10] Batch 387/510               Loss D: 0.0861, loss G: 2.3806\n",
      "Epoch [3/10] Batch 388/510               Loss D: 0.0926, loss G: 2.3730\n",
      "Epoch [3/10] Batch 389/510               Loss D: 0.0827, loss G: 2.4094\n",
      "Epoch [3/10] Batch 390/510               Loss D: 0.1325, loss G: 2.3220\n",
      "Epoch [3/10] Batch 391/510               Loss D: 0.0878, loss G: 2.4245\n",
      "Epoch [3/10] Batch 392/510               Loss D: 0.0821, loss G: 2.4407\n",
      "Epoch [3/10] Batch 393/510               Loss D: 0.1510, loss G: 2.4314\n",
      "Epoch [3/10] Batch 394/510               Loss D: 0.0838, loss G: 2.5087\n",
      "Epoch [3/10] Batch 395/510               Loss D: 0.0805, loss G: 2.4870\n",
      "Epoch [3/10] Batch 396/510               Loss D: 0.0862, loss G: 2.5580\n",
      "Epoch [3/10] Batch 397/510               Loss D: 0.0728, loss G: 2.6257\n",
      "Epoch [3/10] Batch 398/510               Loss D: 0.0699, loss G: 2.6534\n",
      "Epoch [3/10] Batch 399/510               Loss D: 0.0734, loss G: 2.6124\n",
      "Epoch [3/10] Batch 400/510               Loss D: 0.0698, loss G: 2.6937\n",
      "Epoch [3/10] Batch 401/510               Loss D: 0.0760, loss G: 2.8038\n",
      "Epoch [3/10] Batch 402/510               Loss D: 0.0623, loss G: 2.8527\n",
      "Epoch [3/10] Batch 403/510               Loss D: 0.0583, loss G: 2.8761\n",
      "Epoch [3/10] Batch 404/510               Loss D: 0.0774, loss G: 2.8899\n",
      "Epoch [3/10] Batch 405/510               Loss D: 0.6875, loss G: 2.8224\n",
      "Epoch [3/10] Batch 406/510               Loss D: 0.0642, loss G: 2.7347\n",
      "Epoch [3/10] Batch 407/510               Loss D: 0.1051, loss G: 2.6595\n",
      "Epoch [3/10] Batch 408/510               Loss D: 0.0717, loss G: 2.5859\n",
      "Epoch [3/10] Batch 409/510               Loss D: 0.0748, loss G: 2.5562\n",
      "Epoch [3/10] Batch 410/510               Loss D: 0.1573, loss G: 2.5292\n",
      "Epoch [3/10] Batch 411/510               Loss D: 0.0849, loss G: 2.4803\n",
      "Epoch [3/10] Batch 412/510               Loss D: 0.0837, loss G: 2.4915\n",
      "Epoch [3/10] Batch 413/510               Loss D: 0.0826, loss G: 2.5035\n",
      "Epoch [3/10] Batch 414/510               Loss D: 0.0853, loss G: 2.5233\n",
      "Epoch [3/10] Batch 415/510               Loss D: 0.0852, loss G: 2.4885\n",
      "Epoch [3/10] Batch 416/510               Loss D: 0.0898, loss G: 2.5417\n",
      "Epoch [3/10] Batch 417/510               Loss D: 0.0876, loss G: 2.4984\n",
      "Epoch [3/10] Batch 418/510               Loss D: 0.0755, loss G: 2.6506\n",
      "Epoch [3/10] Batch 419/510               Loss D: 0.0728, loss G: 2.6398\n",
      "Epoch [3/10] Batch 420/510               Loss D: 0.0758, loss G: 2.6672\n",
      "Epoch [3/10] Batch 421/510               Loss D: 0.0670, loss G: 2.8159\n",
      "Epoch [3/10] Batch 422/510               Loss D: 0.0674, loss G: 2.8009\n",
      "Epoch [3/10] Batch 423/510               Loss D: 0.0630, loss G: 2.9114\n",
      "Epoch [3/10] Batch 424/510               Loss D: 0.0544, loss G: 2.9865\n",
      "Epoch [3/10] Batch 425/510               Loss D: 0.0679, loss G: 3.0761\n",
      "Epoch [3/10] Batch 426/510               Loss D: 0.0520, loss G: 3.0745\n",
      "Epoch [3/10] Batch 427/510               Loss D: 0.0548, loss G: 2.9974\n",
      "Epoch [3/10] Batch 428/510               Loss D: 0.0472, loss G: 3.1411\n",
      "Epoch [3/10] Batch 429/510               Loss D: 0.0512, loss G: 3.0502\n",
      "Epoch [3/10] Batch 430/510               Loss D: 0.0493, loss G: 3.2091\n",
      "Epoch [3/10] Batch 431/510               Loss D: 0.0455, loss G: 3.2019\n",
      "Epoch [3/10] Batch 432/510               Loss D: 0.0486, loss G: 3.1796\n",
      "Epoch [3/10] Batch 433/510               Loss D: 0.0782, loss G: 3.1504\n",
      "Epoch [3/10] Batch 434/510               Loss D: 0.1673, loss G: 3.1562\n",
      "Epoch [3/10] Batch 435/510               Loss D: 0.0724, loss G: 3.0772\n",
      "Epoch [3/10] Batch 436/510               Loss D: 0.0560, loss G: 3.0454\n",
      "Epoch [3/10] Batch 437/510               Loss D: 0.0689, loss G: 3.0163\n",
      "Epoch [3/10] Batch 438/510               Loss D: 0.0698, loss G: 2.9516\n",
      "Epoch [3/10] Batch 439/510               Loss D: 0.0558, loss G: 2.9593\n",
      "Epoch [3/10] Batch 440/510               Loss D: 0.0613, loss G: 2.9505\n",
      "Epoch [3/10] Batch 441/510               Loss D: 0.0568, loss G: 2.9168\n",
      "Epoch [3/10] Batch 442/510               Loss D: 0.1828, loss G: 2.8232\n",
      "Epoch [3/10] Batch 443/510               Loss D: 0.0587, loss G: 2.8878\n",
      "Epoch [3/10] Batch 444/510               Loss D: 0.0619, loss G: 2.8266\n",
      "Epoch [3/10] Batch 445/510               Loss D: 0.1033, loss G: 2.9504\n",
      "Epoch [3/10] Batch 446/510               Loss D: 0.0639, loss G: 2.8001\n",
      "Epoch [3/10] Batch 447/510               Loss D: 0.0632, loss G: 2.7857\n",
      "Epoch [3/10] Batch 448/510               Loss D: 0.0977, loss G: 2.8321\n",
      "Epoch [3/10] Batch 449/510               Loss D: 0.1439, loss G: 2.7417\n",
      "Epoch [3/10] Batch 450/510               Loss D: 0.0615, loss G: 2.8415\n",
      "Epoch [3/10] Batch 451/510               Loss D: 0.0642, loss G: 2.8222\n",
      "Epoch [3/10] Batch 452/510               Loss D: 0.9247, loss G: 2.7433\n",
      "Epoch [3/10] Batch 453/510               Loss D: 0.0733, loss G: 2.6503\n",
      "Epoch [3/10] Batch 454/510               Loss D: 0.1714, loss G: 2.5582\n",
      "Epoch [3/10] Batch 455/510               Loss D: 0.0796, loss G: 2.4401\n",
      "Epoch [3/10] Batch 456/510               Loss D: 0.0825, loss G: 2.4722\n",
      "Epoch [3/10] Batch 457/510               Loss D: 0.0838, loss G: 2.4327\n",
      "Epoch [3/10] Batch 458/510               Loss D: 0.0902, loss G: 2.3771\n",
      "Epoch [3/10] Batch 459/510               Loss D: 0.0933, loss G: 2.3691\n",
      "Epoch [3/10] Batch 460/510               Loss D: 0.0851, loss G: 2.4780\n",
      "Epoch [3/10] Batch 461/510               Loss D: 0.0820, loss G: 2.5164\n",
      "Epoch [3/10] Batch 462/510               Loss D: 0.0799, loss G: 2.5541\n",
      "Epoch [3/10] Batch 463/510               Loss D: 0.0824, loss G: 2.6163\n",
      "Epoch [3/10] Batch 464/510               Loss D: 0.0661, loss G: 2.7687\n",
      "Epoch [3/10] Batch 465/510               Loss D: 0.0663, loss G: 2.8016\n",
      "Epoch [3/10] Batch 466/510               Loss D: 0.0610, loss G: 2.8910\n",
      "Epoch [3/10] Batch 467/510               Loss D: 0.0546, loss G: 2.9786\n",
      "Epoch [3/10] Batch 468/510               Loss D: 0.0537, loss G: 3.0249\n",
      "Epoch [3/10] Batch 469/510               Loss D: 0.0532, loss G: 3.0332\n",
      "Epoch [3/10] Batch 470/510               Loss D: 0.0517, loss G: 3.1118\n",
      "Epoch [3/10] Batch 471/510               Loss D: 0.0396, loss G: 3.2717\n",
      "Epoch [3/10] Batch 472/510               Loss D: 0.0425, loss G: 3.2554\n",
      "Epoch [3/10] Batch 473/510               Loss D: 0.0389, loss G: 3.2751\n",
      "Epoch [3/10] Batch 474/510               Loss D: 0.0380, loss G: 3.3082\n",
      "Epoch [3/10] Batch 475/510               Loss D: 0.0381, loss G: 3.3127\n",
      "Epoch [3/10] Batch 476/510               Loss D: 0.0423, loss G: 3.3474\n",
      "Epoch [3/10] Batch 477/510               Loss D: 0.0412, loss G: 3.4031\n",
      "Epoch [3/10] Batch 478/510               Loss D: 0.0592, loss G: 3.3303\n",
      "Epoch [3/10] Batch 479/510               Loss D: 0.0335, loss G: 3.4022\n",
      "Epoch [3/10] Batch 480/510               Loss D: 0.0435, loss G: 3.3303\n",
      "Epoch [3/10] Batch 481/510               Loss D: 0.0436, loss G: 3.3641\n",
      "Epoch [3/10] Batch 482/510               Loss D: 0.0357, loss G: 3.2863\n",
      "Epoch [3/10] Batch 483/510               Loss D: 0.0345, loss G: 3.3102\n",
      "Epoch [3/10] Batch 484/510               Loss D: 0.0565, loss G: 3.2894\n",
      "Epoch [3/10] Batch 485/510               Loss D: 0.0362, loss G: 3.2528\n",
      "Epoch [3/10] Batch 486/510               Loss D: 0.0427, loss G: 3.1933\n",
      "Epoch [3/10] Batch 487/510               Loss D: 0.0394, loss G: 3.1912\n",
      "Epoch [3/10] Batch 488/510               Loss D: 0.0397, loss G: 3.1852\n",
      "Epoch [3/10] Batch 489/510               Loss D: 0.0445, loss G: 3.0570\n",
      "Epoch [3/10] Batch 490/510               Loss D: 0.0527, loss G: 3.0419\n",
      "Epoch [3/10] Batch 491/510               Loss D: 0.0499, loss G: 3.0318\n",
      "Epoch [3/10] Batch 492/510               Loss D: 0.0759, loss G: 3.0373\n",
      "Epoch [3/10] Batch 493/510               Loss D: 0.5281, loss G: 2.8548\n",
      "Epoch [3/10] Batch 494/510               Loss D: 0.1176, loss G: 2.6696\n",
      "Epoch [3/10] Batch 495/510               Loss D: 0.0645, loss G: 2.5104\n",
      "Epoch [3/10] Batch 496/510               Loss D: 1.5103, loss G: 2.2058\n",
      "Epoch [3/10] Batch 497/510               Loss D: 0.1028, loss G: 1.9451\n",
      "Epoch [3/10] Batch 498/510               Loss D: 0.1130, loss G: 1.8470\n",
      "Epoch [3/10] Batch 499/510               Loss D: 0.1343, loss G: 1.7452\n",
      "Epoch [3/10] Batch 500/510               Loss D: 0.1418, loss G: 1.7048\n",
      "Epoch [3/10] Batch 501/510               Loss D: 0.1566, loss G: 1.6813\n",
      "Epoch [3/10] Batch 502/510               Loss D: 0.1458, loss G: 1.7846\n",
      "Epoch [3/10] Batch 503/510               Loss D: 0.1503, loss G: 1.9164\n",
      "Epoch [3/10] Batch 504/510               Loss D: 0.1187, loss G: 2.0938\n",
      "Epoch [3/10] Batch 505/510               Loss D: 0.1069, loss G: 2.2373\n",
      "Epoch [3/10] Batch 506/510               Loss D: 0.1000, loss G: 2.3965\n",
      "Epoch [3/10] Batch 507/510               Loss D: 0.0751, loss G: 2.5316\n",
      "Epoch [3/10] Batch 508/510               Loss D: 0.0739, loss G: 2.7204\n",
      "Epoch [3/10] Batch 509/510               Loss D: 0.0847, loss G: 2.9289\n",
      "Epoch [4/10] Batch 0/510               Loss D: 0.0531, loss G: 2.9869\n",
      "Epoch [4/10] Batch 1/510               Loss D: 0.0407, loss G: 3.1438\n",
      "Epoch [4/10] Batch 2/510               Loss D: 0.0376, loss G: 3.2314\n",
      "Epoch [4/10] Batch 3/510               Loss D: 0.0440, loss G: 3.3038\n",
      "Epoch [4/10] Batch 4/510               Loss D: 0.0307, loss G: 3.3864\n",
      "Epoch [4/10] Batch 5/510               Loss D: 0.0389, loss G: 3.4505\n",
      "Epoch [4/10] Batch 6/510               Loss D: 0.0287, loss G: 3.4618\n",
      "Epoch [4/10] Batch 7/510               Loss D: 0.0322, loss G: 3.4543\n",
      "Epoch [4/10] Batch 8/510               Loss D: 0.0264, loss G: 3.4751\n",
      "Epoch [4/10] Batch 9/510               Loss D: 0.0270, loss G: 3.4747\n",
      "Epoch [4/10] Batch 10/510               Loss D: 0.0429, loss G: 3.4681\n",
      "Epoch [4/10] Batch 11/510               Loss D: 0.0338, loss G: 3.5127\n",
      "Epoch [4/10] Batch 12/510               Loss D: 0.0270, loss G: 3.3947\n",
      "Epoch [4/10] Batch 13/510               Loss D: 0.1313, loss G: 3.4051\n",
      "Epoch [4/10] Batch 14/510               Loss D: 0.0882, loss G: 3.2790\n",
      "Epoch [4/10] Batch 15/510               Loss D: 0.0653, loss G: 3.1458\n",
      "Epoch [4/10] Batch 16/510               Loss D: 0.0368, loss G: 3.1208\n",
      "Epoch [4/10] Batch 17/510               Loss D: 0.0756, loss G: 2.9274\n",
      "Epoch [4/10] Batch 18/510               Loss D: 0.0440, loss G: 2.8693\n",
      "Epoch [4/10] Batch 19/510               Loss D: 0.0492, loss G: 2.7747\n",
      "Epoch [4/10] Batch 20/510               Loss D: 0.1062, loss G: 2.7156\n",
      "Epoch [4/10] Batch 21/510               Loss D: 0.0952, loss G: 2.5897\n",
      "Epoch [4/10] Batch 22/510               Loss D: 0.0589, loss G: 2.5829\n",
      "Epoch [4/10] Batch 23/510               Loss D: 0.0628, loss G: 2.5495\n",
      "Epoch [4/10] Batch 24/510               Loss D: 0.0750, loss G: 2.4993\n",
      "Epoch [4/10] Batch 25/510               Loss D: 0.0694, loss G: 2.4905\n",
      "Epoch [4/10] Batch 26/510               Loss D: 0.0682, loss G: 2.5162\n",
      "Epoch [4/10] Batch 27/510               Loss D: 0.0686, loss G: 2.5418\n",
      "Epoch [4/10] Batch 28/510               Loss D: 0.0672, loss G: 2.5784\n",
      "Epoch [4/10] Batch 29/510               Loss D: 0.2181, loss G: 2.5123\n",
      "Epoch [4/10] Batch 30/510               Loss D: 1.9446, loss G: 2.3271\n",
      "Epoch [4/10] Batch 31/510               Loss D: 0.0860, loss G: 2.1480\n",
      "Epoch [4/10] Batch 32/510               Loss D: 0.1031, loss G: 1.9683\n",
      "Epoch [4/10] Batch 33/510               Loss D: 0.1204, loss G: 1.9729\n",
      "Epoch [4/10] Batch 34/510               Loss D: 0.1553, loss G: 1.8776\n",
      "Epoch [4/10] Batch 35/510               Loss D: 0.1330, loss G: 1.9239\n",
      "Epoch [4/10] Batch 36/510               Loss D: 0.1239, loss G: 1.9820\n",
      "Epoch [4/10] Batch 37/510               Loss D: 0.1263, loss G: 1.9963\n",
      "Epoch [4/10] Batch 38/510               Loss D: 0.2007, loss G: 2.1553\n",
      "Epoch [4/10] Batch 39/510               Loss D: 0.1169, loss G: 2.2144\n",
      "Epoch [4/10] Batch 40/510               Loss D: 0.0947, loss G: 2.3484\n",
      "Epoch [4/10] Batch 41/510               Loss D: 0.0849, loss G: 2.4642\n",
      "Epoch [4/10] Batch 42/510               Loss D: 0.0776, loss G: 2.5832\n",
      "Epoch [4/10] Batch 43/510               Loss D: 0.0683, loss G: 2.7103\n",
      "Epoch [4/10] Batch 44/510               Loss D: 0.0622, loss G: 2.8196\n",
      "Epoch [4/10] Batch 45/510               Loss D: 0.0891, loss G: 2.9354\n",
      "Epoch [4/10] Batch 46/510               Loss D: 0.0528, loss G: 2.9928\n",
      "Epoch [4/10] Batch 47/510               Loss D: 0.0473, loss G: 3.0448\n",
      "Epoch [4/10] Batch 48/510               Loss D: 0.2436, loss G: 3.1198\n",
      "Epoch [4/10] Batch 49/510               Loss D: 0.0453, loss G: 3.0885\n",
      "Epoch [4/10] Batch 50/510               Loss D: 0.0418, loss G: 3.1404\n",
      "Epoch [4/10] Batch 51/510               Loss D: 0.0474, loss G: 3.1279\n",
      "Epoch [4/10] Batch 52/510               Loss D: 0.0463, loss G: 3.0650\n",
      "Epoch [4/10] Batch 53/510               Loss D: 0.0449, loss G: 3.0444\n",
      "Epoch [4/10] Batch 54/510               Loss D: 0.0545, loss G: 3.0230\n",
      "Epoch [4/10] Batch 55/510               Loss D: 0.0461, loss G: 2.9914\n",
      "Epoch [4/10] Batch 56/510               Loss D: 0.0499, loss G: 2.9385\n",
      "Epoch [4/10] Batch 57/510               Loss D: 0.0515, loss G: 2.9297\n",
      "Epoch [4/10] Batch 58/510               Loss D: 0.0568, loss G: 2.8870\n",
      "Epoch [4/10] Batch 59/510               Loss D: 0.1199, loss G: 2.7930\n",
      "Epoch [4/10] Batch 60/510               Loss D: 0.0523, loss G: 2.7972\n",
      "Epoch [4/10] Batch 61/510               Loss D: 0.0578, loss G: 2.7823\n",
      "Epoch [4/10] Batch 62/510               Loss D: 0.0668, loss G: 2.7426\n",
      "Epoch [4/10] Batch 63/510               Loss D: 0.0833, loss G: 2.7131\n",
      "Epoch [4/10] Batch 64/510               Loss D: 0.0580, loss G: 2.6697\n",
      "Epoch [4/10] Batch 65/510               Loss D: 0.1150, loss G: 2.6685\n",
      "Epoch [4/10] Batch 66/510               Loss D: 0.0602, loss G: 2.6391\n",
      "Epoch [4/10] Batch 67/510               Loss D: 0.0607, loss G: 2.6128\n",
      "Epoch [4/10] Batch 68/510               Loss D: 0.0692, loss G: 2.6056\n",
      "Epoch [4/10] Batch 69/510               Loss D: 0.0674, loss G: 2.5852\n",
      "Epoch [4/10] Batch 70/510               Loss D: 0.0645, loss G: 2.6232\n",
      "Epoch [4/10] Batch 71/510               Loss D: 0.0780, loss G: 2.7086\n",
      "Epoch [4/10] Batch 72/510               Loss D: 0.0561, loss G: 2.6986\n",
      "Epoch [4/10] Batch 73/510               Loss D: 0.0541, loss G: 2.7305\n",
      "Epoch [4/10] Batch 74/510               Loss D: 0.6567, loss G: 2.6656\n",
      "Epoch [4/10] Batch 75/510               Loss D: 0.1245, loss G: 2.5110\n",
      "Epoch [4/10] Batch 76/510               Loss D: 0.0618, loss G: 2.4663\n",
      "Epoch [4/10] Batch 77/510               Loss D: 0.0675, loss G: 2.3954\n",
      "Epoch [4/10] Batch 78/510               Loss D: 0.0701, loss G: 2.3846\n",
      "Epoch [4/10] Batch 79/510               Loss D: 0.0687, loss G: 2.4159\n",
      "Epoch [4/10] Batch 80/510               Loss D: 0.1407, loss G: 2.3916\n",
      "Epoch [4/10] Batch 81/510               Loss D: 0.4686, loss G: 2.3212\n",
      "Epoch [4/10] Batch 82/510               Loss D: 0.0769, loss G: 2.2376\n",
      "Epoch [4/10] Batch 83/510               Loss D: 0.0786, loss G: 2.2475\n",
      "Epoch [4/10] Batch 84/510               Loss D: 0.0813, loss G: 2.2364\n",
      "Epoch [4/10] Batch 85/510               Loss D: 0.0827, loss G: 2.2482\n",
      "Epoch [4/10] Batch 86/510               Loss D: 0.1024, loss G: 2.3571\n",
      "Epoch [4/10] Batch 87/510               Loss D: 0.0766, loss G: 2.3924\n",
      "Epoch [4/10] Batch 88/510               Loss D: 0.0691, loss G: 2.4805\n",
      "Epoch [4/10] Batch 89/510               Loss D: 0.0630, loss G: 2.5879\n",
      "Epoch [4/10] Batch 90/510               Loss D: 0.0572, loss G: 2.7129\n",
      "Epoch [4/10] Batch 91/510               Loss D: 0.0540, loss G: 2.8311\n",
      "Epoch [4/10] Batch 92/510               Loss D: 0.0449, loss G: 2.9629\n",
      "Epoch [4/10] Batch 93/510               Loss D: 0.0435, loss G: 2.9975\n",
      "Epoch [4/10] Batch 94/510               Loss D: 0.6695, loss G: 2.9947\n",
      "Epoch [4/10] Batch 95/510               Loss D: 0.0436, loss G: 3.0188\n",
      "Epoch [4/10] Batch 96/510               Loss D: 0.0397, loss G: 3.0393\n",
      "Epoch [4/10] Batch 97/510               Loss D: 0.0411, loss G: 3.0172\n",
      "Epoch [4/10] Batch 98/510               Loss D: 0.0510, loss G: 2.9859\n",
      "Epoch [4/10] Batch 99/510               Loss D: 0.0447, loss G: 3.0597\n",
      "Epoch [4/10] Batch 100/510               Loss D: 0.0409, loss G: 3.0385\n",
      "Epoch [4/10] Batch 101/510               Loss D: 0.0429, loss G: 3.0056\n",
      "Epoch [4/10] Batch 102/510               Loss D: 0.1012, loss G: 2.9995\n",
      "Epoch [4/10] Batch 103/510               Loss D: 0.0449, loss G: 2.9642\n",
      "Epoch [4/10] Batch 104/510               Loss D: 0.0476, loss G: 2.9239\n",
      "Epoch [4/10] Batch 105/510               Loss D: 0.0488, loss G: 2.8898\n",
      "Epoch [4/10] Batch 106/510               Loss D: 0.0483, loss G: 2.9028\n",
      "Epoch [4/10] Batch 107/510               Loss D: 0.0522, loss G: 2.8716\n",
      "Epoch [4/10] Batch 108/510               Loss D: 0.0488, loss G: 2.9058\n",
      "Epoch [4/10] Batch 109/510               Loss D: 0.0516, loss G: 2.8371\n",
      "Epoch [4/10] Batch 110/510               Loss D: 0.0581, loss G: 2.8379\n",
      "Epoch [4/10] Batch 111/510               Loss D: 0.0556, loss G: 2.8423\n",
      "Epoch [4/10] Batch 112/510               Loss D: 0.0549, loss G: 2.8782\n",
      "Epoch [4/10] Batch 113/510               Loss D: 0.0544, loss G: 2.9139\n",
      "Epoch [4/10] Batch 114/510               Loss D: 0.0510, loss G: 2.9448\n",
      "Epoch [4/10] Batch 115/510               Loss D: 0.0501, loss G: 2.9321\n",
      "Epoch [4/10] Batch 116/510               Loss D: 0.0968, loss G: 2.9295\n",
      "Epoch [4/10] Batch 117/510               Loss D: 0.0508, loss G: 2.9561\n",
      "Epoch [4/10] Batch 118/510               Loss D: 0.0607, loss G: 2.9077\n",
      "Epoch [4/10] Batch 119/510               Loss D: 0.0540, loss G: 2.9102\n",
      "Epoch [4/10] Batch 120/510               Loss D: 0.0502, loss G: 2.9650\n",
      "Epoch [4/10] Batch 121/510               Loss D: 0.0595, loss G: 2.9145\n",
      "Epoch [4/10] Batch 122/510               Loss D: 0.0490, loss G: 2.9736\n",
      "Epoch [4/10] Batch 123/510               Loss D: 0.1631, loss G: 2.9770\n",
      "Epoch [4/10] Batch 124/510               Loss D: 0.2428, loss G: 2.8204\n",
      "Epoch [4/10] Batch 125/510               Loss D: 0.0638, loss G: 2.8400\n",
      "Epoch [4/10] Batch 126/510               Loss D: 0.0586, loss G: 2.7407\n",
      "Epoch [4/10] Batch 127/510               Loss D: 0.2979, loss G: 2.5992\n",
      "Epoch [4/10] Batch 128/510               Loss D: 0.0770, loss G: 2.4996\n",
      "Epoch [4/10] Batch 129/510               Loss D: 0.0998, loss G: 2.4063\n",
      "Epoch [4/10] Batch 130/510               Loss D: 0.1926, loss G: 2.3179\n",
      "Epoch [4/10] Batch 131/510               Loss D: 0.0892, loss G: 2.3727\n",
      "Epoch [4/10] Batch 132/510               Loss D: 0.0922, loss G: 2.3601\n",
      "Epoch [4/10] Batch 133/510               Loss D: 0.0946, loss G: 2.4002\n",
      "Epoch [4/10] Batch 134/510               Loss D: 0.1163, loss G: 2.5085\n",
      "Epoch [4/10] Batch 135/510               Loss D: 0.0802, loss G: 2.5782\n",
      "Epoch [4/10] Batch 136/510               Loss D: 0.0993, loss G: 2.6822\n",
      "Epoch [4/10] Batch 137/510               Loss D: 0.0686, loss G: 2.7535\n",
      "Epoch [4/10] Batch 138/510               Loss D: 0.0567, loss G: 2.9716\n",
      "Epoch [4/10] Batch 139/510               Loss D: 0.0533, loss G: 3.0140\n",
      "Epoch [4/10] Batch 140/510               Loss D: 0.0834, loss G: 3.0482\n",
      "Epoch [4/10] Batch 141/510               Loss D: 0.0481, loss G: 3.1285\n",
      "Epoch [4/10] Batch 142/510               Loss D: 0.0448, loss G: 3.2058\n",
      "Epoch [4/10] Batch 143/510               Loss D: 0.0475, loss G: 3.2685\n",
      "Epoch [4/10] Batch 144/510               Loss D: 1.6611, loss G: 3.0220\n",
      "Epoch [4/10] Batch 145/510               Loss D: 0.0614, loss G: 2.8667\n",
      "Epoch [4/10] Batch 146/510               Loss D: 0.0552, loss G: 2.7001\n",
      "Epoch [4/10] Batch 147/510               Loss D: 0.7036, loss G: 2.3655\n",
      "Epoch [4/10] Batch 148/510               Loss D: 0.1009, loss G: 2.1512\n",
      "Epoch [4/10] Batch 149/510               Loss D: 0.1108, loss G: 2.0266\n",
      "Epoch [4/10] Batch 150/510               Loss D: 0.1310, loss G: 1.9489\n",
      "Epoch [4/10] Batch 151/510               Loss D: 0.1436, loss G: 1.9413\n",
      "Epoch [4/10] Batch 152/510               Loss D: 0.1412, loss G: 1.9993\n",
      "Epoch [4/10] Batch 153/510               Loss D: 0.1350, loss G: 2.0802\n",
      "Epoch [4/10] Batch 154/510               Loss D: 0.1244, loss G: 2.2186\n",
      "Epoch [4/10] Batch 155/510               Loss D: 0.1114, loss G: 2.3757\n",
      "Epoch [4/10] Batch 156/510               Loss D: 0.0922, loss G: 2.5731\n",
      "Epoch [4/10] Batch 157/510               Loss D: 0.0821, loss G: 2.7190\n",
      "Epoch [4/10] Batch 158/510               Loss D: 0.1912, loss G: 2.9035\n",
      "Epoch [4/10] Batch 159/510               Loss D: 0.0576, loss G: 3.0453\n",
      "Epoch [4/10] Batch 160/510               Loss D: 0.0531, loss G: 3.1519\n",
      "Epoch [4/10] Batch 161/510               Loss D: 0.0636, loss G: 3.2031\n",
      "Epoch [4/10] Batch 162/510               Loss D: 0.0438, loss G: 3.3494\n",
      "Epoch [4/10] Batch 163/510               Loss D: 0.0563, loss G: 3.3892\n",
      "Epoch [4/10] Batch 164/510               Loss D: 0.0426, loss G: 3.3635\n",
      "Epoch [4/10] Batch 165/510               Loss D: 0.0403, loss G: 3.3879\n",
      "Epoch [4/10] Batch 166/510               Loss D: 0.0411, loss G: 3.3446\n",
      "Epoch [4/10] Batch 167/510               Loss D: 0.0395, loss G: 3.4007\n",
      "Epoch [4/10] Batch 168/510               Loss D: 0.0412, loss G: 3.3077\n",
      "Epoch [4/10] Batch 169/510               Loss D: 0.0455, loss G: 3.2593\n",
      "Epoch [4/10] Batch 170/510               Loss D: 0.0481, loss G: 3.2507\n",
      "Epoch [4/10] Batch 171/510               Loss D: 0.0465, loss G: 3.1341\n",
      "Epoch [4/10] Batch 172/510               Loss D: 0.0471, loss G: 3.1160\n",
      "Epoch [4/10] Batch 173/510               Loss D: 0.0469, loss G: 3.1009\n",
      "Epoch [4/10] Batch 174/510               Loss D: 0.0540, loss G: 3.0097\n",
      "Epoch [4/10] Batch 175/510               Loss D: 0.0499, loss G: 3.0440\n",
      "Epoch [4/10] Batch 176/510               Loss D: 0.0547, loss G: 2.9085\n",
      "Epoch [4/10] Batch 177/510               Loss D: 0.0873, loss G: 2.8509\n",
      "Epoch [4/10] Batch 178/510               Loss D: 0.0589, loss G: 2.8066\n",
      "Epoch [4/10] Batch 179/510               Loss D: 0.0839, loss G: 2.8072\n",
      "Epoch [4/10] Batch 180/510               Loss D: 0.0625, loss G: 2.7655\n",
      "Epoch [4/10] Batch 181/510               Loss D: 0.0627, loss G: 2.7684\n",
      "Epoch [4/10] Batch 182/510               Loss D: 0.0864, loss G: 2.7445\n",
      "Epoch [4/10] Batch 183/510               Loss D: 0.0863, loss G: 2.7614\n",
      "Epoch [4/10] Batch 184/510               Loss D: 0.6224, loss G: 2.6184\n",
      "Epoch [4/10] Batch 185/510               Loss D: 0.0675, loss G: 2.5657\n",
      "Epoch [4/10] Batch 186/510               Loss D: 0.0811, loss G: 2.4901\n",
      "Epoch [4/10] Batch 187/510               Loss D: 0.0787, loss G: 2.4929\n",
      "Epoch [4/10] Batch 188/510               Loss D: 0.0864, loss G: 2.4836\n",
      "Epoch [4/10] Batch 189/510               Loss D: 0.1670, loss G: 2.5128\n",
      "Epoch [4/10] Batch 190/510               Loss D: 0.0799, loss G: 2.5322\n",
      "Epoch [4/10] Batch 191/510               Loss D: 0.0754, loss G: 2.5784\n",
      "Epoch [4/10] Batch 192/510               Loss D: 0.0699, loss G: 2.6740\n",
      "Epoch [4/10] Batch 193/510               Loss D: 0.0821, loss G: 2.6786\n",
      "Epoch [4/10] Batch 194/510               Loss D: 0.0771, loss G: 2.8753\n",
      "Epoch [4/10] Batch 195/510               Loss D: 0.0666, loss G: 2.8911\n",
      "Epoch [4/10] Batch 196/510               Loss D: 0.1914, loss G: 2.9421\n",
      "Epoch [4/10] Batch 197/510               Loss D: 0.0525, loss G: 2.9435\n",
      "Epoch [4/10] Batch 198/510               Loss D: 0.0496, loss G: 3.0080\n",
      "Epoch [4/10] Batch 199/510               Loss D: 0.0476, loss G: 3.0081\n",
      "Epoch [4/10] Batch 200/510               Loss D: 0.0503, loss G: 3.0713\n",
      "Epoch [4/10] Batch 201/510               Loss D: 0.0434, loss G: 3.1174\n",
      "Epoch [4/10] Batch 202/510               Loss D: 0.0404, loss G: 3.1627\n",
      "Epoch [4/10] Batch 203/510               Loss D: 0.2452, loss G: 3.1250\n",
      "Epoch [4/10] Batch 204/510               Loss D: 0.2824, loss G: 3.0238\n",
      "Epoch [4/10] Batch 205/510               Loss D: 0.1657, loss G: 2.9479\n",
      "Epoch [4/10] Batch 206/510               Loss D: 0.0468, loss G: 2.9025\n",
      "Epoch [4/10] Batch 207/510               Loss D: 0.0483, loss G: 2.8820\n",
      "Epoch [4/10] Batch 208/510               Loss D: 0.0536, loss G: 2.7920\n",
      "Epoch [4/10] Batch 209/510               Loss D: 0.0557, loss G: 2.7946\n",
      "Epoch [4/10] Batch 210/510               Loss D: 0.0536, loss G: 2.8020\n",
      "Epoch [4/10] Batch 211/510               Loss D: 0.0564, loss G: 2.7864\n",
      "Epoch [4/10] Batch 212/510               Loss D: 0.1112, loss G: 2.8310\n",
      "Epoch [4/10] Batch 213/510               Loss D: 0.0530, loss G: 2.8581\n",
      "Epoch [4/10] Batch 214/510               Loss D: 0.0518, loss G: 2.8829\n",
      "Epoch [4/10] Batch 215/510               Loss D: 0.0515, loss G: 2.9026\n",
      "Epoch [4/10] Batch 216/510               Loss D: 0.0737, loss G: 2.9312\n",
      "Epoch [4/10] Batch 217/510               Loss D: 0.1344, loss G: 2.9512\n",
      "Epoch [4/10] Batch 218/510               Loss D: 0.2596, loss G: 2.8746\n",
      "Epoch [4/10] Batch 219/510               Loss D: 0.0488, loss G: 2.9035\n",
      "Epoch [4/10] Batch 220/510               Loss D: 0.0572, loss G: 2.8254\n",
      "Epoch [4/10] Batch 221/510               Loss D: 0.0552, loss G: 2.7730\n",
      "Epoch [4/10] Batch 222/510               Loss D: 0.0601, loss G: 2.8453\n",
      "Epoch [4/10] Batch 223/510               Loss D: 0.0765, loss G: 2.8836\n",
      "Epoch [4/10] Batch 224/510               Loss D: 0.0506, loss G: 2.9083\n",
      "Epoch [4/10] Batch 225/510               Loss D: 0.0513, loss G: 2.9004\n",
      "Epoch [4/10] Batch 226/510               Loss D: 0.0499, loss G: 2.9644\n",
      "Epoch [4/10] Batch 227/510               Loss D: 0.0507, loss G: 3.0065\n",
      "Epoch [4/10] Batch 228/510               Loss D: 0.0465, loss G: 3.0261\n",
      "Epoch [4/10] Batch 229/510               Loss D: 0.0429, loss G: 3.1067\n",
      "Epoch [4/10] Batch 230/510               Loss D: 0.0455, loss G: 3.0741\n",
      "Epoch [4/10] Batch 231/510               Loss D: 0.0619, loss G: 3.2223\n",
      "Epoch [4/10] Batch 232/510               Loss D: 0.0429, loss G: 3.2431\n",
      "Epoch [4/10] Batch 233/510               Loss D: 0.0384, loss G: 3.3282\n",
      "Epoch [4/10] Batch 234/510               Loss D: 0.0403, loss G: 3.2181\n",
      "Epoch [4/10] Batch 235/510               Loss D: 0.0389, loss G: 3.3109\n",
      "Epoch [4/10] Batch 236/510               Loss D: 0.0376, loss G: 3.2871\n",
      "Epoch [4/10] Batch 237/510               Loss D: 0.0472, loss G: 3.2544\n",
      "Epoch [4/10] Batch 238/510               Loss D: 0.0317, loss G: 3.4481\n",
      "Epoch [4/10] Batch 239/510               Loss D: 0.0455, loss G: 3.3641\n",
      "Epoch [4/10] Batch 240/510               Loss D: 0.0337, loss G: 3.3594\n",
      "Epoch [4/10] Batch 241/510               Loss D: 0.0367, loss G: 3.2937\n",
      "Epoch [4/10] Batch 242/510               Loss D: 0.0363, loss G: 3.3257\n",
      "Epoch [4/10] Batch 243/510               Loss D: 0.0533, loss G: 3.2540\n",
      "Epoch [4/10] Batch 244/510               Loss D: 0.0510, loss G: 3.3116\n",
      "Epoch [4/10] Batch 245/510               Loss D: 0.0389, loss G: 3.2374\n",
      "Epoch [4/10] Batch 246/510               Loss D: 0.0386, loss G: 3.2781\n",
      "Epoch [4/10] Batch 247/510               Loss D: 0.0406, loss G: 3.2218\n",
      "Epoch [4/10] Batch 248/510               Loss D: 0.0801, loss G: 3.1761\n",
      "Epoch [4/10] Batch 249/510               Loss D: 0.0585, loss G: 3.1220\n",
      "Epoch [4/10] Batch 250/510               Loss D: 0.2875, loss G: 3.0680\n",
      "Epoch [4/10] Batch 251/510               Loss D: 0.0495, loss G: 2.9961\n",
      "Epoch [4/10] Batch 252/510               Loss D: 0.0523, loss G: 2.9307\n",
      "Epoch [4/10] Batch 253/510               Loss D: 0.0524, loss G: 2.8824\n",
      "Epoch [4/10] Batch 254/510               Loss D: 0.0542, loss G: 2.8972\n",
      "Epoch [4/10] Batch 255/510               Loss D: 0.0581, loss G: 2.8098\n",
      "Epoch [4/10] Batch 256/510               Loss D: 1.3930, loss G: 2.7405\n",
      "Epoch [4/10] Batch 257/510               Loss D: 0.0808, loss G: 2.6071\n",
      "Epoch [4/10] Batch 258/510               Loss D: 0.0719, loss G: 2.5613\n",
      "Epoch [4/10] Batch 259/510               Loss D: 0.0981, loss G: 2.4887\n",
      "Epoch [4/10] Batch 260/510               Loss D: 0.0807, loss G: 2.5476\n",
      "Epoch [4/10] Batch 261/510               Loss D: 0.0776, loss G: 2.6056\n",
      "Epoch [4/10] Batch 262/510               Loss D: 0.0834, loss G: 2.5823\n",
      "Epoch [4/10] Batch 263/510               Loss D: 0.0791, loss G: 2.7082\n",
      "Epoch [4/10] Batch 264/510               Loss D: 0.1206, loss G: 2.6994\n",
      "Epoch [4/10] Batch 265/510               Loss D: 0.0741, loss G: 2.7151\n",
      "Epoch [4/10] Batch 266/510               Loss D: 0.0623, loss G: 2.8681\n",
      "Epoch [4/10] Batch 267/510               Loss D: 0.0571, loss G: 2.9787\n",
      "Epoch [4/10] Batch 268/510               Loss D: 0.0636, loss G: 2.9856\n",
      "Epoch [4/10] Batch 269/510               Loss D: 0.0521, loss G: 3.0872\n",
      "Epoch [4/10] Batch 270/510               Loss D: 0.0487, loss G: 3.1581\n",
      "Epoch [4/10] Batch 271/510               Loss D: 0.0548, loss G: 3.2805\n",
      "Epoch [4/10] Batch 272/510               Loss D: 0.0463, loss G: 3.3474\n",
      "Epoch [4/10] Batch 273/510               Loss D: 0.0393, loss G: 3.3828\n",
      "Epoch [4/10] Batch 274/510               Loss D: 0.0390, loss G: 3.3871\n",
      "Epoch [4/10] Batch 275/510               Loss D: 0.0375, loss G: 3.4358\n",
      "Epoch [4/10] Batch 276/510               Loss D: 0.0372, loss G: 3.4121\n",
      "Epoch [4/10] Batch 277/510               Loss D: 0.0727, loss G: 3.4601\n",
      "Epoch [4/10] Batch 278/510               Loss D: 0.0403, loss G: 3.3913\n",
      "Epoch [4/10] Batch 279/510               Loss D: 0.0885, loss G: 3.4073\n",
      "Epoch [4/10] Batch 280/510               Loss D: 0.0514, loss G: 3.2532\n",
      "Epoch [4/10] Batch 281/510               Loss D: 0.0383, loss G: 3.2669\n",
      "Epoch [4/10] Batch 282/510               Loss D: 0.0424, loss G: 3.2504\n",
      "Epoch [4/10] Batch 283/510               Loss D: 0.0743, loss G: 3.1259\n",
      "Epoch [4/10] Batch 284/510               Loss D: 0.0544, loss G: 3.0621\n",
      "Epoch [4/10] Batch 285/510               Loss D: 0.0483, loss G: 3.0499\n",
      "Epoch [4/10] Batch 286/510               Loss D: 0.0480, loss G: 3.0279\n",
      "Epoch [4/10] Batch 287/510               Loss D: 0.0598, loss G: 2.9981\n",
      "Epoch [4/10] Batch 288/510               Loss D: 0.0706, loss G: 2.9593\n",
      "Epoch [4/10] Batch 289/510               Loss D: 0.0520, loss G: 3.0296\n",
      "Epoch [4/10] Batch 290/510               Loss D: 0.0511, loss G: 2.9927\n",
      "Epoch [4/10] Batch 291/510               Loss D: 0.0760, loss G: 2.9618\n",
      "Epoch [4/10] Batch 292/510               Loss D: 0.0525, loss G: 3.0090\n",
      "Epoch [4/10] Batch 293/510               Loss D: 0.0605, loss G: 2.9670\n",
      "Epoch [4/10] Batch 294/510               Loss D: 0.0621, loss G: 3.0234\n",
      "Epoch [4/10] Batch 295/510               Loss D: 0.0513, loss G: 3.0203\n",
      "Epoch [4/10] Batch 296/510               Loss D: 0.1565, loss G: 3.0250\n",
      "Epoch [4/10] Batch 297/510               Loss D: 0.0597, loss G: 3.0148\n",
      "Epoch [4/10] Batch 298/510               Loss D: 0.0489, loss G: 2.9611\n",
      "Epoch [4/10] Batch 299/510               Loss D: 0.1349, loss G: 2.9481\n",
      "Epoch [4/10] Batch 300/510               Loss D: 0.0622, loss G: 2.9286\n",
      "Epoch [4/10] Batch 301/510               Loss D: 0.0705, loss G: 2.8968\n",
      "Epoch [4/10] Batch 302/510               Loss D: 0.0497, loss G: 2.8738\n",
      "Epoch [4/10] Batch 303/510               Loss D: 0.0503, loss G: 2.9171\n",
      "Epoch [4/10] Batch 304/510               Loss D: 0.1119, loss G: 2.8815\n",
      "Epoch [4/10] Batch 305/510               Loss D: 0.0827, loss G: 2.8672\n",
      "Epoch [4/10] Batch 306/510               Loss D: 0.0747, loss G: 2.8303\n",
      "Epoch [4/10] Batch 307/510               Loss D: 0.0510, loss G: 2.7890\n",
      "Epoch [4/10] Batch 308/510               Loss D: 0.0479, loss G: 2.8859\n",
      "Epoch [4/10] Batch 309/510               Loss D: 0.0511, loss G: 2.8422\n",
      "Epoch [4/10] Batch 310/510               Loss D: 0.0453, loss G: 2.9141\n",
      "Epoch [4/10] Batch 311/510               Loss D: 0.0440, loss G: 2.9533\n",
      "Epoch [4/10] Batch 312/510               Loss D: 0.0449, loss G: 2.9861\n",
      "Epoch [4/10] Batch 313/510               Loss D: 0.0591, loss G: 3.0261\n",
      "Epoch [4/10] Batch 314/510               Loss D: 0.0438, loss G: 3.0865\n",
      "Epoch [4/10] Batch 315/510               Loss D: 0.0379, loss G: 3.1024\n",
      "Epoch [4/10] Batch 316/510               Loss D: 0.0330, loss G: 3.1822\n",
      "Epoch [4/10] Batch 317/510               Loss D: 0.4434, loss G: 3.1133\n",
      "Epoch [4/10] Batch 318/510               Loss D: 0.0335, loss G: 3.0549\n",
      "Epoch [4/10] Batch 319/510               Loss D: 0.0427, loss G: 3.0336\n",
      "Epoch [4/10] Batch 320/510               Loss D: 0.0364, loss G: 3.0036\n",
      "Epoch [4/10] Batch 321/510               Loss D: 0.0356, loss G: 3.0233\n",
      "Epoch [4/10] Batch 322/510               Loss D: 1.1949, loss G: 2.8209\n",
      "Epoch [4/10] Batch 323/510               Loss D: 0.0554, loss G: 2.7043\n",
      "Epoch [4/10] Batch 324/510               Loss D: 0.0527, loss G: 2.5859\n",
      "Epoch [4/10] Batch 325/510               Loss D: 0.0562, loss G: 2.5563\n",
      "Epoch [4/10] Batch 326/510               Loss D: 0.0620, loss G: 2.4713\n",
      "Epoch [4/10] Batch 327/510               Loss D: 0.4031, loss G: 2.4144\n",
      "Epoch [4/10] Batch 328/510               Loss D: 0.0729, loss G: 2.3545\n",
      "Epoch [4/10] Batch 329/510               Loss D: 0.0745, loss G: 2.4152\n",
      "Epoch [4/10] Batch 330/510               Loss D: 0.0807, loss G: 2.4101\n",
      "Epoch [4/10] Batch 331/510               Loss D: 0.0804, loss G: 2.4819\n",
      "Epoch [4/10] Batch 332/510               Loss D: 0.0794, loss G: 2.5944\n",
      "Epoch [4/10] Batch 333/510               Loss D: 0.0778, loss G: 2.6431\n",
      "Epoch [4/10] Batch 334/510               Loss D: 0.0686, loss G: 2.8387\n",
      "Epoch [4/10] Batch 335/510               Loss D: 0.0644, loss G: 2.9131\n",
      "Epoch [4/10] Batch 336/510               Loss D: 0.0577, loss G: 3.0651\n",
      "Epoch [4/10] Batch 337/510               Loss D: 0.0498, loss G: 3.2285\n",
      "Epoch [4/10] Batch 338/510               Loss D: 0.0456, loss G: 3.3714\n",
      "Epoch [4/10] Batch 339/510               Loss D: 0.0420, loss G: 3.4709\n",
      "Epoch [4/10] Batch 340/510               Loss D: 0.0325, loss G: 3.7145\n",
      "Epoch [4/10] Batch 341/510               Loss D: 0.0327, loss G: 3.6909\n",
      "Epoch [4/10] Batch 342/510               Loss D: 0.0310, loss G: 3.7885\n",
      "Epoch [4/10] Batch 343/510               Loss D: 0.0266, loss G: 3.9058\n",
      "Epoch [4/10] Batch 344/510               Loss D: 0.0300, loss G: 3.9637\n",
      "Epoch [4/10] Batch 345/510               Loss D: 0.0246, loss G: 4.0101\n",
      "Epoch [4/10] Batch 346/510               Loss D: 0.0250, loss G: 4.0403\n",
      "Epoch [4/10] Batch 347/510               Loss D: 0.0234, loss G: 4.0427\n",
      "Epoch [4/10] Batch 348/510               Loss D: 0.0239, loss G: 4.0386\n",
      "Epoch [4/10] Batch 349/510               Loss D: 0.0289, loss G: 4.0589\n",
      "Epoch [4/10] Batch 350/510               Loss D: 0.0229, loss G: 4.0396\n",
      "Epoch [4/10] Batch 351/510               Loss D: 0.0318, loss G: 3.9722\n",
      "Epoch [4/10] Batch 352/510               Loss D: 0.0264, loss G: 3.8586\n",
      "Epoch [4/10] Batch 353/510               Loss D: 0.0255, loss G: 3.9100\n",
      "Epoch [4/10] Batch 354/510               Loss D: 0.0286, loss G: 3.8538\n",
      "Epoch [4/10] Batch 355/510               Loss D: 0.0328, loss G: 3.7477\n",
      "Epoch [4/10] Batch 356/510               Loss D: 0.0610, loss G: 3.8133\n",
      "Epoch [4/10] Batch 357/510               Loss D: 0.0318, loss G: 3.6918\n",
      "Epoch [4/10] Batch 358/510               Loss D: 0.0360, loss G: 3.5704\n",
      "Epoch [4/10] Batch 359/510               Loss D: 0.0341, loss G: 3.5272\n",
      "Epoch [4/10] Batch 360/510               Loss D: 0.0364, loss G: 3.4962\n",
      "Epoch [4/10] Batch 361/510               Loss D: 0.0362, loss G: 3.4832\n",
      "Epoch [4/10] Batch 362/510               Loss D: 0.0423, loss G: 3.3365\n",
      "Epoch [4/10] Batch 363/510               Loss D: 0.0423, loss G: 3.3619\n",
      "Epoch [4/10] Batch 364/510               Loss D: 0.0439, loss G: 3.2494\n",
      "Epoch [4/10] Batch 365/510               Loss D: 0.0442, loss G: 3.2218\n",
      "Epoch [4/10] Batch 366/510               Loss D: 0.1030, loss G: 3.2174\n",
      "Epoch [4/10] Batch 367/510               Loss D: 0.0464, loss G: 3.1182\n",
      "Epoch [4/10] Batch 368/510               Loss D: 0.0439, loss G: 3.1731\n",
      "Epoch [4/10] Batch 369/510               Loss D: 0.0452, loss G: 3.1495\n",
      "Epoch [4/10] Batch 370/510               Loss D: 0.0445, loss G: 3.1854\n",
      "Epoch [4/10] Batch 371/510               Loss D: 0.0708, loss G: 3.1234\n",
      "Epoch [4/10] Batch 372/510               Loss D: 0.0467, loss G: 3.1493\n",
      "Epoch [4/10] Batch 373/510               Loss D: 0.0424, loss G: 3.2195\n",
      "Epoch [4/10] Batch 374/510               Loss D: 0.0634, loss G: 3.2348\n",
      "Epoch [4/10] Batch 375/510               Loss D: 0.0497, loss G: 3.1658\n",
      "Epoch [4/10] Batch 376/510               Loss D: 0.0385, loss G: 3.2285\n",
      "Epoch [4/10] Batch 377/510               Loss D: 0.0411, loss G: 3.2036\n",
      "Epoch [4/10] Batch 378/510               Loss D: 0.0375, loss G: 3.3292\n",
      "Epoch [4/10] Batch 379/510               Loss D: 0.1341, loss G: 3.3073\n",
      "Epoch [4/10] Batch 380/510               Loss D: 0.0344, loss G: 3.2514\n",
      "Epoch [4/10] Batch 381/510               Loss D: 0.1508, loss G: 3.1682\n",
      "Epoch [4/10] Batch 382/510               Loss D: 0.0401, loss G: 3.1490\n",
      "Epoch [4/10] Batch 383/510               Loss D: 0.0378, loss G: 3.1305\n",
      "Epoch [4/10] Batch 384/510               Loss D: 0.0426, loss G: 3.1006\n",
      "Epoch [4/10] Batch 385/510               Loss D: 0.0853, loss G: 3.0918\n",
      "Epoch [4/10] Batch 386/510               Loss D: 0.5059, loss G: 3.0023\n",
      "Epoch [4/10] Batch 387/510               Loss D: 0.4106, loss G: 2.6760\n",
      "Epoch [4/10] Batch 388/510               Loss D: 0.0547, loss G: 2.4815\n",
      "Epoch [4/10] Batch 389/510               Loss D: 0.0691, loss G: 2.3489\n",
      "Epoch [4/10] Batch 390/510               Loss D: 0.0721, loss G: 2.2514\n",
      "Epoch [4/10] Batch 391/510               Loss D: 0.0764, loss G: 2.2066\n",
      "Epoch [4/10] Batch 392/510               Loss D: 0.0798, loss G: 2.2143\n",
      "Epoch [4/10] Batch 393/510               Loss D: 0.0768, loss G: 2.2801\n",
      "Epoch [4/10] Batch 394/510               Loss D: 0.0945, loss G: 2.3535\n",
      "Epoch [4/10] Batch 395/510               Loss D: 0.0754, loss G: 2.4428\n",
      "Epoch [4/10] Batch 396/510               Loss D: 0.0608, loss G: 2.5726\n",
      "Epoch [4/10] Batch 397/510               Loss D: 0.0605, loss G: 2.6839\n",
      "Epoch [4/10] Batch 398/510               Loss D: 0.0514, loss G: 2.7913\n",
      "Epoch [4/10] Batch 399/510               Loss D: 0.0427, loss G: 2.9720\n",
      "Epoch [4/10] Batch 400/510               Loss D: 0.0398, loss G: 3.0695\n",
      "Epoch [4/10] Batch 401/510               Loss D: 0.0319, loss G: 3.2648\n",
      "Epoch [4/10] Batch 402/510               Loss D: 0.0291, loss G: 3.3546\n",
      "Epoch [4/10] Batch 403/510               Loss D: 0.0260, loss G: 3.4839\n",
      "Epoch [4/10] Batch 404/510               Loss D: 0.0283, loss G: 3.5262\n",
      "Epoch [4/10] Batch 405/510               Loss D: 0.0268, loss G: 3.6609\n",
      "Epoch [4/10] Batch 406/510               Loss D: 0.0263, loss G: 3.6779\n",
      "Epoch [4/10] Batch 407/510               Loss D: 0.0195, loss G: 3.7548\n",
      "Epoch [4/10] Batch 408/510               Loss D: 0.3169, loss G: 3.6281\n",
      "Epoch [4/10] Batch 409/510               Loss D: 0.0210, loss G: 3.5592\n",
      "Epoch [4/10] Batch 410/510               Loss D: 0.0282, loss G: 3.4383\n",
      "Epoch [4/10] Batch 411/510               Loss D: 0.0448, loss G: 3.3702\n",
      "Epoch [4/10] Batch 412/510               Loss D: 0.0282, loss G: 3.3041\n",
      "Epoch [4/10] Batch 413/510               Loss D: 0.0316, loss G: 3.2221\n",
      "Epoch [4/10] Batch 414/510               Loss D: 0.0334, loss G: 3.1653\n",
      "Epoch [4/10] Batch 415/510               Loss D: 0.0426, loss G: 3.1266\n",
      "Epoch [4/10] Batch 416/510               Loss D: 0.0520, loss G: 3.1068\n",
      "Epoch [4/10] Batch 417/510               Loss D: 0.0393, loss G: 3.0495\n",
      "Epoch [4/10] Batch 418/510               Loss D: 0.0389, loss G: 3.0408\n",
      "Epoch [4/10] Batch 419/510               Loss D: 0.0410, loss G: 2.9725\n",
      "Epoch [4/10] Batch 420/510               Loss D: 0.1432, loss G: 2.9720\n",
      "Epoch [4/10] Batch 421/510               Loss D: 0.0421, loss G: 2.9471\n",
      "Epoch [4/10] Batch 422/510               Loss D: 0.0448, loss G: 2.9707\n",
      "Epoch [4/10] Batch 423/510               Loss D: 0.0466, loss G: 2.9830\n",
      "Epoch [4/10] Batch 424/510               Loss D: 0.1067, loss G: 2.9526\n",
      "Epoch [4/10] Batch 425/510               Loss D: 0.0419, loss G: 2.9227\n",
      "Epoch [4/10] Batch 426/510               Loss D: 0.0440, loss G: 2.9151\n",
      "Epoch [4/10] Batch 427/510               Loss D: 0.0411, loss G: 2.9970\n",
      "Epoch [4/10] Batch 428/510               Loss D: 0.0384, loss G: 3.0094\n",
      "Epoch [4/10] Batch 429/510               Loss D: 0.0681, loss G: 3.0121\n",
      "Epoch [4/10] Batch 430/510               Loss D: 0.0379, loss G: 3.0187\n",
      "Epoch [4/10] Batch 431/510               Loss D: 0.0368, loss G: 3.0616\n",
      "Epoch [4/10] Batch 432/510               Loss D: 0.0365, loss G: 3.0991\n",
      "Epoch [4/10] Batch 433/510               Loss D: 0.1636, loss G: 3.0764\n",
      "Epoch [4/10] Batch 434/510               Loss D: 0.0527, loss G: 3.0076\n",
      "Epoch [4/10] Batch 435/510               Loss D: 0.1385, loss G: 3.0000\n",
      "Epoch [4/10] Batch 436/510               Loss D: 0.0381, loss G: 2.9765\n",
      "Epoch [4/10] Batch 437/510               Loss D: 0.2505, loss G: 2.8496\n",
      "Epoch [4/10] Batch 438/510               Loss D: 0.1023, loss G: 2.7452\n",
      "Epoch [4/10] Batch 439/510               Loss D: 0.0482, loss G: 2.6683\n",
      "Epoch [4/10] Batch 440/510               Loss D: 0.0510, loss G: 2.6377\n",
      "Epoch [4/10] Batch 441/510               Loss D: 0.0538, loss G: 2.5676\n",
      "Epoch [4/10] Batch 442/510               Loss D: 0.0607, loss G: 2.5994\n",
      "Epoch [4/10] Batch 443/510               Loss D: 0.0745, loss G: 2.5748\n",
      "Epoch [4/10] Batch 444/510               Loss D: 0.0759, loss G: 2.6160\n",
      "Epoch [4/10] Batch 445/510               Loss D: 0.0534, loss G: 2.6359\n",
      "Epoch [4/10] Batch 446/510               Loss D: 0.0517, loss G: 2.6845\n",
      "Epoch [4/10] Batch 447/510               Loss D: 0.0470, loss G: 2.7929\n",
      "Epoch [4/10] Batch 448/510               Loss D: 0.0463, loss G: 2.8268\n",
      "Epoch [4/10] Batch 449/510               Loss D: 0.0445, loss G: 2.8917\n",
      "Epoch [4/10] Batch 450/510               Loss D: 0.0426, loss G: 2.9529\n",
      "Epoch [4/10] Batch 451/510               Loss D: 0.0593, loss G: 3.0197\n",
      "Epoch [4/10] Batch 452/510               Loss D: 0.0367, loss G: 3.0815\n",
      "Epoch [4/10] Batch 453/510               Loss D: 0.0343, loss G: 3.1636\n",
      "Epoch [4/10] Batch 454/510               Loss D: 0.0359, loss G: 3.2522\n",
      "Epoch [4/10] Batch 455/510               Loss D: 0.0316, loss G: 3.2955\n",
      "Epoch [4/10] Batch 456/510               Loss D: 0.0287, loss G: 3.3627\n",
      "Epoch [4/10] Batch 457/510               Loss D: 0.0270, loss G: 3.4464\n",
      "Epoch [4/10] Batch 458/510               Loss D: 0.0269, loss G: 3.4432\n",
      "Epoch [4/10] Batch 459/510               Loss D: 0.0293, loss G: 3.5126\n",
      "Epoch [4/10] Batch 460/510               Loss D: 0.0443, loss G: 3.4640\n",
      "Epoch [4/10] Batch 461/510               Loss D: 0.0256, loss G: 3.4770\n",
      "Epoch [4/10] Batch 462/510               Loss D: 0.0247, loss G: 3.5234\n",
      "Epoch [4/10] Batch 463/510               Loss D: 0.0429, loss G: 3.4759\n",
      "Epoch [4/10] Batch 464/510               Loss D: 0.2423, loss G: 3.3986\n",
      "Epoch [4/10] Batch 465/510               Loss D: 0.0274, loss G: 3.3575\n",
      "Epoch [4/10] Batch 466/510               Loss D: 0.0298, loss G: 3.2899\n",
      "Epoch [4/10] Batch 467/510               Loss D: 0.0420, loss G: 3.2160\n",
      "Epoch [4/10] Batch 468/510               Loss D: 0.0335, loss G: 3.1797\n",
      "Epoch [4/10] Batch 469/510               Loss D: 0.0337, loss G: 3.1471\n",
      "Epoch [4/10] Batch 470/510               Loss D: 0.0366, loss G: 3.1515\n",
      "Epoch [4/10] Batch 471/510               Loss D: 0.0660, loss G: 3.0845\n",
      "Epoch [4/10] Batch 472/510               Loss D: 0.0381, loss G: 3.0572\n",
      "Epoch [4/10] Batch 473/510               Loss D: 0.0380, loss G: 3.0502\n",
      "Epoch [4/10] Batch 474/510               Loss D: 0.0408, loss G: 3.0113\n",
      "Epoch [4/10] Batch 475/510               Loss D: 0.0395, loss G: 3.0259\n",
      "Epoch [4/10] Batch 476/510               Loss D: 0.0432, loss G: 2.9807\n",
      "Epoch [4/10] Batch 477/510               Loss D: 0.0700, loss G: 3.0046\n",
      "Epoch [4/10] Batch 478/510               Loss D: 0.9503, loss G: 2.8009\n",
      "Epoch [4/10] Batch 479/510               Loss D: 0.0661, loss G: 2.5955\n",
      "Epoch [4/10] Batch 480/510               Loss D: 0.0627, loss G: 2.5050\n",
      "Epoch [4/10] Batch 481/510               Loss D: 0.0707, loss G: 2.3737\n",
      "Epoch [4/10] Batch 482/510               Loss D: 0.0772, loss G: 2.3825\n",
      "Epoch [4/10] Batch 483/510               Loss D: 0.0767, loss G: 2.3112\n",
      "Epoch [4/10] Batch 484/510               Loss D: 0.0759, loss G: 2.3522\n",
      "Epoch [4/10] Batch 485/510               Loss D: 0.0793, loss G: 2.3356\n",
      "Epoch [4/10] Batch 486/510               Loss D: 0.0751, loss G: 2.4214\n",
      "Epoch [4/10] Batch 487/510               Loss D: 0.0692, loss G: 2.4879\n",
      "Epoch [4/10] Batch 488/510               Loss D: 0.0625, loss G: 2.6289\n",
      "Epoch [4/10] Batch 489/510               Loss D: 0.0626, loss G: 2.6740\n",
      "Epoch [4/10] Batch 490/510               Loss D: 0.0574, loss G: 2.8226\n",
      "Epoch [4/10] Batch 491/510               Loss D: 0.0507, loss G: 2.8867\n",
      "Epoch [4/10] Batch 492/510               Loss D: 0.0471, loss G: 3.0014\n",
      "Epoch [4/10] Batch 493/510               Loss D: 0.0410, loss G: 3.1146\n",
      "Epoch [4/10] Batch 494/510               Loss D: 0.1181, loss G: 3.2096\n",
      "Epoch [4/10] Batch 495/510               Loss D: 0.1151, loss G: 3.1379\n",
      "Epoch [4/10] Batch 496/510               Loss D: 0.0349, loss G: 3.2072\n",
      "Epoch [4/10] Batch 497/510               Loss D: 0.0344, loss G: 3.2927\n",
      "Epoch [4/10] Batch 498/510               Loss D: 0.0330, loss G: 3.2694\n",
      "Epoch [4/10] Batch 499/510               Loss D: 0.0348, loss G: 3.2930\n",
      "Epoch [4/10] Batch 500/510               Loss D: 0.0388, loss G: 3.2947\n",
      "Epoch [4/10] Batch 501/510               Loss D: 0.0302, loss G: 3.4040\n",
      "Epoch [4/10] Batch 502/510               Loss D: 0.0324, loss G: 3.3323\n",
      "Epoch [4/10] Batch 503/510               Loss D: 0.0314, loss G: 3.3431\n",
      "Epoch [4/10] Batch 504/510               Loss D: 0.0329, loss G: 3.3367\n",
      "Epoch [4/10] Batch 505/510               Loss D: 0.0342, loss G: 3.4020\n",
      "Epoch [4/10] Batch 506/510               Loss D: 0.0372, loss G: 3.3664\n",
      "Epoch [4/10] Batch 507/510               Loss D: 0.2865, loss G: 3.2308\n",
      "Epoch [4/10] Batch 508/510               Loss D: 0.0318, loss G: 3.2482\n",
      "Epoch [4/10] Batch 509/510               Loss D: 0.0579, loss G: 3.1196\n",
      "Epoch [5/10] Batch 0/510               Loss D: 0.0418, loss G: 3.0481\n",
      "Epoch [5/10] Batch 1/510               Loss D: 0.0411, loss G: 3.0328\n",
      "Epoch [5/10] Batch 2/510               Loss D: 0.0431, loss G: 2.9777\n",
      "Epoch [5/10] Batch 3/510               Loss D: 0.0472, loss G: 2.9117\n",
      "Epoch [5/10] Batch 4/510               Loss D: 0.0446, loss G: 2.9504\n",
      "Epoch [5/10] Batch 5/510               Loss D: 0.0452, loss G: 2.9500\n",
      "Epoch [5/10] Batch 6/510               Loss D: 0.0486, loss G: 2.8971\n",
      "Epoch [5/10] Batch 7/510               Loss D: 0.0441, loss G: 2.9748\n",
      "Epoch [5/10] Batch 8/510               Loss D: 0.0460, loss G: 2.9564\n",
      "Epoch [5/10] Batch 9/510               Loss D: 0.0444, loss G: 2.9718\n",
      "Epoch [5/10] Batch 10/510               Loss D: 0.0431, loss G: 3.0039\n",
      "Epoch [5/10] Batch 11/510               Loss D: 0.0399, loss G: 3.0787\n",
      "Epoch [5/10] Batch 12/510               Loss D: 0.0406, loss G: 3.0573\n",
      "Epoch [5/10] Batch 13/510               Loss D: 0.0379, loss G: 3.1300\n",
      "Epoch [5/10] Batch 14/510               Loss D: 0.0355, loss G: 3.2041\n",
      "Epoch [5/10] Batch 15/510               Loss D: 0.0431, loss G: 3.2183\n",
      "Epoch [5/10] Batch 16/510               Loss D: 0.0336, loss G: 3.2128\n",
      "Epoch [5/10] Batch 17/510               Loss D: 0.0452, loss G: 3.2292\n",
      "Epoch [5/10] Batch 18/510               Loss D: 0.0362, loss G: 3.2462\n",
      "Epoch [5/10] Batch 19/510               Loss D: 0.0309, loss G: 3.2706\n",
      "Epoch [5/10] Batch 20/510               Loss D: 0.0307, loss G: 3.2901\n",
      "Epoch [5/10] Batch 21/510               Loss D: 0.0829, loss G: 3.2568\n",
      "Epoch [5/10] Batch 22/510               Loss D: 0.0382, loss G: 3.2614\n",
      "Epoch [5/10] Batch 23/510               Loss D: 0.0478, loss G: 3.1872\n",
      "Epoch [5/10] Batch 24/510               Loss D: 0.0314, loss G: 3.2491\n",
      "Epoch [5/10] Batch 25/510               Loss D: 0.0306, loss G: 3.2259\n",
      "Epoch [5/10] Batch 26/510               Loss D: 0.0318, loss G: 3.2009\n",
      "Epoch [5/10] Batch 27/510               Loss D: 0.0310, loss G: 3.2302\n",
      "Epoch [5/10] Batch 28/510               Loss D: 0.4080, loss G: 3.1168\n",
      "Epoch [5/10] Batch 29/510               Loss D: 0.5283, loss G: 2.9565\n",
      "Epoch [5/10] Batch 30/510               Loss D: 0.0525, loss G: 2.8160\n",
      "Epoch [5/10] Batch 31/510               Loss D: 0.0593, loss G: 2.6801\n",
      "Epoch [5/10] Batch 32/510               Loss D: 0.0574, loss G: 2.6190\n",
      "Epoch [5/10] Batch 33/510               Loss D: 0.0583, loss G: 2.5335\n",
      "Epoch [5/10] Batch 34/510               Loss D: 0.0648, loss G: 2.4816\n",
      "Epoch [5/10] Batch 35/510               Loss D: 0.0659, loss G: 2.4977\n",
      "Epoch [5/10] Batch 36/510               Loss D: 0.0662, loss G: 2.5388\n",
      "Epoch [5/10] Batch 37/510               Loss D: 0.0653, loss G: 2.5680\n",
      "Epoch [5/10] Batch 38/510               Loss D: 0.0615, loss G: 2.6604\n",
      "Epoch [5/10] Batch 39/510               Loss D: 0.0597, loss G: 2.6876\n",
      "Epoch [5/10] Batch 40/510               Loss D: 0.4460, loss G: 2.7116\n",
      "Epoch [5/10] Batch 41/510               Loss D: 0.0555, loss G: 2.6732\n",
      "Epoch [5/10] Batch 42/510               Loss D: 0.0521, loss G: 2.7256\n",
      "Epoch [5/10] Batch 43/510               Loss D: 0.0536, loss G: 2.6824\n",
      "Epoch [5/10] Batch 44/510               Loss D: 0.0495, loss G: 2.7909\n",
      "Epoch [5/10] Batch 45/510               Loss D: 0.0507, loss G: 2.8169\n",
      "Epoch [5/10] Batch 46/510               Loss D: 0.0487, loss G: 2.8389\n",
      "Epoch [5/10] Batch 47/510               Loss D: 0.0460, loss G: 2.9163\n",
      "Epoch [5/10] Batch 48/510               Loss D: 0.0446, loss G: 2.9757\n",
      "Epoch [5/10] Batch 49/510               Loss D: 0.0440, loss G: 2.9962\n",
      "Epoch [5/10] Batch 50/510               Loss D: 0.0540, loss G: 3.0570\n",
      "Epoch [5/10] Batch 51/510               Loss D: 0.0390, loss G: 3.1430\n",
      "Epoch [5/10] Batch 52/510               Loss D: 0.0412, loss G: 3.1037\n",
      "Epoch [5/10] Batch 53/510               Loss D: 0.0405, loss G: 3.1528\n",
      "Epoch [5/10] Batch 54/510               Loss D: 0.0388, loss G: 3.2421\n",
      "Epoch [5/10] Batch 55/510               Loss D: 0.0379, loss G: 3.2245\n",
      "Epoch [5/10] Batch 56/510               Loss D: 0.0372, loss G: 3.2325\n",
      "Epoch [5/10] Batch 57/510               Loss D: 0.0393, loss G: 3.1606\n",
      "Epoch [5/10] Batch 58/510               Loss D: 0.0378, loss G: 3.2607\n",
      "Epoch [5/10] Batch 59/510               Loss D: 0.0361, loss G: 3.2385\n",
      "Epoch [5/10] Batch 60/510               Loss D: 0.0376, loss G: 3.2313\n",
      "Epoch [5/10] Batch 61/510               Loss D: 0.0418, loss G: 3.1790\n",
      "Epoch [5/10] Batch 62/510               Loss D: 0.2170, loss G: 3.1425\n",
      "Epoch [5/10] Batch 63/510               Loss D: 0.0396, loss G: 3.0982\n",
      "Epoch [5/10] Batch 64/510               Loss D: 0.0414, loss G: 3.0817\n",
      "Epoch [5/10] Batch 65/510               Loss D: 0.0447, loss G: 2.9964\n",
      "Epoch [5/10] Batch 66/510               Loss D: 0.0476, loss G: 2.9105\n",
      "Epoch [5/10] Batch 67/510               Loss D: 0.0464, loss G: 2.9313\n",
      "Epoch [5/10] Batch 68/510               Loss D: 0.0563, loss G: 2.8689\n",
      "Epoch [5/10] Batch 69/510               Loss D: 0.0488, loss G: 2.8770\n",
      "Epoch [5/10] Batch 70/510               Loss D: 0.0539, loss G: 2.7989\n",
      "Epoch [5/10] Batch 71/510               Loss D: 0.0554, loss G: 2.7691\n",
      "Epoch [5/10] Batch 72/510               Loss D: 0.0570, loss G: 2.8736\n",
      "Epoch [5/10] Batch 73/510               Loss D: 0.0542, loss G: 2.8200\n",
      "Epoch [5/10] Batch 74/510               Loss D: 0.0585, loss G: 2.7482\n",
      "Epoch [5/10] Batch 75/510               Loss D: 0.0541, loss G: 2.8216\n",
      "Epoch [5/10] Batch 76/510               Loss D: 0.0543, loss G: 2.8463\n",
      "Epoch [5/10] Batch 77/510               Loss D: 0.0521, loss G: 2.8893\n",
      "Epoch [5/10] Batch 78/510               Loss D: 0.0508, loss G: 2.9096\n",
      "Epoch [5/10] Batch 79/510               Loss D: 0.0517, loss G: 2.8913\n",
      "Epoch [5/10] Batch 80/510               Loss D: 0.0496, loss G: 2.9289\n",
      "Epoch [5/10] Batch 81/510               Loss D: 0.0466, loss G: 2.9967\n",
      "Epoch [5/10] Batch 82/510               Loss D: 0.0453, loss G: 3.0243\n",
      "Epoch [5/10] Batch 83/510               Loss D: 0.0449, loss G: 3.0669\n",
      "Epoch [5/10] Batch 84/510               Loss D: 0.0487, loss G: 3.1038\n",
      "Epoch [5/10] Batch 85/510               Loss D: 0.0443, loss G: 3.0924\n",
      "Epoch [5/10] Batch 86/510               Loss D: 0.0640, loss G: 3.0920\n",
      "Epoch [5/10] Batch 87/510               Loss D: 0.0487, loss G: 3.0080\n",
      "Epoch [5/10] Batch 88/510               Loss D: 0.0704, loss G: 3.1226\n",
      "Epoch [5/10] Batch 89/510               Loss D: 0.0504, loss G: 3.0949\n",
      "Epoch [5/10] Batch 90/510               Loss D: 0.0899, loss G: 3.0784\n",
      "Epoch [5/10] Batch 91/510               Loss D: 0.0674, loss G: 3.0428\n",
      "Epoch [5/10] Batch 92/510               Loss D: 0.0469, loss G: 3.0430\n",
      "Epoch [5/10] Batch 93/510               Loss D: 0.0543, loss G: 2.9569\n",
      "Epoch [5/10] Batch 94/510               Loss D: 0.0522, loss G: 2.9641\n",
      "Epoch [5/10] Batch 95/510               Loss D: 0.0558, loss G: 3.0301\n",
      "Epoch [5/10] Batch 96/510               Loss D: 0.0503, loss G: 3.0346\n",
      "Epoch [5/10] Batch 97/510               Loss D: 0.0495, loss G: 3.0586\n",
      "Epoch [5/10] Batch 98/510               Loss D: 0.1232, loss G: 2.9742\n",
      "Epoch [5/10] Batch 99/510               Loss D: 0.0635, loss G: 2.9946\n",
      "Epoch [5/10] Batch 100/510               Loss D: 0.1593, loss G: 2.8949\n",
      "Epoch [5/10] Batch 101/510               Loss D: 0.4431, loss G: 2.7404\n",
      "Epoch [5/10] Batch 102/510               Loss D: 0.0657, loss G: 2.6449\n",
      "Epoch [5/10] Batch 103/510               Loss D: 0.0808, loss G: 2.5144\n",
      "Epoch [5/10] Batch 104/510               Loss D: 0.0810, loss G: 2.4978\n",
      "Epoch [5/10] Batch 105/510               Loss D: 0.0845, loss G: 2.4462\n",
      "Epoch [5/10] Batch 106/510               Loss D: 0.0956, loss G: 2.4364\n",
      "Epoch [5/10] Batch 107/510               Loss D: 0.1050, loss G: 2.4361\n",
      "Epoch [5/10] Batch 108/510               Loss D: 0.0811, loss G: 2.6051\n",
      "Epoch [5/10] Batch 109/510               Loss D: 0.1078, loss G: 2.6464\n",
      "Epoch [5/10] Batch 110/510               Loss D: 0.1633, loss G: 2.6764\n",
      "Epoch [5/10] Batch 111/510               Loss D: 0.0792, loss G: 2.6658\n",
      "Epoch [5/10] Batch 112/510               Loss D: 0.0685, loss G: 2.8237\n",
      "Epoch [5/10] Batch 113/510               Loss D: 0.3171, loss G: 2.7627\n",
      "Epoch [5/10] Batch 114/510               Loss D: 1.3542, loss G: 2.6779\n",
      "Epoch [5/10] Batch 115/510               Loss D: 0.0896, loss G: 2.5205\n",
      "Epoch [5/10] Batch 116/510               Loss D: 0.0855, loss G: 2.4601\n",
      "Epoch [5/10] Batch 117/510               Loss D: 0.0955, loss G: 2.3692\n",
      "Epoch [5/10] Batch 118/510               Loss D: 0.0983, loss G: 2.3726\n",
      "Epoch [5/10] Batch 119/510               Loss D: 0.0945, loss G: 2.4409\n",
      "Epoch [5/10] Batch 120/510               Loss D: 0.0993, loss G: 2.4676\n",
      "Epoch [5/10] Batch 121/510               Loss D: 0.0920, loss G: 2.5259\n",
      "Epoch [5/10] Batch 122/510               Loss D: 0.0800, loss G: 2.6620\n",
      "Epoch [5/10] Batch 123/510               Loss D: 0.0799, loss G: 2.7131\n",
      "Epoch [5/10] Batch 124/510               Loss D: 0.3266, loss G: 2.8247\n",
      "Epoch [5/10] Batch 125/510               Loss D: 0.0670, loss G: 2.8930\n",
      "Epoch [5/10] Batch 126/510               Loss D: 0.0585, loss G: 2.9315\n",
      "Epoch [5/10] Batch 127/510               Loss D: 0.0590, loss G: 2.9786\n",
      "Epoch [5/10] Batch 128/510               Loss D: 0.0531, loss G: 3.1172\n",
      "Epoch [5/10] Batch 129/510               Loss D: 0.0523, loss G: 3.1572\n",
      "Epoch [5/10] Batch 130/510               Loss D: 0.0647, loss G: 3.2034\n",
      "Epoch [5/10] Batch 131/510               Loss D: 0.0460, loss G: 3.2656\n",
      "Epoch [5/10] Batch 132/510               Loss D: 0.0474, loss G: 3.2737\n",
      "Epoch [5/10] Batch 133/510               Loss D: 0.0364, loss G: 3.5137\n",
      "Epoch [5/10] Batch 134/510               Loss D: 0.0391, loss G: 3.4782\n",
      "Epoch [5/10] Batch 135/510               Loss D: 0.0397, loss G: 3.4616\n",
      "Epoch [5/10] Batch 136/510               Loss D: 0.0369, loss G: 3.5356\n",
      "Epoch [5/10] Batch 137/510               Loss D: 0.0386, loss G: 3.4415\n",
      "Epoch [5/10] Batch 138/510               Loss D: 0.0368, loss G: 3.5276\n",
      "Epoch [5/10] Batch 139/510               Loss D: 0.0405, loss G: 3.4753\n",
      "Epoch [5/10] Batch 140/510               Loss D: 0.0364, loss G: 3.4997\n",
      "Epoch [5/10] Batch 141/510               Loss D: 0.0397, loss G: 3.4695\n",
      "Epoch [5/10] Batch 142/510               Loss D: 0.0480, loss G: 3.4304\n",
      "Epoch [5/10] Batch 143/510               Loss D: 0.0622, loss G: 3.4131\n",
      "Epoch [5/10] Batch 144/510               Loss D: 0.0403, loss G: 3.3943\n",
      "Epoch [5/10] Batch 145/510               Loss D: 0.0438, loss G: 3.3735\n",
      "Epoch [5/10] Batch 146/510               Loss D: 0.0438, loss G: 3.3474\n",
      "Epoch [5/10] Batch 147/510               Loss D: 0.0436, loss G: 3.3319\n",
      "Epoch [5/10] Batch 148/510               Loss D: 0.0610, loss G: 3.2841\n",
      "Epoch [5/10] Batch 149/510               Loss D: 0.0452, loss G: 3.2189\n",
      "Epoch [5/10] Batch 150/510               Loss D: 0.0541, loss G: 3.3033\n",
      "Epoch [5/10] Batch 151/510               Loss D: 0.0421, loss G: 3.3059\n",
      "Epoch [5/10] Batch 152/510               Loss D: 0.0511, loss G: 3.1989\n",
      "Epoch [5/10] Batch 153/510               Loss D: 0.0401, loss G: 3.3858\n",
      "Epoch [5/10] Batch 154/510               Loss D: 0.0410, loss G: 3.3623\n",
      "Epoch [5/10] Batch 155/510               Loss D: 0.0421, loss G: 3.3161\n",
      "Epoch [5/10] Batch 156/510               Loss D: 0.0587, loss G: 3.3483\n",
      "Epoch [5/10] Batch 157/510               Loss D: 0.0897, loss G: 3.2131\n",
      "Epoch [5/10] Batch 158/510               Loss D: 0.3371, loss G: 3.1912\n",
      "Epoch [5/10] Batch 159/510               Loss D: 0.0477, loss G: 3.0389\n",
      "Epoch [5/10] Batch 160/510               Loss D: 0.0482, loss G: 3.0004\n",
      "Epoch [5/10] Batch 161/510               Loss D: 0.1268, loss G: 2.8468\n",
      "Epoch [5/10] Batch 162/510               Loss D: 0.0575, loss G: 2.8157\n",
      "Epoch [5/10] Batch 163/510               Loss D: 0.1285, loss G: 2.8285\n",
      "Epoch [5/10] Batch 164/510               Loss D: 0.0635, loss G: 2.7431\n",
      "Epoch [5/10] Batch 165/510               Loss D: 0.0777, loss G: 2.7603\n",
      "Epoch [5/10] Batch 166/510               Loss D: 0.0644, loss G: 2.7815\n",
      "Epoch [5/10] Batch 167/510               Loss D: 0.9040, loss G: 2.6783\n",
      "Epoch [5/10] Batch 168/510               Loss D: 0.0696, loss G: 2.5569\n",
      "Epoch [5/10] Batch 169/510               Loss D: 0.0754, loss G: 2.4894\n",
      "Epoch [5/10] Batch 170/510               Loss D: 0.1062, loss G: 2.4381\n",
      "Epoch [5/10] Batch 171/510               Loss D: 0.0808, loss G: 2.5377\n",
      "Epoch [5/10] Batch 172/510               Loss D: 0.0819, loss G: 2.5362\n",
      "Epoch [5/10] Batch 173/510               Loss D: 0.0789, loss G: 2.6005\n",
      "Epoch [5/10] Batch 174/510               Loss D: 0.0732, loss G: 2.7093\n",
      "Epoch [5/10] Batch 175/510               Loss D: 0.0625, loss G: 2.8673\n",
      "Epoch [5/10] Batch 176/510               Loss D: 0.0752, loss G: 2.9304\n",
      "Epoch [5/10] Batch 177/510               Loss D: 0.0532, loss G: 3.0827\n",
      "Epoch [5/10] Batch 178/510               Loss D: 0.0514, loss G: 3.2073\n",
      "Epoch [5/10] Batch 179/510               Loss D: 0.0424, loss G: 3.4005\n",
      "Epoch [5/10] Batch 180/510               Loss D: 0.0394, loss G: 3.5191\n",
      "Epoch [5/10] Batch 181/510               Loss D: 0.1394, loss G: 3.5422\n",
      "Epoch [5/10] Batch 182/510               Loss D: 0.0341, loss G: 3.6768\n",
      "Epoch [5/10] Batch 183/510               Loss D: 0.0295, loss G: 3.7944\n",
      "Epoch [5/10] Batch 184/510               Loss D: 0.0299, loss G: 3.8343\n",
      "Epoch [5/10] Batch 185/510               Loss D: 0.0335, loss G: 3.7158\n",
      "Epoch [5/10] Batch 186/510               Loss D: 0.0643, loss G: 3.9300\n",
      "Epoch [5/10] Batch 187/510               Loss D: 0.0265, loss G: 3.9028\n",
      "Epoch [5/10] Batch 188/510               Loss D: 0.0290, loss G: 3.8885\n",
      "Epoch [5/10] Batch 189/510               Loss D: 0.0278, loss G: 3.8847\n",
      "Epoch [5/10] Batch 190/510               Loss D: 0.0337, loss G: 3.8090\n",
      "Epoch [5/10] Batch 191/510               Loss D: 0.0288, loss G: 3.8575\n",
      "Epoch [5/10] Batch 192/510               Loss D: 0.0280, loss G: 3.8809\n",
      "Epoch [5/10] Batch 193/510               Loss D: 0.0358, loss G: 3.7666\n",
      "Epoch [5/10] Batch 194/510               Loss D: 0.0336, loss G: 3.8057\n",
      "Epoch [5/10] Batch 195/510               Loss D: 0.0420, loss G: 3.7482\n",
      "Epoch [5/10] Batch 196/510               Loss D: 0.0333, loss G: 3.7149\n",
      "Epoch [5/10] Batch 197/510               Loss D: 0.0305, loss G: 3.8010\n",
      "Epoch [5/10] Batch 198/510               Loss D: 0.0330, loss G: 3.7791\n",
      "Epoch [5/10] Batch 199/510               Loss D: 0.0350, loss G: 3.6651\n",
      "Epoch [5/10] Batch 200/510               Loss D: 0.0329, loss G: 3.7081\n",
      "Epoch [5/10] Batch 201/510               Loss D: 0.0348, loss G: 3.6699\n",
      "Epoch [5/10] Batch 202/510               Loss D: 0.2108, loss G: 3.6575\n",
      "Epoch [5/10] Batch 203/510               Loss D: 0.0419, loss G: 3.5409\n",
      "Epoch [5/10] Batch 204/510               Loss D: 0.0368, loss G: 3.5343\n",
      "Epoch [5/10] Batch 205/510               Loss D: 0.0398, loss G: 3.4295\n",
      "Epoch [5/10] Batch 206/510               Loss D: 0.0427, loss G: 3.4022\n",
      "Epoch [5/10] Batch 207/510               Loss D: 0.0457, loss G: 3.3824\n",
      "Epoch [5/10] Batch 208/510               Loss D: 0.0434, loss G: 3.3780\n",
      "Epoch [5/10] Batch 209/510               Loss D: 0.0929, loss G: 3.3808\n",
      "Epoch [5/10] Batch 210/510               Loss D: 0.0798, loss G: 3.2741\n",
      "Epoch [5/10] Batch 211/510               Loss D: 0.0433, loss G: 3.3920\n",
      "Epoch [5/10] Batch 212/510               Loss D: 0.0443, loss G: 3.3890\n",
      "Epoch [5/10] Batch 213/510               Loss D: 0.0493, loss G: 3.3562\n",
      "Epoch [5/10] Batch 214/510               Loss D: 0.0498, loss G: 3.3947\n",
      "Epoch [5/10] Batch 215/510               Loss D: 0.0493, loss G: 3.4818\n",
      "Epoch [5/10] Batch 216/510               Loss D: 0.0546, loss G: 3.4625\n",
      "Epoch [5/10] Batch 217/510               Loss D: 0.1311, loss G: 3.4696\n",
      "Epoch [5/10] Batch 218/510               Loss D: 0.0634, loss G: 3.4163\n",
      "Epoch [5/10] Batch 219/510               Loss D: 0.0404, loss G: 3.4585\n",
      "Epoch [5/10] Batch 220/510               Loss D: 0.0408, loss G: 3.4675\n",
      "Epoch [5/10] Batch 221/510               Loss D: 0.0427, loss G: 3.5209\n",
      "Epoch [5/10] Batch 222/510               Loss D: 0.2043, loss G: 3.4834\n",
      "Epoch [5/10] Batch 223/510               Loss D: 0.0550, loss G: 3.3412\n",
      "Epoch [5/10] Batch 224/510               Loss D: 0.0433, loss G: 3.4744\n",
      "Epoch [5/10] Batch 225/510               Loss D: 0.0587, loss G: 3.3904\n",
      "Epoch [5/10] Batch 226/510               Loss D: 0.0475, loss G: 3.3641\n",
      "Epoch [5/10] Batch 227/510               Loss D: 0.1069, loss G: 3.3038\n",
      "Epoch [5/10] Batch 228/510               Loss D: 0.0391, loss G: 3.4365\n",
      "Epoch [5/10] Batch 229/510               Loss D: 0.0433, loss G: 3.3492\n",
      "Epoch [5/10] Batch 230/510               Loss D: 0.0418, loss G: 3.3499\n",
      "Epoch [5/10] Batch 231/510               Loss D: 0.0444, loss G: 3.3399\n",
      "Epoch [5/10] Batch 232/510               Loss D: 0.0403, loss G: 3.4782\n",
      "Epoch [5/10] Batch 233/510               Loss D: 0.0362, loss G: 3.5168\n",
      "Epoch [5/10] Batch 234/510               Loss D: 0.0359, loss G: 3.5577\n",
      "Epoch [5/10] Batch 235/510               Loss D: 0.0413, loss G: 3.6031\n",
      "Epoch [5/10] Batch 236/510               Loss D: 0.0323, loss G: 3.6390\n",
      "Epoch [5/10] Batch 237/510               Loss D: 0.0292, loss G: 3.7007\n",
      "Epoch [5/10] Batch 238/510               Loss D: 0.0267, loss G: 3.7622\n",
      "Epoch [5/10] Batch 239/510               Loss D: 0.0262, loss G: 3.7817\n",
      "Epoch [5/10] Batch 240/510               Loss D: 0.0244, loss G: 3.8584\n",
      "Epoch [5/10] Batch 241/510               Loss D: 0.0251, loss G: 3.8525\n",
      "Epoch [5/10] Batch 242/510               Loss D: 0.1227, loss G: 3.8101\n",
      "Epoch [5/10] Batch 243/510               Loss D: 0.8672, loss G: 3.6285\n",
      "Epoch [5/10] Batch 244/510               Loss D: 1.0049, loss G: 3.3221\n",
      "Epoch [5/10] Batch 245/510               Loss D: 0.0371, loss G: 3.1664\n",
      "Epoch [5/10] Batch 246/510               Loss D: 0.0465, loss G: 2.9723\n",
      "Epoch [5/10] Batch 247/510               Loss D: 0.0517, loss G: 2.8908\n",
      "Epoch [5/10] Batch 248/510               Loss D: 0.0532, loss G: 2.8923\n",
      "Epoch [5/10] Batch 249/510               Loss D: 0.0630, loss G: 2.7539\n",
      "Epoch [5/10] Batch 250/510               Loss D: 0.0569, loss G: 2.9043\n",
      "Epoch [5/10] Batch 251/510               Loss D: 0.0609, loss G: 2.8268\n",
      "Epoch [5/10] Batch 252/510               Loss D: 0.0553, loss G: 2.9971\n",
      "Epoch [5/10] Batch 253/510               Loss D: 0.0538, loss G: 2.9521\n",
      "Epoch [5/10] Batch 254/510               Loss D: 0.0455, loss G: 3.1347\n",
      "Epoch [5/10] Batch 255/510               Loss D: 0.0411, loss G: 3.2900\n",
      "Epoch [5/10] Batch 256/510               Loss D: 0.1158, loss G: 3.3898\n",
      "Epoch [5/10] Batch 257/510               Loss D: 0.0332, loss G: 3.4292\n",
      "Epoch [5/10] Batch 258/510               Loss D: 0.0273, loss G: 3.6668\n",
      "Epoch [5/10] Batch 259/510               Loss D: 0.0255, loss G: 3.6919\n",
      "Epoch [5/10] Batch 260/510               Loss D: 0.1973, loss G: 3.8295\n",
      "Epoch [5/10] Batch 261/510               Loss D: 0.0218, loss G: 3.8113\n",
      "Epoch [5/10] Batch 262/510               Loss D: 0.0360, loss G: 3.8133\n",
      "Epoch [5/10] Batch 263/510               Loss D: 0.0235, loss G: 3.8333\n",
      "Epoch [5/10] Batch 264/510               Loss D: 0.0205, loss G: 3.8559\n",
      "Epoch [5/10] Batch 265/510               Loss D: 0.0209, loss G: 3.7949\n",
      "Epoch [5/10] Batch 266/510               Loss D: 0.0190, loss G: 3.8845\n",
      "Epoch [5/10] Batch 267/510               Loss D: 0.0190, loss G: 3.9110\n",
      "Epoch [5/10] Batch 268/510               Loss D: 0.0206, loss G: 3.8007\n",
      "Epoch [5/10] Batch 269/510               Loss D: 0.0217, loss G: 3.8002\n",
      "Epoch [5/10] Batch 270/510               Loss D: 0.0206, loss G: 3.7965\n",
      "Epoch [5/10] Batch 271/510               Loss D: 0.0416, loss G: 3.8295\n",
      "Epoch [5/10] Batch 272/510               Loss D: 0.0211, loss G: 3.7666\n",
      "Epoch [5/10] Batch 273/510               Loss D: 0.0212, loss G: 3.7964\n",
      "Epoch [5/10] Batch 274/510               Loss D: 0.0214, loss G: 3.7806\n",
      "Epoch [5/10] Batch 275/510               Loss D: 0.0214, loss G: 3.7951\n",
      "Epoch [5/10] Batch 276/510               Loss D: 0.0242, loss G: 3.6753\n",
      "Epoch [5/10] Batch 277/510               Loss D: 0.0893, loss G: 3.7156\n",
      "Epoch [5/10] Batch 278/510               Loss D: 0.0228, loss G: 3.7127\n",
      "Epoch [5/10] Batch 279/510               Loss D: 0.0239, loss G: 3.7230\n",
      "Epoch [5/10] Batch 280/510               Loss D: 0.0249, loss G: 3.6279\n",
      "Epoch [5/10] Batch 281/510               Loss D: 0.0262, loss G: 3.6089\n",
      "Epoch [5/10] Batch 282/510               Loss D: 0.0259, loss G: 3.6178\n",
      "Epoch [5/10] Batch 283/510               Loss D: 0.0272, loss G: 3.6733\n",
      "Epoch [5/10] Batch 284/510               Loss D: 0.0261, loss G: 3.6268\n",
      "Epoch [5/10] Batch 285/510               Loss D: 0.0653, loss G: 3.5742\n",
      "Epoch [5/10] Batch 286/510               Loss D: 0.0272, loss G: 3.5885\n",
      "Epoch [5/10] Batch 287/510               Loss D: 0.0270, loss G: 3.6172\n",
      "Epoch [5/10] Batch 288/510               Loss D: 0.0325, loss G: 3.6583\n",
      "Epoch [5/10] Batch 289/510               Loss D: 0.0269, loss G: 3.6277\n",
      "Epoch [5/10] Batch 290/510               Loss D: 0.0275, loss G: 3.6065\n",
      "Epoch [5/10] Batch 291/510               Loss D: 0.0265, loss G: 3.6436\n",
      "Epoch [5/10] Batch 292/510               Loss D: 0.0596, loss G: 3.6073\n",
      "Epoch [5/10] Batch 293/510               Loss D: 0.0263, loss G: 3.6682\n",
      "Epoch [5/10] Batch 294/510               Loss D: 0.0264, loss G: 3.6749\n",
      "Epoch [5/10] Batch 295/510               Loss D: 0.0324, loss G: 3.6434\n",
      "Epoch [5/10] Batch 296/510               Loss D: 0.0272, loss G: 3.6735\n",
      "Epoch [5/10] Batch 297/510               Loss D: 0.0243, loss G: 3.7440\n",
      "Epoch [5/10] Batch 298/510               Loss D: 0.0406, loss G: 3.7025\n",
      "Epoch [5/10] Batch 299/510               Loss D: 0.0244, loss G: 3.7498\n",
      "Epoch [5/10] Batch 300/510               Loss D: 0.0247, loss G: 3.7497\n",
      "Epoch [5/10] Batch 301/510               Loss D: 0.0264, loss G: 3.7724\n",
      "Epoch [5/10] Batch 302/510               Loss D: 0.0259, loss G: 3.7916\n",
      "Epoch [5/10] Batch 303/510               Loss D: 0.0242, loss G: 3.7412\n",
      "Epoch [5/10] Batch 304/510               Loss D: 0.0267, loss G: 3.8258\n",
      "Epoch [5/10] Batch 305/510               Loss D: 0.0244, loss G: 3.7694\n",
      "Epoch [5/10] Batch 306/510               Loss D: 0.0230, loss G: 3.8084\n",
      "Epoch [5/10] Batch 307/510               Loss D: 0.0211, loss G: 3.8602\n",
      "Epoch [5/10] Batch 308/510               Loss D: 0.0223, loss G: 3.7983\n",
      "Epoch [5/10] Batch 309/510               Loss D: 0.0221, loss G: 3.8298\n",
      "Epoch [5/10] Batch 310/510               Loss D: 0.0223, loss G: 3.8264\n",
      "Epoch [5/10] Batch 311/510               Loss D: 0.9713, loss G: 3.6497\n",
      "Epoch [5/10] Batch 312/510               Loss D: 0.0254, loss G: 3.5898\n",
      "Epoch [5/10] Batch 313/510               Loss D: 0.0329, loss G: 3.3977\n",
      "Epoch [5/10] Batch 314/510               Loss D: 0.0342, loss G: 3.3812\n",
      "Epoch [5/10] Batch 315/510               Loss D: 0.0381, loss G: 3.2897\n",
      "Epoch [5/10] Batch 316/510               Loss D: 0.0931, loss G: 3.1907\n",
      "Epoch [5/10] Batch 317/510               Loss D: 0.0476, loss G: 3.1893\n",
      "Epoch [5/10] Batch 318/510               Loss D: 0.0487, loss G: 3.2114\n",
      "Epoch [5/10] Batch 319/510               Loss D: 0.0491, loss G: 3.2231\n",
      "Epoch [5/10] Batch 320/510               Loss D: 0.0528, loss G: 3.2757\n",
      "Epoch [5/10] Batch 321/510               Loss D: 0.0683, loss G: 3.3264\n",
      "Epoch [5/10] Batch 322/510               Loss D: 0.0450, loss G: 3.4154\n",
      "Epoch [5/10] Batch 323/510               Loss D: 0.0649, loss G: 3.3606\n",
      "Epoch [5/10] Batch 324/510               Loss D: 0.0440, loss G: 3.5225\n",
      "Epoch [5/10] Batch 325/510               Loss D: 0.0407, loss G: 3.5873\n",
      "Epoch [5/10] Batch 326/510               Loss D: 0.0351, loss G: 3.6984\n",
      "Epoch [5/10] Batch 327/510               Loss D: 0.0433, loss G: 3.6767\n",
      "Epoch [5/10] Batch 328/510               Loss D: 0.0362, loss G: 3.7213\n",
      "Epoch [5/10] Batch 329/510               Loss D: 0.0305, loss G: 3.8539\n",
      "Epoch [5/10] Batch 330/510               Loss D: 0.0285, loss G: 3.9659\n",
      "Epoch [5/10] Batch 331/510               Loss D: 0.0388, loss G: 4.0060\n",
      "Epoch [5/10] Batch 332/510               Loss D: 0.0290, loss G: 3.9117\n",
      "Epoch [5/10] Batch 333/510               Loss D: 0.0247, loss G: 4.0497\n",
      "Epoch [5/10] Batch 334/510               Loss D: 0.0247, loss G: 4.0277\n",
      "Epoch [5/10] Batch 335/510               Loss D: 0.0496, loss G: 4.0319\n",
      "Epoch [5/10] Batch 336/510               Loss D: 0.0215, loss G: 4.1132\n",
      "Epoch [5/10] Batch 337/510               Loss D: 0.0237, loss G: 4.0392\n",
      "Epoch [5/10] Batch 338/510               Loss D: 0.0226, loss G: 4.0257\n",
      "Epoch [5/10] Batch 339/510               Loss D: 0.0327, loss G: 3.9901\n",
      "Epoch [5/10] Batch 340/510               Loss D: 0.0261, loss G: 3.9974\n",
      "Epoch [5/10] Batch 341/510               Loss D: 0.0264, loss G: 4.0118\n",
      "Epoch [5/10] Batch 342/510               Loss D: 0.0248, loss G: 3.9256\n",
      "Epoch [5/10] Batch 343/510               Loss D: 0.0250, loss G: 3.8532\n",
      "Epoch [5/10] Batch 344/510               Loss D: 0.0270, loss G: 3.8535\n",
      "Epoch [5/10] Batch 345/510               Loss D: 0.0277, loss G: 3.8364\n",
      "Epoch [5/10] Batch 346/510               Loss D: 0.1121, loss G: 3.7684\n",
      "Epoch [5/10] Batch 347/510               Loss D: 0.0305, loss G: 3.7614\n",
      "Epoch [5/10] Batch 348/510               Loss D: 0.0349, loss G: 3.6403\n",
      "Epoch [5/10] Batch 349/510               Loss D: 0.0343, loss G: 3.6670\n",
      "Epoch [5/10] Batch 350/510               Loss D: 0.0688, loss G: 3.5114\n",
      "Epoch [5/10] Batch 351/510               Loss D: 0.0359, loss G: 3.4261\n",
      "Epoch [5/10] Batch 352/510               Loss D: 0.0427, loss G: 3.3919\n",
      "Epoch [5/10] Batch 353/510               Loss D: 0.0665, loss G: 3.4268\n",
      "Epoch [5/10] Batch 354/510               Loss D: 0.0456, loss G: 3.3265\n",
      "Epoch [5/10] Batch 355/510               Loss D: 0.0530, loss G: 3.3365\n",
      "Epoch [5/10] Batch 356/510               Loss D: 0.0422, loss G: 3.3021\n",
      "Epoch [5/10] Batch 357/510               Loss D: 0.0400, loss G: 3.3309\n",
      "Epoch [5/10] Batch 358/510               Loss D: 0.0408, loss G: 3.3238\n",
      "Epoch [5/10] Batch 359/510               Loss D: 0.0404, loss G: 3.3285\n",
      "Epoch [5/10] Batch 360/510               Loss D: 0.0409, loss G: 3.4280\n",
      "Epoch [5/10] Batch 361/510               Loss D: 0.0365, loss G: 3.4446\n",
      "Epoch [5/10] Batch 362/510               Loss D: 0.0345, loss G: 3.4744\n",
      "Epoch [5/10] Batch 363/510               Loss D: 0.0313, loss G: 3.5950\n",
      "Epoch [5/10] Batch 364/510               Loss D: 0.0992, loss G: 3.5338\n",
      "Epoch [5/10] Batch 365/510               Loss D: 0.0313, loss G: 3.5463\n",
      "Epoch [5/10] Batch 366/510               Loss D: 0.0968, loss G: 3.5553\n",
      "Epoch [5/10] Batch 367/510               Loss D: 0.0997, loss G: 3.4693\n",
      "Epoch [5/10] Batch 368/510               Loss D: 0.0298, loss G: 3.5083\n",
      "Epoch [5/10] Batch 369/510               Loss D: 0.0355, loss G: 3.3906\n",
      "Epoch [5/10] Batch 370/510               Loss D: 0.0340, loss G: 3.3841\n",
      "Epoch [5/10] Batch 371/510               Loss D: 0.0360, loss G: 3.3596\n",
      "Epoch [5/10] Batch 372/510               Loss D: 0.0332, loss G: 3.4304\n",
      "Epoch [5/10] Batch 373/510               Loss D: 0.0350, loss G: 3.4566\n",
      "Epoch [5/10] Batch 374/510               Loss D: 0.0376, loss G: 3.3859\n",
      "Epoch [5/10] Batch 375/510               Loss D: 0.0341, loss G: 3.4265\n",
      "Epoch [5/10] Batch 376/510               Loss D: 0.0619, loss G: 3.4830\n",
      "Epoch [5/10] Batch 377/510               Loss D: 0.0342, loss G: 3.4785\n",
      "Epoch [5/10] Batch 378/510               Loss D: 0.0344, loss G: 3.4536\n",
      "Epoch [5/10] Batch 379/510               Loss D: 0.0357, loss G: 3.4614\n",
      "Epoch [5/10] Batch 380/510               Loss D: 0.0333, loss G: 3.4991\n",
      "Epoch [5/10] Batch 381/510               Loss D: 0.0336, loss G: 3.5316\n",
      "Epoch [5/10] Batch 382/510               Loss D: 2.8636, loss G: 3.3174\n",
      "Epoch [5/10] Batch 383/510               Loss D: 0.0445, loss G: 3.0658\n",
      "Epoch [5/10] Batch 384/510               Loss D: 0.0495, loss G: 2.8910\n",
      "Epoch [5/10] Batch 385/510               Loss D: 0.0590, loss G: 2.7642\n",
      "Epoch [5/10] Batch 386/510               Loss D: 0.0623, loss G: 2.6871\n",
      "Epoch [5/10] Batch 387/510               Loss D: 0.0660, loss G: 2.6569\n",
      "Epoch [5/10] Batch 388/510               Loss D: 1.1848, loss G: 2.4906\n",
      "Epoch [5/10] Batch 389/510               Loss D: 0.1197, loss G: 2.3020\n",
      "Epoch [5/10] Batch 390/510               Loss D: 0.0834, loss G: 2.3416\n",
      "Epoch [5/10] Batch 391/510               Loss D: 0.0899, loss G: 2.3180\n",
      "Epoch [5/10] Batch 392/510               Loss D: 0.0962, loss G: 2.3021\n",
      "Epoch [5/10] Batch 393/510               Loss D: 0.0827, loss G: 2.4519\n",
      "Epoch [5/10] Batch 394/510               Loss D: 0.0816, loss G: 2.5723\n",
      "Epoch [5/10] Batch 395/510               Loss D: 0.0756, loss G: 2.6246\n",
      "Epoch [5/10] Batch 396/510               Loss D: 0.0639, loss G: 2.7803\n",
      "Epoch [5/10] Batch 397/510               Loss D: 0.0539, loss G: 2.9669\n",
      "Epoch [5/10] Batch 398/510               Loss D: 0.0467, loss G: 3.1634\n",
      "Epoch [5/10] Batch 399/510               Loss D: 0.0436, loss G: 3.2538\n",
      "Epoch [5/10] Batch 400/510               Loss D: 0.0811, loss G: 3.3940\n",
      "Epoch [5/10] Batch 401/510               Loss D: 0.0349, loss G: 3.4318\n",
      "Epoch [5/10] Batch 402/510               Loss D: 0.0310, loss G: 3.5957\n",
      "Epoch [5/10] Batch 403/510               Loss D: 0.0297, loss G: 3.6460\n",
      "Epoch [5/10] Batch 404/510               Loss D: 0.0294, loss G: 3.6414\n",
      "Epoch [5/10] Batch 405/510               Loss D: 0.0283, loss G: 3.6889\n",
      "Epoch [5/10] Batch 406/510               Loss D: 0.0237, loss G: 3.8577\n",
      "Epoch [5/10] Batch 407/510               Loss D: 0.0243, loss G: 3.8407\n",
      "Epoch [5/10] Batch 408/510               Loss D: 0.0239, loss G: 3.8696\n",
      "Epoch [5/10] Batch 409/510               Loss D: 0.0707, loss G: 3.8164\n",
      "Epoch [5/10] Batch 410/510               Loss D: 0.0247, loss G: 3.7987\n",
      "Epoch [5/10] Batch 411/510               Loss D: 0.0255, loss G: 3.7750\n",
      "Epoch [5/10] Batch 412/510               Loss D: 0.0295, loss G: 3.6440\n",
      "Epoch [5/10] Batch 413/510               Loss D: 0.0272, loss G: 3.6657\n",
      "Epoch [5/10] Batch 414/510               Loss D: 0.0309, loss G: 3.6004\n",
      "Epoch [5/10] Batch 415/510               Loss D: 0.0299, loss G: 3.6183\n",
      "Epoch [5/10] Batch 416/510               Loss D: 0.0310, loss G: 3.5534\n",
      "Epoch [5/10] Batch 417/510               Loss D: 0.0341, loss G: 3.4795\n",
      "Epoch [5/10] Batch 418/510               Loss D: 0.0384, loss G: 3.3892\n",
      "Epoch [5/10] Batch 419/510               Loss D: 0.0373, loss G: 3.4146\n",
      "Epoch [5/10] Batch 420/510               Loss D: 0.0375, loss G: 3.4038\n",
      "Epoch [5/10] Batch 421/510               Loss D: 0.0426, loss G: 3.2887\n",
      "Epoch [5/10] Batch 422/510               Loss D: 0.0424, loss G: 3.3024\n",
      "Epoch [5/10] Batch 423/510               Loss D: 0.0427, loss G: 3.3252\n",
      "Epoch [5/10] Batch 424/510               Loss D: 0.0396, loss G: 3.4052\n",
      "Epoch [5/10] Batch 425/510               Loss D: 0.0560, loss G: 3.3137\n",
      "Epoch [5/10] Batch 426/510               Loss D: 0.1429, loss G: 3.2687\n",
      "Epoch [5/10] Batch 427/510               Loss D: 0.0481, loss G: 3.1937\n",
      "Epoch [5/10] Batch 428/510               Loss D: 0.0483, loss G: 3.1449\n",
      "Epoch [5/10] Batch 429/510               Loss D: 0.0514, loss G: 3.1334\n",
      "Epoch [5/10] Batch 430/510               Loss D: 0.1007, loss G: 3.0910\n",
      "Epoch [5/10] Batch 431/510               Loss D: 0.0526, loss G: 3.1277\n",
      "Epoch [5/10] Batch 432/510               Loss D: 0.0609, loss G: 3.0727\n",
      "Epoch [5/10] Batch 433/510               Loss D: 0.0541, loss G: 3.1724\n",
      "Epoch [5/10] Batch 434/510               Loss D: 0.0559, loss G: 3.1093\n",
      "Epoch [5/10] Batch 435/510               Loss D: 0.0547, loss G: 3.1142\n",
      "Epoch [5/10] Batch 436/510               Loss D: 0.0562, loss G: 3.1245\n",
      "Epoch [5/10] Batch 437/510               Loss D: 0.0530, loss G: 3.2539\n",
      "Epoch [5/10] Batch 438/510               Loss D: 0.0526, loss G: 3.2706\n",
      "Epoch [5/10] Batch 439/510               Loss D: 0.0510, loss G: 3.2725\n",
      "Epoch [5/10] Batch 440/510               Loss D: 0.0487, loss G: 3.2904\n",
      "Epoch [5/10] Batch 441/510               Loss D: 0.0469, loss G: 3.3823\n",
      "Epoch [5/10] Batch 442/510               Loss D: 0.6076, loss G: 3.1416\n",
      "Epoch [5/10] Batch 443/510               Loss D: 0.0566, loss G: 3.0615\n",
      "Epoch [5/10] Batch 444/510               Loss D: 0.0640, loss G: 2.9587\n",
      "Epoch [5/10] Batch 445/510               Loss D: 0.0677, loss G: 2.8365\n",
      "Epoch [5/10] Batch 446/510               Loss D: 0.0767, loss G: 2.7424\n",
      "Epoch [5/10] Batch 447/510               Loss D: 0.0798, loss G: 2.7482\n",
      "Epoch [5/10] Batch 448/510               Loss D: 0.0785, loss G: 2.7811\n",
      "Epoch [5/10] Batch 449/510               Loss D: 0.0753, loss G: 2.8729\n",
      "Epoch [5/10] Batch 450/510               Loss D: 0.0832, loss G: 2.7839\n",
      "Epoch [5/10] Batch 451/510               Loss D: 0.0730, loss G: 2.9653\n",
      "Epoch [5/10] Batch 452/510               Loss D: 0.0910, loss G: 3.0694\n",
      "Epoch [5/10] Batch 453/510               Loss D: 0.0622, loss G: 3.0886\n",
      "Epoch [5/10] Batch 454/510               Loss D: 0.0553, loss G: 3.2360\n",
      "Epoch [5/10] Batch 455/510               Loss D: 0.1574, loss G: 3.2799\n",
      "Epoch [5/10] Batch 456/510               Loss D: 0.0487, loss G: 3.3066\n",
      "Epoch [5/10] Batch 457/510               Loss D: 0.0445, loss G: 3.3488\n",
      "Epoch [5/10] Batch 458/510               Loss D: 0.1123, loss G: 3.4172\n",
      "Epoch [5/10] Batch 459/510               Loss D: 0.0444, loss G: 3.3892\n",
      "Epoch [5/10] Batch 460/510               Loss D: 0.0432, loss G: 3.4673\n",
      "Epoch [5/10] Batch 461/510               Loss D: 0.0447, loss G: 3.5021\n",
      "Epoch [5/10] Batch 462/510               Loss D: 0.0371, loss G: 3.5737\n",
      "Epoch [5/10] Batch 463/510               Loss D: 0.0399, loss G: 3.5100\n",
      "Epoch [5/10] Batch 464/510               Loss D: 0.0378, loss G: 3.5610\n",
      "Epoch [5/10] Batch 465/510               Loss D: 0.0694, loss G: 3.5893\n",
      "Epoch [5/10] Batch 466/510               Loss D: 0.0374, loss G: 3.5834\n",
      "Epoch [5/10] Batch 467/510               Loss D: 0.0380, loss G: 3.5389\n",
      "Epoch [5/10] Batch 468/510               Loss D: 0.0421, loss G: 3.5759\n",
      "Epoch [5/10] Batch 469/510               Loss D: 0.0372, loss G: 3.6071\n",
      "Epoch [5/10] Batch 470/510               Loss D: 0.0424, loss G: 3.6242\n",
      "Epoch [5/10] Batch 471/510               Loss D: 0.0380, loss G: 3.6283\n",
      "Epoch [5/10] Batch 472/510               Loss D: 0.0357, loss G: 3.6330\n",
      "Epoch [5/10] Batch 473/510               Loss D: 0.0418, loss G: 3.5355\n",
      "Epoch [5/10] Batch 474/510               Loss D: 0.0418, loss G: 3.6308\n",
      "Epoch [5/10] Batch 475/510               Loss D: 0.0417, loss G: 3.5321\n",
      "Epoch [5/10] Batch 476/510               Loss D: 0.0409, loss G: 3.5079\n",
      "Epoch [5/10] Batch 477/510               Loss D: 0.0470, loss G: 3.5246\n",
      "Epoch [5/10] Batch 478/510               Loss D: 0.0392, loss G: 3.6043\n",
      "Epoch [5/10] Batch 479/510               Loss D: 0.0545, loss G: 3.5241\n",
      "Epoch [5/10] Batch 480/510               Loss D: 0.0439, loss G: 3.5043\n",
      "Epoch [5/10] Batch 481/510               Loss D: 0.0526, loss G: 3.5430\n",
      "Epoch [5/10] Batch 482/510               Loss D: 0.0942, loss G: 3.5101\n",
      "Epoch [5/10] Batch 483/510               Loss D: 0.0458, loss G: 3.4505\n",
      "Epoch [5/10] Batch 484/510               Loss D: 0.0456, loss G: 3.4518\n",
      "Epoch [5/10] Batch 485/510               Loss D: 0.6628, loss G: 3.3182\n",
      "Epoch [5/10] Batch 486/510               Loss D: 0.0543, loss G: 3.0376\n",
      "Epoch [5/10] Batch 487/510               Loss D: 0.2353, loss G: 2.8034\n",
      "Epoch [5/10] Batch 488/510               Loss D: 0.0835, loss G: 2.6406\n",
      "Epoch [5/10] Batch 489/510               Loss D: 0.1245, loss G: 2.5062\n",
      "Epoch [5/10] Batch 490/510               Loss D: 0.1082, loss G: 2.5302\n",
      "Epoch [5/10] Batch 491/510               Loss D: 0.1134, loss G: 2.4991\n",
      "Epoch [5/10] Batch 492/510               Loss D: 0.1056, loss G: 2.6180\n",
      "Epoch [5/10] Batch 493/510               Loss D: 0.1170, loss G: 2.7109\n",
      "Epoch [5/10] Batch 494/510               Loss D: 0.1935, loss G: 2.7813\n",
      "Epoch [5/10] Batch 495/510               Loss D: 0.0824, loss G: 2.9009\n",
      "Epoch [5/10] Batch 496/510               Loss D: 0.0747, loss G: 3.0487\n",
      "Epoch [5/10] Batch 497/510               Loss D: 0.0666, loss G: 3.1442\n",
      "Epoch [5/10] Batch 498/510               Loss D: 0.0572, loss G: 3.4294\n",
      "Epoch [5/10] Batch 499/510               Loss D: 0.0499, loss G: 3.5217\n",
      "Epoch [5/10] Batch 500/510               Loss D: 0.0456, loss G: 3.6606\n",
      "Epoch [5/10] Batch 501/510               Loss D: 0.0368, loss G: 3.8653\n",
      "Epoch [5/10] Batch 502/510               Loss D: 0.0344, loss G: 3.9550\n",
      "Epoch [5/10] Batch 503/510               Loss D: 0.0331, loss G: 3.9886\n",
      "Epoch [5/10] Batch 504/510               Loss D: 0.0586, loss G: 4.0837\n",
      "Epoch [5/10] Batch 505/510               Loss D: 0.0256, loss G: 4.1561\n",
      "Epoch [5/10] Batch 506/510               Loss D: 0.0306, loss G: 4.1890\n",
      "Epoch [5/10] Batch 507/510               Loss D: 0.0254, loss G: 4.2600\n",
      "Epoch [5/10] Batch 508/510               Loss D: 0.0626, loss G: 4.2031\n",
      "Epoch [5/10] Batch 509/510               Loss D: 0.8682, loss G: 3.9522\n",
      "Epoch [6/10] Batch 0/510               Loss D: 0.0391, loss G: 3.6359\n",
      "Epoch [6/10] Batch 1/510               Loss D: 0.0373, loss G: 3.6054\n",
      "Epoch [6/10] Batch 2/510               Loss D: 0.0493, loss G: 3.2520\n",
      "Epoch [6/10] Batch 3/510               Loss D: 0.1746, loss G: 3.1613\n",
      "Epoch [6/10] Batch 4/510               Loss D: 0.0611, loss G: 3.1596\n",
      "Epoch [6/10] Batch 5/510               Loss D: 0.0712, loss G: 3.0061\n",
      "Epoch [6/10] Batch 6/510               Loss D: 0.0753, loss G: 3.0286\n",
      "Epoch [6/10] Batch 7/510               Loss D: 0.0725, loss G: 3.0333\n",
      "Epoch [6/10] Batch 8/510               Loss D: 0.0723, loss G: 3.0768\n",
      "Epoch [6/10] Batch 9/510               Loss D: 0.0726, loss G: 3.0932\n",
      "Epoch [6/10] Batch 10/510               Loss D: 0.0619, loss G: 3.3225\n",
      "Epoch [6/10] Batch 11/510               Loss D: 0.0791, loss G: 3.3968\n",
      "Epoch [6/10] Batch 12/510               Loss D: 0.0475, loss G: 3.4977\n",
      "Epoch [6/10] Batch 13/510               Loss D: 0.0418, loss G: 3.5978\n",
      "Epoch [6/10] Batch 14/510               Loss D: 0.0381, loss G: 3.7512\n",
      "Epoch [6/10] Batch 15/510               Loss D: 0.0318, loss G: 3.8822\n",
      "Epoch [6/10] Batch 16/510               Loss D: 0.0282, loss G: 3.9553\n",
      "Epoch [6/10] Batch 17/510               Loss D: 0.1528, loss G: 3.8741\n",
      "Epoch [6/10] Batch 18/510               Loss D: 0.0250, loss G: 4.0188\n",
      "Epoch [6/10] Batch 19/510               Loss D: 0.0272, loss G: 4.0050\n",
      "Epoch [6/10] Batch 20/510               Loss D: 0.0249, loss G: 3.9961\n",
      "Epoch [6/10] Batch 21/510               Loss D: 0.0241, loss G: 4.0343\n",
      "Epoch [6/10] Batch 22/510               Loss D: 0.0237, loss G: 4.0222\n",
      "Epoch [6/10] Batch 23/510               Loss D: 0.0223, loss G: 4.0929\n",
      "Epoch [6/10] Batch 24/510               Loss D: 0.0232, loss G: 3.9962\n",
      "Epoch [6/10] Batch 25/510               Loss D: 0.0220, loss G: 4.0974\n",
      "Epoch [6/10] Batch 26/510               Loss D: 0.0230, loss G: 4.0857\n",
      "Epoch [6/10] Batch 27/510               Loss D: 0.0239, loss G: 3.9590\n",
      "Epoch [6/10] Batch 28/510               Loss D: 0.0789, loss G: 4.0008\n",
      "Epoch [6/10] Batch 29/510               Loss D: 0.0243, loss G: 3.9250\n",
      "Epoch [6/10] Batch 30/510               Loss D: 0.0245, loss G: 3.9555\n",
      "Epoch [6/10] Batch 31/510               Loss D: 0.0270, loss G: 3.8202\n",
      "Epoch [6/10] Batch 32/510               Loss D: 0.0277, loss G: 3.7660\n",
      "Epoch [6/10] Batch 33/510               Loss D: 0.0307, loss G: 3.6715\n",
      "Epoch [6/10] Batch 34/510               Loss D: 0.0306, loss G: 3.6796\n",
      "Epoch [6/10] Batch 35/510               Loss D: 0.0326, loss G: 3.6828\n",
      "Epoch [6/10] Batch 36/510               Loss D: 0.1005, loss G: 3.6408\n",
      "Epoch [6/10] Batch 37/510               Loss D: 0.0324, loss G: 3.5838\n",
      "Epoch [6/10] Batch 38/510               Loss D: 0.0329, loss G: 3.5504\n",
      "Epoch [6/10] Batch 39/510               Loss D: 0.0383, loss G: 3.5870\n",
      "Epoch [6/10] Batch 40/510               Loss D: 0.0320, loss G: 3.6344\n",
      "Epoch [6/10] Batch 41/510               Loss D: 0.0371, loss G: 3.5135\n",
      "Epoch [6/10] Batch 42/510               Loss D: 0.0306, loss G: 3.6739\n",
      "Epoch [6/10] Batch 43/510               Loss D: 0.0291, loss G: 3.7224\n",
      "Epoch [6/10] Batch 44/510               Loss D: 0.0300, loss G: 3.6586\n",
      "Epoch [6/10] Batch 45/510               Loss D: 0.0295, loss G: 3.6990\n",
      "Epoch [6/10] Batch 46/510               Loss D: 0.0266, loss G: 3.8138\n",
      "Epoch [6/10] Batch 47/510               Loss D: 0.0314, loss G: 3.7645\n",
      "Epoch [6/10] Batch 48/510               Loss D: 0.0252, loss G: 3.8561\n",
      "Epoch [6/10] Batch 49/510               Loss D: 0.0257, loss G: 3.8143\n",
      "Epoch [6/10] Batch 50/510               Loss D: 1.2063, loss G: 3.6969\n",
      "Epoch [6/10] Batch 51/510               Loss D: 0.0291, loss G: 3.5343\n",
      "Epoch [6/10] Batch 52/510               Loss D: 0.0439, loss G: 3.3770\n",
      "Epoch [6/10] Batch 53/510               Loss D: 0.0347, loss G: 3.3190\n",
      "Epoch [6/10] Batch 54/510               Loss D: 0.0380, loss G: 3.2283\n",
      "Epoch [6/10] Batch 55/510               Loss D: 0.0387, loss G: 3.2284\n",
      "Epoch [6/10] Batch 56/510               Loss D: 0.0489, loss G: 3.1369\n",
      "Epoch [6/10] Batch 57/510               Loss D: 0.0400, loss G: 3.2039\n",
      "Epoch [6/10] Batch 58/510               Loss D: 0.0408, loss G: 3.2159\n",
      "Epoch [6/10] Batch 59/510               Loss D: 0.0380, loss G: 3.2851\n",
      "Epoch [6/10] Batch 60/510               Loss D: 0.1391, loss G: 3.3397\n",
      "Epoch [6/10] Batch 61/510               Loss D: 0.0374, loss G: 3.2956\n",
      "Epoch [6/10] Batch 62/510               Loss D: 0.0354, loss G: 3.3731\n",
      "Epoch [6/10] Batch 63/510               Loss D: 0.0332, loss G: 3.4392\n",
      "Epoch [6/10] Batch 64/510               Loss D: 0.0343, loss G: 3.4280\n",
      "Epoch [6/10] Batch 65/510               Loss D: 0.0335, loss G: 3.4806\n",
      "Epoch [6/10] Batch 66/510               Loss D: 0.0301, loss G: 3.5995\n",
      "Epoch [6/10] Batch 67/510               Loss D: 0.0312, loss G: 3.6149\n",
      "Epoch [6/10] Batch 68/510               Loss D: 0.0266, loss G: 3.7287\n",
      "Epoch [6/10] Batch 69/510               Loss D: 0.0263, loss G: 3.7290\n",
      "Epoch [6/10] Batch 70/510               Loss D: 0.0274, loss G: 3.7537\n",
      "Epoch [6/10] Batch 71/510               Loss D: 0.0289, loss G: 3.7524\n",
      "Epoch [6/10] Batch 72/510               Loss D: 0.0250, loss G: 3.8327\n",
      "Epoch [6/10] Batch 73/510               Loss D: 0.0275, loss G: 3.8402\n",
      "Epoch [6/10] Batch 74/510               Loss D: 0.0246, loss G: 3.8466\n",
      "Epoch [6/10] Batch 75/510               Loss D: 0.0248, loss G: 3.8371\n",
      "Epoch [6/10] Batch 76/510               Loss D: 0.0259, loss G: 3.8005\n",
      "Epoch [6/10] Batch 77/510               Loss D: 0.0286, loss G: 3.7785\n",
      "Epoch [6/10] Batch 78/510               Loss D: 0.0782, loss G: 3.7666\n",
      "Epoch [6/10] Batch 79/510               Loss D: 0.0289, loss G: 3.8118\n",
      "Epoch [6/10] Batch 80/510               Loss D: 0.0295, loss G: 3.7462\n",
      "Epoch [6/10] Batch 81/510               Loss D: 0.0280, loss G: 3.7279\n",
      "Epoch [6/10] Batch 82/510               Loss D: 0.0349, loss G: 3.7090\n",
      "Epoch [6/10] Batch 83/510               Loss D: 0.0313, loss G: 3.6576\n",
      "Epoch [6/10] Batch 84/510               Loss D: 0.0323, loss G: 3.6162\n",
      "Epoch [6/10] Batch 85/510               Loss D: 0.0350, loss G: 3.6307\n",
      "Epoch [6/10] Batch 86/510               Loss D: 0.0315, loss G: 3.6809\n",
      "Epoch [6/10] Batch 87/510               Loss D: 0.0321, loss G: 3.6312\n",
      "Epoch [6/10] Batch 88/510               Loss D: 0.1165, loss G: 3.6104\n",
      "Epoch [6/10] Batch 89/510               Loss D: 0.0358, loss G: 3.6356\n",
      "Epoch [6/10] Batch 90/510               Loss D: 0.0323, loss G: 3.5909\n",
      "Epoch [6/10] Batch 91/510               Loss D: 0.0314, loss G: 3.6062\n",
      "Epoch [6/10] Batch 92/510               Loss D: 0.0397, loss G: 3.5503\n",
      "Epoch [6/10] Batch 93/510               Loss D: 0.0337, loss G: 3.5719\n",
      "Epoch [6/10] Batch 94/510               Loss D: 0.0341, loss G: 3.5808\n",
      "Epoch [6/10] Batch 95/510               Loss D: 0.0348, loss G: 3.5488\n",
      "Epoch [6/10] Batch 96/510               Loss D: 0.0520, loss G: 3.5540\n",
      "Epoch [6/10] Batch 97/510               Loss D: 0.0696, loss G: 3.6393\n",
      "Epoch [6/10] Batch 98/510               Loss D: 0.0305, loss G: 3.7170\n",
      "Epoch [6/10] Batch 99/510               Loss D: 0.0290, loss G: 3.7212\n",
      "Epoch [6/10] Batch 100/510               Loss D: 0.0301, loss G: 3.6788\n",
      "Epoch [6/10] Batch 101/510               Loss D: 0.1613, loss G: 3.6652\n",
      "Epoch [6/10] Batch 102/510               Loss D: 0.0400, loss G: 3.7094\n",
      "Epoch [6/10] Batch 103/510               Loss D: 0.1375, loss G: 3.6929\n",
      "Epoch [6/10] Batch 104/510               Loss D: 0.0297, loss G: 3.5704\n",
      "Epoch [6/10] Batch 105/510               Loss D: 0.0319, loss G: 3.5589\n",
      "Epoch [6/10] Batch 106/510               Loss D: 0.0303, loss G: 3.5858\n",
      "Epoch [6/10] Batch 107/510               Loss D: 0.0330, loss G: 3.5355\n",
      "Epoch [6/10] Batch 108/510               Loss D: 0.0355, loss G: 3.5426\n",
      "Epoch [6/10] Batch 109/510               Loss D: 0.0521, loss G: 3.5844\n",
      "Epoch [6/10] Batch 110/510               Loss D: 0.0329, loss G: 3.5019\n",
      "Epoch [6/10] Batch 111/510               Loss D: 0.0323, loss G: 3.5491\n",
      "Epoch [6/10] Batch 112/510               Loss D: 0.0480, loss G: 3.5482\n",
      "Epoch [6/10] Batch 113/510               Loss D: 1.1562, loss G: 3.3668\n",
      "Epoch [6/10] Batch 114/510               Loss D: 0.0345, loss G: 3.2710\n",
      "Epoch [6/10] Batch 115/510               Loss D: 0.0423, loss G: 3.0841\n",
      "Epoch [6/10] Batch 116/510               Loss D: 0.0459, loss G: 3.0329\n",
      "Epoch [6/10] Batch 117/510               Loss D: 0.0505, loss G: 2.9885\n",
      "Epoch [6/10] Batch 118/510               Loss D: 0.0503, loss G: 2.9511\n",
      "Epoch [6/10] Batch 119/510               Loss D: 0.0543, loss G: 2.9414\n",
      "Epoch [6/10] Batch 120/510               Loss D: 0.0465, loss G: 3.0715\n",
      "Epoch [6/10] Batch 121/510               Loss D: 0.0466, loss G: 3.0868\n",
      "Epoch [6/10] Batch 122/510               Loss D: 0.0431, loss G: 3.1558\n",
      "Epoch [6/10] Batch 123/510               Loss D: 0.0419, loss G: 3.2299\n",
      "Epoch [6/10] Batch 124/510               Loss D: 0.0414, loss G: 3.2288\n",
      "Epoch [6/10] Batch 125/510               Loss D: 0.0349, loss G: 3.4470\n",
      "Epoch [6/10] Batch 126/510               Loss D: 0.0341, loss G: 3.4136\n",
      "Epoch [6/10] Batch 127/510               Loss D: 0.0303, loss G: 3.5161\n",
      "Epoch [6/10] Batch 128/510               Loss D: 0.0273, loss G: 3.6677\n",
      "Epoch [6/10] Batch 129/510               Loss D: 0.0294, loss G: 3.5746\n",
      "Epoch [6/10] Batch 130/510               Loss D: 0.0249, loss G: 3.7319\n",
      "Epoch [6/10] Batch 131/510               Loss D: 0.0244, loss G: 3.7622\n",
      "Epoch [6/10] Batch 132/510               Loss D: 0.0232, loss G: 3.7961\n",
      "Epoch [6/10] Batch 133/510               Loss D: 0.0213, loss G: 3.8655\n",
      "Epoch [6/10] Batch 134/510               Loss D: 0.0215, loss G: 3.8693\n",
      "Epoch [6/10] Batch 135/510               Loss D: 0.3371, loss G: 3.7333\n",
      "Epoch [6/10] Batch 136/510               Loss D: 0.0239, loss G: 3.6208\n",
      "Epoch [6/10] Batch 137/510               Loss D: 0.0248, loss G: 3.5594\n",
      "Epoch [6/10] Batch 138/510               Loss D: 0.0386, loss G: 3.4998\n",
      "Epoch [6/10] Batch 139/510               Loss D: 0.0293, loss G: 3.3881\n",
      "Epoch [6/10] Batch 140/510               Loss D: 0.0329, loss G: 3.2818\n",
      "Epoch [6/10] Batch 141/510               Loss D: 0.0352, loss G: 3.2356\n",
      "Epoch [6/10] Batch 142/510               Loss D: 0.0465, loss G: 3.2375\n",
      "Epoch [6/10] Batch 143/510               Loss D: 0.0354, loss G: 3.2225\n",
      "Epoch [6/10] Batch 144/510               Loss D: 1.1624, loss G: 2.9827\n",
      "Epoch [6/10] Batch 145/510               Loss D: 0.0435, loss G: 2.8588\n",
      "Epoch [6/10] Batch 146/510               Loss D: 0.0494, loss G: 2.7473\n",
      "Epoch [6/10] Batch 147/510               Loss D: 0.0593, loss G: 2.5825\n",
      "Epoch [6/10] Batch 148/510               Loss D: 0.0661, loss G: 2.5381\n",
      "Epoch [6/10] Batch 149/510               Loss D: 0.0645, loss G: 2.6084\n",
      "Epoch [6/10] Batch 150/510               Loss D: 0.0659, loss G: 2.6128\n",
      "Epoch [6/10] Batch 151/510               Loss D: 0.0637, loss G: 2.6765\n",
      "Epoch [6/10] Batch 152/510               Loss D: 0.0604, loss G: 2.7615\n",
      "Epoch [6/10] Batch 153/510               Loss D: 0.0539, loss G: 2.8902\n",
      "Epoch [6/10] Batch 154/510               Loss D: 0.0477, loss G: 3.0411\n",
      "Epoch [6/10] Batch 155/510               Loss D: 0.0461, loss G: 3.0846\n",
      "Epoch [6/10] Batch 156/510               Loss D: 0.0429, loss G: 3.1664\n",
      "Epoch [6/10] Batch 157/510               Loss D: 0.0361, loss G: 3.3306\n",
      "Epoch [6/10] Batch 158/510               Loss D: 0.0311, loss G: 3.4864\n",
      "Epoch [6/10] Batch 159/510               Loss D: 0.0299, loss G: 3.5471\n",
      "Epoch [6/10] Batch 160/510               Loss D: 0.0260, loss G: 3.6689\n",
      "Epoch [6/10] Batch 161/510               Loss D: 0.0242, loss G: 3.7397\n",
      "Epoch [6/10] Batch 162/510               Loss D: 0.0240, loss G: 3.7998\n",
      "Epoch [6/10] Batch 163/510               Loss D: 0.0220, loss G: 3.9292\n",
      "Epoch [6/10] Batch 164/510               Loss D: 0.0408, loss G: 3.8811\n",
      "Epoch [6/10] Batch 165/510               Loss D: 0.0197, loss G: 3.9279\n",
      "Epoch [6/10] Batch 166/510               Loss D: 0.0211, loss G: 3.8966\n",
      "Epoch [6/10] Batch 167/510               Loss D: 0.0208, loss G: 3.9396\n",
      "Epoch [6/10] Batch 168/510               Loss D: 0.0222, loss G: 3.8383\n",
      "Epoch [6/10] Batch 169/510               Loss D: 0.0223, loss G: 3.8164\n",
      "Epoch [6/10] Batch 170/510               Loss D: 0.0223, loss G: 3.8333\n",
      "Epoch [6/10] Batch 171/510               Loss D: 0.0218, loss G: 3.8426\n",
      "Epoch [6/10] Batch 172/510               Loss D: 0.0256, loss G: 3.6949\n",
      "Epoch [6/10] Batch 173/510               Loss D: 0.0255, loss G: 3.6923\n",
      "Epoch [6/10] Batch 174/510               Loss D: 0.0280, loss G: 3.6347\n",
      "Epoch [6/10] Batch 175/510               Loss D: 0.0262, loss G: 3.6957\n",
      "Epoch [6/10] Batch 176/510               Loss D: 0.0275, loss G: 3.6788\n",
      "Epoch [6/10] Batch 177/510               Loss D: 0.0365, loss G: 3.5833\n",
      "Epoch [6/10] Batch 178/510               Loss D: 0.0298, loss G: 3.5498\n",
      "Epoch [6/10] Batch 179/510               Loss D: 0.0341, loss G: 3.5449\n",
      "Epoch [6/10] Batch 180/510               Loss D: 0.0284, loss G: 3.6581\n",
      "Epoch [6/10] Batch 181/510               Loss D: 0.0315, loss G: 3.5893\n",
      "Epoch [6/10] Batch 182/510               Loss D: 0.0345, loss G: 3.5248\n",
      "Epoch [6/10] Batch 183/510               Loss D: 0.0329, loss G: 3.5428\n",
      "Epoch [6/10] Batch 184/510               Loss D: 0.0326, loss G: 3.5489\n",
      "Epoch [6/10] Batch 185/510               Loss D: 0.0340, loss G: 3.5480\n",
      "Epoch [6/10] Batch 186/510               Loss D: 0.0327, loss G: 3.5989\n",
      "Epoch [6/10] Batch 187/510               Loss D: 0.0321, loss G: 3.5974\n",
      "Epoch [6/10] Batch 188/510               Loss D: 0.0410, loss G: 3.6174\n",
      "Epoch [6/10] Batch 189/510               Loss D: 0.0295, loss G: 3.6943\n",
      "Epoch [6/10] Batch 190/510               Loss D: 0.0292, loss G: 3.6993\n",
      "Epoch [6/10] Batch 191/510               Loss D: 0.0333, loss G: 3.6079\n",
      "Epoch [6/10] Batch 192/510               Loss D: 0.0273, loss G: 3.7909\n",
      "Epoch [6/10] Batch 193/510               Loss D: 0.0382, loss G: 3.6849\n",
      "Epoch [6/10] Batch 194/510               Loss D: 0.0297, loss G: 3.7472\n",
      "Epoch [6/10] Batch 195/510               Loss D: 0.0271, loss G: 3.8476\n",
      "Epoch [6/10] Batch 196/510               Loss D: 0.0254, loss G: 3.8747\n",
      "Epoch [6/10] Batch 197/510               Loss D: 0.0252, loss G: 3.8553\n",
      "Epoch [6/10] Batch 198/510               Loss D: 0.0226, loss G: 3.9477\n",
      "Epoch [6/10] Batch 199/510               Loss D: 0.0291, loss G: 3.9670\n",
      "Epoch [6/10] Batch 200/510               Loss D: 0.0289, loss G: 3.9196\n",
      "Epoch [6/10] Batch 201/510               Loss D: 0.0229, loss G: 3.9888\n",
      "Epoch [6/10] Batch 202/510               Loss D: 0.0322, loss G: 3.9486\n",
      "Epoch [6/10] Batch 203/510               Loss D: 0.0210, loss G: 4.0029\n",
      "Epoch [6/10] Batch 204/510               Loss D: 0.0450, loss G: 3.9992\n",
      "Epoch [6/10] Batch 205/510               Loss D: 0.0223, loss G: 3.9476\n",
      "Epoch [6/10] Batch 206/510               Loss D: 0.0215, loss G: 4.0068\n",
      "Epoch [6/10] Batch 207/510               Loss D: 0.0200, loss G: 4.0688\n",
      "Epoch [6/10] Batch 208/510               Loss D: 0.0223, loss G: 3.9892\n",
      "Epoch [6/10] Batch 209/510               Loss D: 0.0231, loss G: 3.9889\n",
      "Epoch [6/10] Batch 210/510               Loss D: 0.0215, loss G: 3.9543\n",
      "Epoch [6/10] Batch 211/510               Loss D: 0.1779, loss G: 3.8925\n",
      "Epoch [6/10] Batch 212/510               Loss D: 1.0482, loss G: 3.5378\n",
      "Epoch [6/10] Batch 213/510               Loss D: 0.0845, loss G: 3.2116\n",
      "Epoch [6/10] Batch 214/510               Loss D: 0.5855, loss G: 2.8951\n",
      "Epoch [6/10] Batch 215/510               Loss D: 2.7377, loss G: 2.1985\n",
      "Epoch [6/10] Batch 216/510               Loss D: 0.1390, loss G: 1.7618\n",
      "Epoch [6/10] Batch 217/510               Loss D: 0.1466, loss G: 1.5474\n",
      "Epoch [6/10] Batch 218/510               Loss D: 0.1839, loss G: 1.4250\n",
      "Epoch [6/10] Batch 219/510               Loss D: 0.1948, loss G: 1.4476\n",
      "Epoch [6/10] Batch 220/510               Loss D: 0.1873, loss G: 1.5889\n",
      "Epoch [6/10] Batch 221/510               Loss D: 0.1638, loss G: 1.7934\n",
      "Epoch [6/10] Batch 222/510               Loss D: 0.1283, loss G: 2.0788\n",
      "Epoch [6/10] Batch 223/510               Loss D: 0.0886, loss G: 2.4923\n",
      "Epoch [6/10] Batch 224/510               Loss D: 0.1838, loss G: 2.7830\n",
      "Epoch [6/10] Batch 225/510               Loss D: 0.0468, loss G: 3.0992\n",
      "Epoch [6/10] Batch 226/510               Loss D: 0.0359, loss G: 3.3766\n",
      "Epoch [6/10] Batch 227/510               Loss D: 0.0263, loss G: 3.7074\n",
      "Epoch [6/10] Batch 228/510               Loss D: 0.0210, loss G: 3.9431\n",
      "Epoch [6/10] Batch 229/510               Loss D: 0.0170, loss G: 4.1401\n",
      "Epoch [6/10] Batch 230/510               Loss D: 0.0144, loss G: 4.3008\n",
      "Epoch [6/10] Batch 231/510               Loss D: 0.0116, loss G: 4.5197\n",
      "Epoch [6/10] Batch 232/510               Loss D: 0.0108, loss G: 4.5907\n",
      "Epoch [6/10] Batch 233/510               Loss D: 0.0095, loss G: 4.7266\n",
      "Epoch [6/10] Batch 234/510               Loss D: 0.0086, loss G: 4.7401\n",
      "Epoch [6/10] Batch 235/510               Loss D: 0.0087, loss G: 4.7485\n",
      "Epoch [6/10] Batch 236/510               Loss D: 0.0088, loss G: 4.7699\n",
      "Epoch [6/10] Batch 237/510               Loss D: 0.0088, loss G: 4.7249\n",
      "Epoch [6/10] Batch 238/510               Loss D: 0.0091, loss G: 4.7214\n",
      "Epoch [6/10] Batch 239/510               Loss D: 0.0096, loss G: 4.6032\n",
      "Epoch [6/10] Batch 240/510               Loss D: 0.0108, loss G: 4.4848\n",
      "Epoch [6/10] Batch 241/510               Loss D: 0.0120, loss G: 4.4165\n",
      "Epoch [6/10] Batch 242/510               Loss D: 0.0121, loss G: 4.3263\n",
      "Epoch [6/10] Batch 243/510               Loss D: 0.0149, loss G: 4.2445\n",
      "Epoch [6/10] Batch 244/510               Loss D: 0.0145, loss G: 4.1236\n",
      "Epoch [6/10] Batch 245/510               Loss D: 0.0168, loss G: 3.9454\n",
      "Epoch [6/10] Batch 246/510               Loss D: 0.0167, loss G: 3.9507\n",
      "Epoch [6/10] Batch 247/510               Loss D: 0.0195, loss G: 3.8140\n",
      "Epoch [6/10] Batch 248/510               Loss D: 0.0235, loss G: 3.6359\n",
      "Epoch [6/10] Batch 249/510               Loss D: 0.0256, loss G: 3.5542\n",
      "Epoch [6/10] Batch 250/510               Loss D: 0.0260, loss G: 3.5183\n",
      "Epoch [6/10] Batch 251/510               Loss D: 0.0321, loss G: 3.3321\n",
      "Epoch [6/10] Batch 252/510               Loss D: 0.0335, loss G: 3.3273\n",
      "Epoch [6/10] Batch 253/510               Loss D: 0.0333, loss G: 3.2607\n",
      "Epoch [6/10] Batch 254/510               Loss D: 0.0355, loss G: 3.2747\n",
      "Epoch [6/10] Batch 255/510               Loss D: 0.0376, loss G: 3.1891\n",
      "Epoch [6/10] Batch 256/510               Loss D: 0.0384, loss G: 3.2011\n",
      "Epoch [6/10] Batch 257/510               Loss D: 0.0413, loss G: 3.1536\n",
      "Epoch [6/10] Batch 258/510               Loss D: 0.0570, loss G: 3.1459\n",
      "Epoch [6/10] Batch 259/510               Loss D: 0.0424, loss G: 3.1173\n",
      "Epoch [6/10] Batch 260/510               Loss D: 0.0412, loss G: 3.1705\n",
      "Epoch [6/10] Batch 261/510               Loss D: 0.0403, loss G: 3.1927\n",
      "Epoch [6/10] Batch 262/510               Loss D: 0.0408, loss G: 3.1931\n",
      "Epoch [6/10] Batch 263/510               Loss D: 0.0354, loss G: 3.3486\n",
      "Epoch [6/10] Batch 264/510               Loss D: 0.0382, loss G: 3.2762\n",
      "Epoch [6/10] Batch 265/510               Loss D: 0.0367, loss G: 3.3304\n",
      "Epoch [6/10] Batch 266/510               Loss D: 0.0364, loss G: 3.3556\n",
      "Epoch [6/10] Batch 267/510               Loss D: 0.0343, loss G: 3.4505\n",
      "Epoch [6/10] Batch 268/510               Loss D: 0.0342, loss G: 3.4896\n",
      "Epoch [6/10] Batch 269/510               Loss D: 0.0516, loss G: 3.4855\n",
      "Epoch [6/10] Batch 270/510               Loss D: 0.0304, loss G: 3.5348\n",
      "Epoch [6/10] Batch 271/510               Loss D: 0.0292, loss G: 3.5685\n",
      "Epoch [6/10] Batch 272/510               Loss D: 0.1494, loss G: 3.5355\n",
      "Epoch [6/10] Batch 273/510               Loss D: 0.0387, loss G: 3.4516\n",
      "Epoch [6/10] Batch 274/510               Loss D: 0.0336, loss G: 3.3989\n",
      "Epoch [6/10] Batch 275/510               Loss D: 0.0338, loss G: 3.4042\n",
      "Epoch [6/10] Batch 276/510               Loss D: 0.0336, loss G: 3.4098\n",
      "Epoch [6/10] Batch 277/510               Loss D: 0.0362, loss G: 3.3785\n",
      "Epoch [6/10] Batch 278/510               Loss D: 0.0393, loss G: 3.3437\n",
      "Epoch [6/10] Batch 279/510               Loss D: 0.0368, loss G: 3.3287\n",
      "Epoch [6/10] Batch 280/510               Loss D: 0.0401, loss G: 3.3788\n",
      "Epoch [6/10] Batch 281/510               Loss D: 0.2394, loss G: 3.2187\n",
      "Epoch [6/10] Batch 282/510               Loss D: 0.0418, loss G: 3.2071\n",
      "Epoch [6/10] Batch 283/510               Loss D: 0.0433, loss G: 3.1429\n",
      "Epoch [6/10] Batch 284/510               Loss D: 0.0517, loss G: 3.0181\n",
      "Epoch [6/10] Batch 285/510               Loss D: 0.0521, loss G: 3.0057\n",
      "Epoch [6/10] Batch 286/510               Loss D: 0.0543, loss G: 3.0446\n",
      "Epoch [6/10] Batch 287/510               Loss D: 0.0520, loss G: 3.0760\n",
      "Epoch [6/10] Batch 288/510               Loss D: 0.0536, loss G: 3.0466\n",
      "Epoch [6/10] Batch 289/510               Loss D: 0.0473, loss G: 3.2261\n",
      "Epoch [6/10] Batch 290/510               Loss D: 0.0504, loss G: 3.2454\n",
      "Epoch [6/10] Batch 291/510               Loss D: 0.0418, loss G: 3.3911\n",
      "Epoch [6/10] Batch 292/510               Loss D: 0.0394, loss G: 3.4766\n",
      "Epoch [6/10] Batch 293/510               Loss D: 0.0354, loss G: 3.5298\n",
      "Epoch [6/10] Batch 294/510               Loss D: 0.0330, loss G: 3.6636\n",
      "Epoch [6/10] Batch 295/510               Loss D: 0.0342, loss G: 3.6167\n",
      "Epoch [6/10] Batch 296/510               Loss D: 0.0494, loss G: 3.7322\n",
      "Epoch [6/10] Batch 297/510               Loss D: 0.0503, loss G: 3.7444\n",
      "Epoch [6/10] Batch 298/510               Loss D: 0.0294, loss G: 3.8651\n",
      "Epoch [6/10] Batch 299/510               Loss D: 0.0294, loss G: 3.7876\n",
      "Epoch [6/10] Batch 300/510               Loss D: 0.0308, loss G: 3.9089\n",
      "Epoch [6/10] Batch 301/510               Loss D: 0.0250, loss G: 3.9144\n",
      "Epoch [6/10] Batch 302/510               Loss D: 0.0266, loss G: 3.8619\n",
      "Epoch [6/10] Batch 303/510               Loss D: 0.0246, loss G: 3.8578\n",
      "Epoch [6/10] Batch 304/510               Loss D: 0.0336, loss G: 3.8500\n",
      "Epoch [6/10] Batch 305/510               Loss D: 0.1659, loss G: 3.7588\n",
      "Epoch [6/10] Batch 306/510               Loss D: 2.3058, loss G: 3.5392\n",
      "Epoch [6/10] Batch 307/510               Loss D: 0.0446, loss G: 3.2389\n",
      "Epoch [6/10] Batch 308/510               Loss D: 0.0422, loss G: 3.1458\n",
      "Epoch [6/10] Batch 309/510               Loss D: 0.8149, loss G: 2.7545\n",
      "Epoch [6/10] Batch 310/510               Loss D: 0.0653, loss G: 2.5903\n",
      "Epoch [6/10] Batch 311/510               Loss D: 0.0861, loss G: 2.4090\n",
      "Epoch [6/10] Batch 312/510               Loss D: 0.0934, loss G: 2.3608\n",
      "Epoch [6/10] Batch 313/510               Loss D: 0.0963, loss G: 2.4314\n",
      "Epoch [6/10] Batch 314/510               Loss D: 0.0994, loss G: 2.4274\n",
      "Epoch [6/10] Batch 315/510               Loss D: 0.0953, loss G: 2.5220\n",
      "Epoch [6/10] Batch 316/510               Loss D: 0.0970, loss G: 2.7022\n",
      "Epoch [6/10] Batch 317/510               Loss D: 0.0813, loss G: 2.8296\n",
      "Epoch [6/10] Batch 318/510               Loss D: 0.0676, loss G: 3.0271\n",
      "Epoch [6/10] Batch 319/510               Loss D: 0.0571, loss G: 3.2446\n",
      "Epoch [6/10] Batch 320/510               Loss D: 0.0497, loss G: 3.3604\n",
      "Epoch [6/10] Batch 321/510               Loss D: 0.0511, loss G: 3.4816\n",
      "Epoch [6/10] Batch 322/510               Loss D: 0.0398, loss G: 3.7456\n",
      "Epoch [6/10] Batch 323/510               Loss D: 0.0342, loss G: 3.9427\n",
      "Epoch [6/10] Batch 324/510               Loss D: 0.0296, loss G: 4.1076\n",
      "Epoch [6/10] Batch 325/510               Loss D: 0.0265, loss G: 4.1967\n",
      "Epoch [6/10] Batch 326/510               Loss D: 0.0429, loss G: 4.1265\n",
      "Epoch [6/10] Batch 327/510               Loss D: 0.0246, loss G: 4.2902\n",
      "Epoch [6/10] Batch 328/510               Loss D: 0.3996, loss G: 4.2323\n",
      "Epoch [6/10] Batch 329/510               Loss D: 0.1111, loss G: 4.2692\n",
      "Epoch [6/10] Batch 330/510               Loss D: 1.5417, loss G: 3.6506\n",
      "Epoch [6/10] Batch 331/510               Loss D: 0.0358, loss G: 3.4811\n",
      "Epoch [6/10] Batch 332/510               Loss D: 0.0510, loss G: 3.1661\n",
      "Epoch [6/10] Batch 333/510               Loss D: 0.0624, loss G: 2.9301\n",
      "Epoch [6/10] Batch 334/510               Loss D: 0.0744, loss G: 2.7934\n",
      "Epoch [6/10] Batch 335/510               Loss D: 0.0820, loss G: 2.7472\n",
      "Epoch [6/10] Batch 336/510               Loss D: 0.0866, loss G: 2.6975\n",
      "Epoch [6/10] Batch 337/510               Loss D: 0.0998, loss G: 2.6176\n",
      "Epoch [6/10] Batch 338/510               Loss D: 0.0986, loss G: 2.7392\n",
      "Epoch [6/10] Batch 339/510               Loss D: 0.0950, loss G: 2.8458\n",
      "Epoch [6/10] Batch 340/510               Loss D: 0.0800, loss G: 3.0198\n",
      "Epoch [6/10] Batch 341/510               Loss D: 0.0947, loss G: 3.1056\n",
      "Epoch [6/10] Batch 342/510               Loss D: 0.0644, loss G: 3.3202\n",
      "Epoch [6/10] Batch 343/510               Loss D: 0.0539, loss G: 3.5108\n",
      "Epoch [6/10] Batch 344/510               Loss D: 0.0508, loss G: 3.6078\n",
      "Epoch [6/10] Batch 345/510               Loss D: 0.0431, loss G: 3.7807\n",
      "Epoch [6/10] Batch 346/510               Loss D: 0.0393, loss G: 3.8925\n",
      "Epoch [6/10] Batch 347/510               Loss D: 0.0326, loss G: 4.0425\n",
      "Epoch [6/10] Batch 348/510               Loss D: 0.0300, loss G: 4.0866\n",
      "Epoch [6/10] Batch 349/510               Loss D: 0.0280, loss G: 4.1729\n",
      "Epoch [6/10] Batch 350/510               Loss D: 0.0270, loss G: 4.2948\n",
      "Epoch [6/10] Batch 351/510               Loss D: 0.0249, loss G: 4.3706\n",
      "Epoch [6/10] Batch 352/510               Loss D: 0.0250, loss G: 4.3885\n",
      "Epoch [6/10] Batch 353/510               Loss D: 0.0250, loss G: 4.3433\n",
      "Epoch [6/10] Batch 354/510               Loss D: 0.0278, loss G: 4.3219\n",
      "Epoch [6/10] Batch 355/510               Loss D: 0.0270, loss G: 4.2739\n",
      "Epoch [6/10] Batch 356/510               Loss D: 0.0281, loss G: 4.2561\n",
      "Epoch [6/10] Batch 357/510               Loss D: 0.0294, loss G: 4.1788\n",
      "Epoch [6/10] Batch 358/510               Loss D: 0.0281, loss G: 4.1950\n",
      "Epoch [6/10] Batch 359/510               Loss D: 0.0326, loss G: 4.0463\n",
      "Epoch [6/10] Batch 360/510               Loss D: 0.0343, loss G: 4.0764\n",
      "Epoch [6/10] Batch 361/510               Loss D: 0.0383, loss G: 3.9161\n",
      "Epoch [6/10] Batch 362/510               Loss D: 0.0363, loss G: 3.9058\n",
      "Epoch [6/10] Batch 363/510               Loss D: 0.0364, loss G: 3.9333\n",
      "Epoch [6/10] Batch 364/510               Loss D: 0.0410, loss G: 3.7568\n",
      "Epoch [6/10] Batch 365/510               Loss D: 0.0423, loss G: 3.7528\n",
      "Epoch [6/10] Batch 366/510               Loss D: 0.0424, loss G: 3.7980\n",
      "Epoch [6/10] Batch 367/510               Loss D: 0.0425, loss G: 3.6915\n",
      "Epoch [6/10] Batch 368/510               Loss D: 0.0427, loss G: 3.7253\n",
      "Epoch [6/10] Batch 369/510               Loss D: 0.0385, loss G: 3.7870\n",
      "Epoch [6/10] Batch 370/510               Loss D: 0.7555, loss G: 3.6939\n",
      "Epoch [6/10] Batch 371/510               Loss D: 0.0439, loss G: 3.4502\n",
      "Epoch [6/10] Batch 372/510               Loss D: 0.0473, loss G: 3.4229\n",
      "Epoch [6/10] Batch 373/510               Loss D: 0.0499, loss G: 3.3858\n",
      "Epoch [6/10] Batch 374/510               Loss D: 0.0561, loss G: 3.2652\n",
      "Epoch [6/10] Batch 375/510               Loss D: 0.0543, loss G: 3.3206\n",
      "Epoch [6/10] Batch 376/510               Loss D: 0.0585, loss G: 3.2324\n",
      "Epoch [6/10] Batch 377/510               Loss D: 0.0572, loss G: 3.2135\n",
      "Epoch [6/10] Batch 378/510               Loss D: 0.0541, loss G: 3.3105\n",
      "Epoch [6/10] Batch 379/510               Loss D: 0.0509, loss G: 3.3470\n",
      "Epoch [6/10] Batch 380/510               Loss D: 0.0466, loss G: 3.4442\n",
      "Epoch [6/10] Batch 381/510               Loss D: 0.0476, loss G: 3.4311\n",
      "Epoch [6/10] Batch 382/510               Loss D: 0.0455, loss G: 3.4346\n",
      "Epoch [6/10] Batch 383/510               Loss D: 0.0418, loss G: 3.5134\n",
      "Epoch [6/10] Batch 384/510               Loss D: 0.1477, loss G: 3.6103\n",
      "Epoch [6/10] Batch 385/510               Loss D: 0.0374, loss G: 3.5560\n",
      "Epoch [6/10] Batch 386/510               Loss D: 0.0408, loss G: 3.5233\n",
      "Epoch [6/10] Batch 387/510               Loss D: 0.0415, loss G: 3.5563\n",
      "Epoch [6/10] Batch 388/510               Loss D: 0.0337, loss G: 3.6498\n",
      "Epoch [6/10] Batch 389/510               Loss D: 0.0380, loss G: 3.5553\n",
      "Epoch [6/10] Batch 390/510               Loss D: 0.0421, loss G: 3.7000\n",
      "Epoch [6/10] Batch 391/510               Loss D: 0.0509, loss G: 3.5903\n",
      "Epoch [6/10] Batch 392/510               Loss D: 0.0378, loss G: 3.5604\n",
      "Epoch [6/10] Batch 393/510               Loss D: 0.0348, loss G: 3.6274\n",
      "Epoch [6/10] Batch 394/510               Loss D: 0.0354, loss G: 3.5889\n",
      "Epoch [6/10] Batch 395/510               Loss D: 0.0349, loss G: 3.5217\n",
      "Epoch [6/10] Batch 396/510               Loss D: 0.0329, loss G: 3.6004\n",
      "Epoch [6/10] Batch 397/510               Loss D: 0.0336, loss G: 3.6399\n",
      "Epoch [6/10] Batch 398/510               Loss D: 0.0383, loss G: 3.5749\n",
      "Epoch [6/10] Batch 399/510               Loss D: 0.0335, loss G: 3.5537\n",
      "Epoch [6/10] Batch 400/510               Loss D: 0.0345, loss G: 3.5435\n",
      "Epoch [6/10] Batch 401/510               Loss D: 0.0328, loss G: 3.6042\n",
      "Epoch [6/10] Batch 402/510               Loss D: 0.0397, loss G: 3.5215\n",
      "Epoch [6/10] Batch 403/510               Loss D: 0.0311, loss G: 3.6164\n",
      "Epoch [6/10] Batch 404/510               Loss D: 0.0333, loss G: 3.5782\n",
      "Epoch [6/10] Batch 405/510               Loss D: 0.0450, loss G: 3.5212\n",
      "Epoch [6/10] Batch 406/510               Loss D: 0.0341, loss G: 3.5355\n",
      "Epoch [6/10] Batch 407/510               Loss D: 0.0542, loss G: 3.5568\n",
      "Epoch [6/10] Batch 408/510               Loss D: 0.0314, loss G: 3.5864\n",
      "Epoch [6/10] Batch 409/510               Loss D: 0.0346, loss G: 3.5084\n",
      "Epoch [6/10] Batch 410/510               Loss D: 0.0361, loss G: 3.4697\n",
      "Epoch [6/10] Batch 411/510               Loss D: 0.0335, loss G: 3.5280\n",
      "Epoch [6/10] Batch 412/510               Loss D: 0.0319, loss G: 3.5746\n",
      "Epoch [6/10] Batch 413/510               Loss D: 0.0338, loss G: 3.5641\n",
      "Epoch [6/10] Batch 414/510               Loss D: 0.0340, loss G: 3.5423\n",
      "Epoch [6/10] Batch 415/510               Loss D: 0.0376, loss G: 3.5482\n",
      "Epoch [6/10] Batch 416/510               Loss D: 0.0412, loss G: 3.5792\n",
      "Epoch [6/10] Batch 417/510               Loss D: 0.0338, loss G: 3.5573\n",
      "Epoch [6/10] Batch 418/510               Loss D: 0.0341, loss G: 3.5722\n",
      "Epoch [6/10] Batch 419/510               Loss D: 0.0339, loss G: 3.5568\n",
      "Epoch [6/10] Batch 420/510               Loss D: 0.0286, loss G: 3.6295\n",
      "Epoch [6/10] Batch 421/510               Loss D: 0.0410, loss G: 3.6421\n",
      "Epoch [6/10] Batch 422/510               Loss D: 0.0335, loss G: 3.5623\n",
      "Epoch [6/10] Batch 423/510               Loss D: 0.0725, loss G: 3.5565\n",
      "Epoch [6/10] Batch 424/510               Loss D: 0.0315, loss G: 3.5582\n",
      "Epoch [6/10] Batch 425/510               Loss D: 0.0555, loss G: 3.6306\n",
      "Epoch [6/10] Batch 426/510               Loss D: 0.1910, loss G: 3.5464\n",
      "Epoch [6/10] Batch 427/510               Loss D: 0.4572, loss G: 3.4112\n",
      "Epoch [6/10] Batch 428/510               Loss D: 0.0399, loss G: 3.2524\n",
      "Epoch [6/10] Batch 429/510               Loss D: 0.0398, loss G: 3.2112\n",
      "Epoch [6/10] Batch 430/510               Loss D: 0.0470, loss G: 3.1425\n",
      "Epoch [6/10] Batch 431/510               Loss D: 0.0611, loss G: 3.0147\n",
      "Epoch [6/10] Batch 432/510               Loss D: 0.0498, loss G: 3.0446\n",
      "Epoch [6/10] Batch 433/510               Loss D: 0.2019, loss G: 2.9935\n",
      "Epoch [6/10] Batch 434/510               Loss D: 0.1456, loss G: 2.9240\n",
      "Epoch [6/10] Batch 435/510               Loss D: 0.0560, loss G: 2.9388\n",
      "Epoch [6/10] Batch 436/510               Loss D: 0.1680, loss G: 2.9375\n",
      "Epoch [6/10] Batch 437/510               Loss D: 0.0575, loss G: 2.9395\n",
      "Epoch [6/10] Batch 438/510               Loss D: 0.0566, loss G: 2.9912\n",
      "Epoch [6/10] Batch 439/510               Loss D: 0.0521, loss G: 3.0556\n",
      "Epoch [6/10] Batch 440/510               Loss D: 0.0467, loss G: 3.2037\n",
      "Epoch [6/10] Batch 441/510               Loss D: 0.0509, loss G: 3.1836\n",
      "Epoch [6/10] Batch 442/510               Loss D: 0.0437, loss G: 3.3246\n",
      "Epoch [6/10] Batch 443/510               Loss D: 0.0569, loss G: 3.4123\n",
      "Epoch [6/10] Batch 444/510               Loss D: 0.0548, loss G: 3.5598\n",
      "Epoch [6/10] Batch 445/510               Loss D: 0.0359, loss G: 3.5643\n",
      "Epoch [6/10] Batch 446/510               Loss D: 0.0338, loss G: 3.6666\n",
      "Epoch [6/10] Batch 447/510               Loss D: 0.0294, loss G: 3.7989\n",
      "Epoch [6/10] Batch 448/510               Loss D: 0.0306, loss G: 3.8812\n",
      "Epoch [6/10] Batch 449/510               Loss D: 0.0272, loss G: 3.9242\n",
      "Epoch [6/10] Batch 450/510               Loss D: 0.0238, loss G: 3.9846\n",
      "Epoch [6/10] Batch 451/510               Loss D: 0.0283, loss G: 3.9762\n",
      "Epoch [6/10] Batch 452/510               Loss D: 0.0250, loss G: 4.0003\n",
      "Epoch [6/10] Batch 453/510               Loss D: 0.0225, loss G: 4.0777\n",
      "Epoch [6/10] Batch 454/510               Loss D: 0.0252, loss G: 4.0296\n",
      "Epoch [6/10] Batch 455/510               Loss D: 0.0243, loss G: 4.0571\n",
      "Epoch [6/10] Batch 456/510               Loss D: 0.0244, loss G: 4.0511\n",
      "Epoch [6/10] Batch 457/510               Loss D: 0.0236, loss G: 4.0913\n",
      "Epoch [6/10] Batch 458/510               Loss D: 0.0257, loss G: 4.0503\n",
      "Epoch [6/10] Batch 459/510               Loss D: 0.0253, loss G: 4.0216\n",
      "Epoch [6/10] Batch 460/510               Loss D: 0.0245, loss G: 4.0236\n",
      "Epoch [6/10] Batch 461/510               Loss D: 0.0265, loss G: 3.9822\n",
      "Epoch [6/10] Batch 462/510               Loss D: 0.0312, loss G: 3.9127\n",
      "Epoch [6/10] Batch 463/510               Loss D: 0.0284, loss G: 3.9771\n",
      "Epoch [6/10] Batch 464/510               Loss D: 0.0299, loss G: 3.9157\n",
      "Epoch [6/10] Batch 465/510               Loss D: 0.0290, loss G: 3.8780\n",
      "Epoch [6/10] Batch 466/510               Loss D: 0.0408, loss G: 3.9143\n",
      "Epoch [6/10] Batch 467/510               Loss D: 0.2490, loss G: 3.8167\n",
      "Epoch [6/10] Batch 468/510               Loss D: 0.0366, loss G: 3.6481\n",
      "Epoch [6/10] Batch 469/510               Loss D: 0.0421, loss G: 3.5439\n",
      "Epoch [6/10] Batch 470/510               Loss D: 0.0399, loss G: 3.5827\n",
      "Epoch [6/10] Batch 471/510               Loss D: 0.0401, loss G: 3.5090\n",
      "Epoch [6/10] Batch 472/510               Loss D: 0.0387, loss G: 3.5380\n",
      "Epoch [6/10] Batch 473/510               Loss D: 0.0430, loss G: 3.5442\n",
      "Epoch [6/10] Batch 474/510               Loss D: 0.0424, loss G: 3.4424\n",
      "Epoch [6/10] Batch 475/510               Loss D: 0.0396, loss G: 3.5674\n",
      "Epoch [6/10] Batch 476/510               Loss D: 0.0458, loss G: 3.5024\n",
      "Epoch [6/10] Batch 477/510               Loss D: 0.0427, loss G: 3.5692\n",
      "Epoch [6/10] Batch 478/510               Loss D: 0.0508, loss G: 3.5619\n",
      "Epoch [6/10] Batch 479/510               Loss D: 1.1066, loss G: 3.4536\n",
      "Epoch [6/10] Batch 480/510               Loss D: 0.1341, loss G: 3.1776\n",
      "Epoch [6/10] Batch 481/510               Loss D: 0.4322, loss G: 2.9806\n",
      "Epoch [6/10] Batch 482/510               Loss D: 0.0603, loss G: 2.7465\n",
      "Epoch [6/10] Batch 483/510               Loss D: 0.0683, loss G: 2.6440\n",
      "Epoch [6/10] Batch 484/510               Loss D: 0.0810, loss G: 2.5880\n",
      "Epoch [6/10] Batch 485/510               Loss D: 0.0846, loss G: 2.5634\n",
      "Epoch [6/10] Batch 486/510               Loss D: 0.0873, loss G: 2.5727\n",
      "Epoch [6/10] Batch 487/510               Loss D: 0.0851, loss G: 2.5967\n",
      "Epoch [6/10] Batch 488/510               Loss D: 0.0795, loss G: 2.7169\n",
      "Epoch [6/10] Batch 489/510               Loss D: 0.0918, loss G: 2.8339\n",
      "Epoch [6/10] Batch 490/510               Loss D: 0.0671, loss G: 2.9961\n",
      "Epoch [6/10] Batch 491/510               Loss D: 0.0596, loss G: 3.1143\n",
      "Epoch [6/10] Batch 492/510               Loss D: 0.0551, loss G: 3.2327\n",
      "Epoch [6/10] Batch 493/510               Loss D: 0.0442, loss G: 3.4832\n",
      "Epoch [6/10] Batch 494/510               Loss D: 0.0349, loss G: 3.6961\n",
      "Epoch [6/10] Batch 495/510               Loss D: 0.0349, loss G: 3.7148\n",
      "Epoch [6/10] Batch 496/510               Loss D: 0.0323, loss G: 3.8365\n",
      "Epoch [6/10] Batch 497/510               Loss D: 0.0284, loss G: 4.0307\n",
      "Epoch [6/10] Batch 498/510               Loss D: 0.0259, loss G: 4.0744\n",
      "Epoch [6/10] Batch 499/510               Loss D: 0.0209, loss G: 4.2230\n",
      "Epoch [6/10] Batch 500/510               Loss D: 0.0214, loss G: 4.3290\n",
      "Epoch [6/10] Batch 501/510               Loss D: 0.0227, loss G: 4.2453\n",
      "Epoch [6/10] Batch 502/510               Loss D: 0.0217, loss G: 4.2682\n",
      "Epoch [6/10] Batch 503/510               Loss D: 0.0204, loss G: 4.3250\n",
      "Epoch [6/10] Batch 504/510               Loss D: 0.5080, loss G: 4.2162\n",
      "Epoch [6/10] Batch 505/510               Loss D: 0.0223, loss G: 4.0478\n",
      "Epoch [6/10] Batch 506/510               Loss D: 0.0239, loss G: 3.8716\n",
      "Epoch [6/10] Batch 507/510               Loss D: 0.0301, loss G: 3.7544\n",
      "Epoch [6/10] Batch 508/510               Loss D: 0.0355, loss G: 3.6153\n",
      "Epoch [6/10] Batch 509/510               Loss D: 0.0400, loss G: 3.5276\n",
      "Epoch [7/10] Batch 0/510               Loss D: 0.0561, loss G: 3.4283\n",
      "Epoch [7/10] Batch 1/510               Loss D: 0.0502, loss G: 3.3829\n",
      "Epoch [7/10] Batch 2/510               Loss D: 0.0470, loss G: 3.4466\n",
      "Epoch [7/10] Batch 3/510               Loss D: 0.0536, loss G: 3.2856\n",
      "Epoch [7/10] Batch 4/510               Loss D: 0.0544, loss G: 3.3225\n",
      "Epoch [7/10] Batch 5/510               Loss D: 0.0530, loss G: 3.4507\n",
      "Epoch [7/10] Batch 6/510               Loss D: 0.0517, loss G: 3.5248\n",
      "Epoch [7/10] Batch 7/510               Loss D: 0.0479, loss G: 3.5881\n",
      "Epoch [7/10] Batch 8/510               Loss D: 0.0480, loss G: 3.5606\n",
      "Epoch [7/10] Batch 9/510               Loss D: 0.0470, loss G: 3.7012\n",
      "Epoch [7/10] Batch 10/510               Loss D: 0.0459, loss G: 3.7228\n",
      "Epoch [7/10] Batch 11/510               Loss D: 0.0465, loss G: 3.7305\n",
      "Epoch [7/10] Batch 12/510               Loss D: 0.0413, loss G: 3.8566\n",
      "Epoch [7/10] Batch 13/510               Loss D: 0.0378, loss G: 4.0230\n",
      "Epoch [7/10] Batch 14/510               Loss D: 0.0387, loss G: 4.0201\n",
      "Epoch [7/10] Batch 15/510               Loss D: 0.0332, loss G: 4.1658\n",
      "Epoch [7/10] Batch 16/510               Loss D: 0.0271, loss G: 4.2977\n",
      "Epoch [7/10] Batch 17/510               Loss D: 0.0688, loss G: 4.2326\n",
      "Epoch [7/10] Batch 18/510               Loss D: 0.0274, loss G: 4.2984\n",
      "Epoch [7/10] Batch 19/510               Loss D: 0.0268, loss G: 4.2307\n",
      "Epoch [7/10] Batch 20/510               Loss D: 0.0263, loss G: 4.2938\n",
      "Epoch [7/10] Batch 21/510               Loss D: 0.0271, loss G: 4.3190\n",
      "Epoch [7/10] Batch 22/510               Loss D: 0.0234, loss G: 4.4386\n",
      "Epoch [7/10] Batch 23/510               Loss D: 0.0252, loss G: 4.3009\n",
      "Epoch [7/10] Batch 24/510               Loss D: 0.0220, loss G: 4.3324\n",
      "Epoch [7/10] Batch 25/510               Loss D: 0.0566, loss G: 4.3539\n",
      "Epoch [7/10] Batch 26/510               Loss D: 0.0220, loss G: 4.3589\n",
      "Epoch [7/10] Batch 27/510               Loss D: 0.0221, loss G: 4.2975\n",
      "Epoch [7/10] Batch 28/510               Loss D: 0.0225, loss G: 4.3637\n",
      "Epoch [7/10] Batch 29/510               Loss D: 0.0219, loss G: 4.3070\n",
      "Epoch [7/10] Batch 30/510               Loss D: 0.1855, loss G: 4.2962\n",
      "Epoch [7/10] Batch 31/510               Loss D: 0.0260, loss G: 4.0632\n",
      "Epoch [7/10] Batch 32/510               Loss D: 0.0260, loss G: 4.0268\n",
      "Epoch [7/10] Batch 33/510               Loss D: 0.0346, loss G: 3.9228\n",
      "Epoch [7/10] Batch 34/510               Loss D: 0.3639, loss G: 3.6929\n",
      "Epoch [7/10] Batch 35/510               Loss D: 0.0350, loss G: 3.4227\n",
      "Epoch [7/10] Batch 36/510               Loss D: 0.0394, loss G: 3.3905\n",
      "Epoch [7/10] Batch 37/510               Loss D: 0.0419, loss G: 3.2515\n",
      "Epoch [7/10] Batch 38/510               Loss D: 0.0458, loss G: 3.2012\n",
      "Epoch [7/10] Batch 39/510               Loss D: 0.1375, loss G: 3.1488\n",
      "Epoch [7/10] Batch 40/510               Loss D: 0.0468, loss G: 3.1197\n",
      "Epoch [7/10] Batch 41/510               Loss D: 0.0556, loss G: 3.1296\n",
      "Epoch [7/10] Batch 42/510               Loss D: 0.0465, loss G: 3.1451\n",
      "Epoch [7/10] Batch 43/510               Loss D: 0.0449, loss G: 3.1997\n",
      "Epoch [7/10] Batch 44/510               Loss D: 0.0405, loss G: 3.3040\n",
      "Epoch [7/10] Batch 45/510               Loss D: 0.0361, loss G: 3.4185\n",
      "Epoch [7/10] Batch 46/510               Loss D: 0.0308, loss G: 3.5534\n",
      "Epoch [7/10] Batch 47/510               Loss D: 0.0276, loss G: 3.6569\n",
      "Epoch [7/10] Batch 48/510               Loss D: 0.0257, loss G: 3.7495\n",
      "Epoch [7/10] Batch 49/510               Loss D: 0.0389, loss G: 3.8682\n",
      "Epoch [7/10] Batch 50/510               Loss D: 0.0206, loss G: 3.9378\n",
      "Epoch [7/10] Batch 51/510               Loss D: 0.0188, loss G: 4.0719\n",
      "Epoch [7/10] Batch 52/510               Loss D: 0.0197, loss G: 4.0738\n",
      "Epoch [7/10] Batch 53/510               Loss D: 0.0159, loss G: 4.2215\n",
      "Epoch [7/10] Batch 54/510               Loss D: 0.0158, loss G: 4.2254\n",
      "Epoch [7/10] Batch 55/510               Loss D: 0.0140, loss G: 4.3213\n",
      "Epoch [7/10] Batch 56/510               Loss D: 0.0132, loss G: 4.3918\n",
      "Epoch [7/10] Batch 57/510               Loss D: 0.0134, loss G: 4.3367\n",
      "Epoch [7/10] Batch 58/510               Loss D: 0.2476, loss G: 4.2540\n",
      "Epoch [7/10] Batch 59/510               Loss D: 0.1032, loss G: 4.0704\n",
      "Epoch [7/10] Batch 60/510               Loss D: 0.0176, loss G: 3.9928\n",
      "Epoch [7/10] Batch 61/510               Loss D: 0.0719, loss G: 3.8255\n",
      "Epoch [7/10] Batch 62/510               Loss D: 0.0221, loss G: 3.7113\n",
      "Epoch [7/10] Batch 63/510               Loss D: 0.0274, loss G: 3.5379\n",
      "Epoch [7/10] Batch 64/510               Loss D: 0.0294, loss G: 3.5104\n",
      "Epoch [7/10] Batch 65/510               Loss D: 0.0315, loss G: 3.4341\n",
      "Epoch [7/10] Batch 66/510               Loss D: 0.0328, loss G: 3.4493\n",
      "Epoch [7/10] Batch 67/510               Loss D: 0.0337, loss G: 3.4560\n",
      "Epoch [7/10] Batch 68/510               Loss D: 0.0319, loss G: 3.5246\n",
      "Epoch [7/10] Batch 69/510               Loss D: 0.0336, loss G: 3.4769\n",
      "Epoch [7/10] Batch 70/510               Loss D: 0.0406, loss G: 3.5508\n",
      "Epoch [7/10] Batch 71/510               Loss D: 0.0828, loss G: 3.5316\n",
      "Epoch [7/10] Batch 72/510               Loss D: 0.0301, loss G: 3.6013\n",
      "Epoch [7/10] Batch 73/510               Loss D: 0.0320, loss G: 3.5620\n",
      "Epoch [7/10] Batch 74/510               Loss D: 0.0317, loss G: 3.6579\n",
      "Epoch [7/10] Batch 75/510               Loss D: 0.0287, loss G: 3.7262\n",
      "Epoch [7/10] Batch 76/510               Loss D: 0.0280, loss G: 3.7686\n",
      "Epoch [7/10] Batch 77/510               Loss D: 0.0267, loss G: 3.7913\n",
      "Epoch [7/10] Batch 78/510               Loss D: 0.0254, loss G: 3.8502\n",
      "Epoch [7/10] Batch 79/510               Loss D: 0.0226, loss G: 3.9478\n",
      "Epoch [7/10] Batch 80/510               Loss D: 0.0219, loss G: 4.0023\n",
      "Epoch [7/10] Batch 81/510               Loss D: 0.0196, loss G: 4.1435\n",
      "Epoch [7/10] Batch 82/510               Loss D: 0.0222, loss G: 4.0736\n",
      "Epoch [7/10] Batch 83/510               Loss D: 0.0174, loss G: 4.2179\n",
      "Epoch [7/10] Batch 84/510               Loss D: 0.0198, loss G: 4.1945\n",
      "Epoch [7/10] Batch 85/510               Loss D: 0.0188, loss G: 4.2469\n",
      "Epoch [7/10] Batch 86/510               Loss D: 0.0203, loss G: 4.2209\n",
      "Epoch [7/10] Batch 87/510               Loss D: 0.0360, loss G: 4.2533\n",
      "Epoch [7/10] Batch 88/510               Loss D: 0.0317, loss G: 4.0905\n",
      "Epoch [7/10] Batch 89/510               Loss D: 0.0194, loss G: 4.1202\n",
      "Epoch [7/10] Batch 90/510               Loss D: 0.4294, loss G: 3.9334\n",
      "Epoch [7/10] Batch 91/510               Loss D: 0.0227, loss G: 3.8009\n",
      "Epoch [7/10] Batch 92/510               Loss D: 0.0234, loss G: 3.7203\n",
      "Epoch [7/10] Batch 93/510               Loss D: 0.0285, loss G: 3.5615\n",
      "Epoch [7/10] Batch 94/510               Loss D: 0.0316, loss G: 3.4294\n",
      "Epoch [7/10] Batch 95/510               Loss D: 0.0326, loss G: 3.3758\n",
      "Epoch [7/10] Batch 96/510               Loss D: 0.0363, loss G: 3.3178\n",
      "Epoch [7/10] Batch 97/510               Loss D: 0.0378, loss G: 3.2409\n",
      "Epoch [7/10] Batch 98/510               Loss D: 0.0348, loss G: 3.3237\n",
      "Epoch [7/10] Batch 99/510               Loss D: 0.0335, loss G: 3.3929\n",
      "Epoch [7/10] Batch 100/510               Loss D: 0.0394, loss G: 3.4310\n",
      "Epoch [7/10] Batch 101/510               Loss D: 0.0286, loss G: 3.5152\n",
      "Epoch [7/10] Batch 102/510               Loss D: 0.0271, loss G: 3.5461\n",
      "Epoch [7/10] Batch 103/510               Loss D: 0.0269, loss G: 3.5874\n",
      "Epoch [7/10] Batch 104/510               Loss D: 0.0253, loss G: 3.6687\n",
      "Epoch [7/10] Batch 105/510               Loss D: 0.0223, loss G: 3.7314\n",
      "Epoch [7/10] Batch 106/510               Loss D: 0.0223, loss G: 3.7778\n",
      "Epoch [7/10] Batch 107/510               Loss D: 0.0251, loss G: 3.7985\n",
      "Epoch [7/10] Batch 108/510               Loss D: 0.0199, loss G: 3.9258\n",
      "Epoch [7/10] Batch 109/510               Loss D: 0.0181, loss G: 3.9391\n",
      "Epoch [7/10] Batch 110/510               Loss D: 0.0176, loss G: 4.0301\n",
      "Epoch [7/10] Batch 111/510               Loss D: 0.0395, loss G: 3.9680\n",
      "Epoch [7/10] Batch 112/510               Loss D: 0.0161, loss G: 4.0278\n",
      "Epoch [7/10] Batch 113/510               Loss D: 0.0161, loss G: 3.9953\n",
      "Epoch [7/10] Batch 114/510               Loss D: 0.0170, loss G: 4.0004\n",
      "Epoch [7/10] Batch 115/510               Loss D: 0.0202, loss G: 4.0209\n",
      "Epoch [7/10] Batch 116/510               Loss D: 0.0158, loss G: 3.9989\n",
      "Epoch [7/10] Batch 117/510               Loss D: 0.2608, loss G: 3.9683\n",
      "Epoch [7/10] Batch 118/510               Loss D: 0.0166, loss G: 3.8754\n",
      "Epoch [7/10] Batch 119/510               Loss D: 0.0674, loss G: 3.7495\n",
      "Epoch [7/10] Batch 120/510               Loss D: 0.0212, loss G: 3.6052\n",
      "Epoch [7/10] Batch 121/510               Loss D: 0.0288, loss G: 3.5421\n",
      "Epoch [7/10] Batch 122/510               Loss D: 0.0225, loss G: 3.5170\n",
      "Epoch [7/10] Batch 123/510               Loss D: 0.6315, loss G: 3.2692\n",
      "Epoch [7/10] Batch 124/510               Loss D: 0.0303, loss G: 3.0615\n",
      "Epoch [7/10] Batch 125/510               Loss D: 0.0330, loss G: 3.0161\n",
      "Epoch [7/10] Batch 126/510               Loss D: 0.0391, loss G: 2.8883\n",
      "Epoch [7/10] Batch 127/510               Loss D: 0.0487, loss G: 2.7742\n",
      "Epoch [7/10] Batch 128/510               Loss D: 0.0462, loss G: 2.8154\n",
      "Epoch [7/10] Batch 129/510               Loss D: 0.0480, loss G: 2.8032\n",
      "Epoch [7/10] Batch 130/510               Loss D: 0.0482, loss G: 2.8175\n",
      "Epoch [7/10] Batch 131/510               Loss D: 0.0497, loss G: 2.8342\n",
      "Epoch [7/10] Batch 132/510               Loss D: 0.0458, loss G: 2.9526\n",
      "Epoch [7/10] Batch 133/510               Loss D: 0.0445, loss G: 2.9960\n",
      "Epoch [7/10] Batch 134/510               Loss D: 0.0450, loss G: 3.0108\n",
      "Epoch [7/10] Batch 135/510               Loss D: 0.0378, loss G: 3.2146\n",
      "Epoch [7/10] Batch 136/510               Loss D: 0.0375, loss G: 3.2327\n",
      "Epoch [7/10] Batch 137/510               Loss D: 0.0314, loss G: 3.4351\n",
      "Epoch [7/10] Batch 138/510               Loss D: 0.0279, loss G: 3.5342\n",
      "Epoch [7/10] Batch 139/510               Loss D: 0.0345, loss G: 3.6248\n",
      "Epoch [7/10] Batch 140/510               Loss D: 0.0546, loss G: 3.7044\n",
      "Epoch [7/10] Batch 141/510               Loss D: 0.0317, loss G: 3.7210\n",
      "Epoch [7/10] Batch 142/510               Loss D: 0.0198, loss G: 3.8677\n",
      "Epoch [7/10] Batch 143/510               Loss D: 0.0207, loss G: 3.8685\n",
      "Epoch [7/10] Batch 144/510               Loss D: 0.0195, loss G: 3.8735\n",
      "Epoch [7/10] Batch 145/510               Loss D: 0.0190, loss G: 3.9021\n",
      "Epoch [7/10] Batch 146/510               Loss D: 0.0169, loss G: 4.0722\n",
      "Epoch [7/10] Batch 147/510               Loss D: 0.0177, loss G: 3.9974\n",
      "Epoch [7/10] Batch 148/510               Loss D: 0.0166, loss G: 4.0519\n",
      "Epoch [7/10] Batch 149/510               Loss D: 0.0162, loss G: 4.0512\n",
      "Epoch [7/10] Batch 150/510               Loss D: 0.0160, loss G: 4.0649\n",
      "Epoch [7/10] Batch 151/510               Loss D: 0.0171, loss G: 3.9825\n",
      "Epoch [7/10] Batch 152/510               Loss D: 0.0162, loss G: 4.0547\n",
      "Epoch [7/10] Batch 153/510               Loss D: 0.0159, loss G: 4.1003\n",
      "Epoch [7/10] Batch 154/510               Loss D: 0.0165, loss G: 4.0401\n",
      "Epoch [7/10] Batch 155/510               Loss D: 0.0183, loss G: 4.0251\n",
      "Epoch [7/10] Batch 156/510               Loss D: 0.0208, loss G: 4.0284\n",
      "Epoch [7/10] Batch 157/510               Loss D: 0.0189, loss G: 3.9480\n",
      "Epoch [7/10] Batch 158/510               Loss D: 0.0184, loss G: 3.9575\n",
      "Epoch [7/10] Batch 159/510               Loss D: 0.0276, loss G: 3.8804\n",
      "Epoch [7/10] Batch 160/510               Loss D: 0.0186, loss G: 3.8713\n",
      "Epoch [7/10] Batch 161/510               Loss D: 0.4828, loss G: 3.7019\n",
      "Epoch [7/10] Batch 162/510               Loss D: 0.0229, loss G: 3.5583\n",
      "Epoch [7/10] Batch 163/510               Loss D: 0.0595, loss G: 3.4406\n",
      "Epoch [7/10] Batch 164/510               Loss D: 0.0467, loss G: 3.3513\n",
      "Epoch [7/10] Batch 165/510               Loss D: 0.0311, loss G: 3.2271\n",
      "Epoch [7/10] Batch 166/510               Loss D: 0.0343, loss G: 3.1815\n",
      "Epoch [7/10] Batch 167/510               Loss D: 0.0359, loss G: 3.1623\n",
      "Epoch [7/10] Batch 168/510               Loss D: 0.0390, loss G: 3.0910\n",
      "Epoch [7/10] Batch 169/510               Loss D: 0.0381, loss G: 3.1333\n",
      "Epoch [7/10] Batch 170/510               Loss D: 0.0398, loss G: 3.1274\n",
      "Epoch [7/10] Batch 171/510               Loss D: 0.0397, loss G: 3.1455\n",
      "Epoch [7/10] Batch 172/510               Loss D: 0.0380, loss G: 3.2037\n",
      "Epoch [7/10] Batch 173/510               Loss D: 0.0480, loss G: 3.3025\n",
      "Epoch [7/10] Batch 174/510               Loss D: 0.0980, loss G: 3.2644\n",
      "Epoch [7/10] Batch 175/510               Loss D: 0.0325, loss G: 3.3178\n",
      "Epoch [7/10] Batch 176/510               Loss D: 0.0309, loss G: 3.4025\n",
      "Epoch [7/10] Batch 177/510               Loss D: 0.0309, loss G: 3.4265\n",
      "Epoch [7/10] Batch 178/510               Loss D: 0.0273, loss G: 3.5303\n",
      "Epoch [7/10] Batch 179/510               Loss D: 0.0275, loss G: 3.5284\n",
      "Epoch [7/10] Batch 180/510               Loss D: 0.0251, loss G: 3.6443\n",
      "Epoch [7/10] Batch 181/510               Loss D: 0.0250, loss G: 3.6577\n",
      "Epoch [7/10] Batch 182/510               Loss D: 0.0241, loss G: 3.7349\n",
      "Epoch [7/10] Batch 183/510               Loss D: 0.0229, loss G: 3.7651\n",
      "Epoch [7/10] Batch 184/510               Loss D: 0.0220, loss G: 3.7653\n",
      "Epoch [7/10] Batch 185/510               Loss D: 0.0194, loss G: 3.9125\n",
      "Epoch [7/10] Batch 186/510               Loss D: 0.0196, loss G: 3.8906\n",
      "Epoch [7/10] Batch 187/510               Loss D: 0.0179, loss G: 3.9841\n",
      "Epoch [7/10] Batch 188/510               Loss D: 0.0180, loss G: 3.9572\n",
      "Epoch [7/10] Batch 189/510               Loss D: 0.0176, loss G: 3.9842\n",
      "Epoch [7/10] Batch 190/510               Loss D: 0.0151, loss G: 4.1020\n",
      "Epoch [7/10] Batch 191/510               Loss D: 0.0168, loss G: 4.0400\n",
      "Epoch [7/10] Batch 192/510               Loss D: 0.0163, loss G: 4.0351\n",
      "Epoch [7/10] Batch 193/510               Loss D: 0.0382, loss G: 4.0418\n",
      "Epoch [7/10] Batch 194/510               Loss D: 0.0162, loss G: 4.0383\n",
      "Epoch [7/10] Batch 195/510               Loss D: 0.0177, loss G: 3.9548\n",
      "Epoch [7/10] Batch 196/510               Loss D: 0.0172, loss G: 3.9845\n",
      "Epoch [7/10] Batch 197/510               Loss D: 0.0179, loss G: 3.9682\n",
      "Epoch [7/10] Batch 198/510               Loss D: 0.0224, loss G: 3.9437\n",
      "Epoch [7/10] Batch 199/510               Loss D: 0.0189, loss G: 3.9210\n",
      "Epoch [7/10] Batch 200/510               Loss D: 0.0184, loss G: 3.8949\n",
      "Epoch [7/10] Batch 201/510               Loss D: 0.0220, loss G: 3.9003\n",
      "Epoch [7/10] Batch 202/510               Loss D: 0.0198, loss G: 3.8765\n",
      "Epoch [7/10] Batch 203/510               Loss D: 0.0200, loss G: 3.9104\n",
      "Epoch [7/10] Batch 204/510               Loss D: 0.0217, loss G: 3.8710\n",
      "Epoch [7/10] Batch 205/510               Loss D: 0.0193, loss G: 3.8542\n",
      "Epoch [7/10] Batch 206/510               Loss D: 0.0194, loss G: 3.8438\n",
      "Epoch [7/10] Batch 207/510               Loss D: 0.0214, loss G: 3.7856\n",
      "Epoch [7/10] Batch 208/510               Loss D: 0.0206, loss G: 3.8679\n",
      "Epoch [7/10] Batch 209/510               Loss D: 0.0209, loss G: 3.8379\n",
      "Epoch [7/10] Batch 210/510               Loss D: 0.5352, loss G: 3.6668\n",
      "Epoch [7/10] Batch 211/510               Loss D: 0.0244, loss G: 3.5035\n",
      "Epoch [7/10] Batch 212/510               Loss D: 0.0449, loss G: 3.3875\n",
      "Epoch [7/10] Batch 213/510               Loss D: 0.0528, loss G: 3.3399\n",
      "Epoch [7/10] Batch 214/510               Loss D: 0.0349, loss G: 3.2566\n",
      "Epoch [7/10] Batch 215/510               Loss D: 0.2103, loss G: 3.1815\n",
      "Epoch [7/10] Batch 216/510               Loss D: 0.0400, loss G: 3.1320\n",
      "Epoch [7/10] Batch 217/510               Loss D: 0.0467, loss G: 3.0449\n",
      "Epoch [7/10] Batch 218/510               Loss D: 0.0451, loss G: 3.0984\n",
      "Epoch [7/10] Batch 219/510               Loss D: 0.0462, loss G: 3.0742\n",
      "Epoch [7/10] Batch 220/510               Loss D: 0.0442, loss G: 3.1584\n",
      "Epoch [7/10] Batch 221/510               Loss D: 0.0418, loss G: 3.2072\n",
      "Epoch [7/10] Batch 222/510               Loss D: 0.0398, loss G: 3.2880\n",
      "Epoch [7/10] Batch 223/510               Loss D: 0.0416, loss G: 3.2713\n",
      "Epoch [7/10] Batch 224/510               Loss D: 0.0359, loss G: 3.4501\n",
      "Epoch [7/10] Batch 225/510               Loss D: 0.0363, loss G: 3.4155\n",
      "Epoch [7/10] Batch 226/510               Loss D: 0.0305, loss G: 3.5858\n",
      "Epoch [7/10] Batch 227/510               Loss D: 0.0326, loss G: 3.5603\n",
      "Epoch [7/10] Batch 228/510               Loss D: 0.0304, loss G: 3.6482\n",
      "Epoch [7/10] Batch 229/510               Loss D: 0.0270, loss G: 3.7235\n",
      "Epoch [7/10] Batch 230/510               Loss D: 0.0241, loss G: 3.8464\n",
      "Epoch [7/10] Batch 231/510               Loss D: 0.0239, loss G: 3.8292\n",
      "Epoch [7/10] Batch 232/510               Loss D: 0.0395, loss G: 3.8977\n",
      "Epoch [7/10] Batch 233/510               Loss D: 0.0208, loss G: 3.9963\n",
      "Epoch [7/10] Batch 234/510               Loss D: 0.0214, loss G: 3.9495\n",
      "Epoch [7/10] Batch 235/510               Loss D: 0.0201, loss G: 4.0480\n",
      "Epoch [7/10] Batch 236/510               Loss D: 0.0179, loss G: 4.1151\n",
      "Epoch [7/10] Batch 237/510               Loss D: 0.0179, loss G: 4.0965\n",
      "Epoch [7/10] Batch 238/510               Loss D: 0.0193, loss G: 4.0373\n",
      "Epoch [7/10] Batch 239/510               Loss D: 0.0183, loss G: 4.0641\n",
      "Epoch [7/10] Batch 240/510               Loss D: 0.0184, loss G: 4.0558\n",
      "Epoch [7/10] Batch 241/510               Loss D: 0.0175, loss G: 4.0813\n",
      "Epoch [7/10] Batch 242/510               Loss D: 0.0197, loss G: 4.0366\n",
      "Epoch [7/10] Batch 243/510               Loss D: 0.0202, loss G: 4.0062\n",
      "Epoch [7/10] Batch 244/510               Loss D: 0.0192, loss G: 4.0177\n",
      "Epoch [7/10] Batch 245/510               Loss D: 0.0198, loss G: 4.0511\n",
      "Epoch [7/10] Batch 246/510               Loss D: 0.0187, loss G: 4.0194\n",
      "Epoch [7/10] Batch 247/510               Loss D: 0.0189, loss G: 3.9569\n",
      "Epoch [7/10] Batch 248/510               Loss D: 0.0276, loss G: 3.8303\n",
      "Epoch [7/10] Batch 249/510               Loss D: 0.0220, loss G: 3.8922\n",
      "Epoch [7/10] Batch 250/510               Loss D: 0.0224, loss G: 3.8225\n",
      "Epoch [7/10] Batch 251/510               Loss D: 0.0232, loss G: 3.8499\n",
      "Epoch [7/10] Batch 252/510               Loss D: 0.0229, loss G: 3.8453\n",
      "Epoch [7/10] Batch 253/510               Loss D: 0.0238, loss G: 3.7821\n",
      "Epoch [7/10] Batch 254/510               Loss D: 0.0246, loss G: 3.7947\n",
      "Epoch [7/10] Batch 255/510               Loss D: 0.0256, loss G: 3.7175\n",
      "Epoch [7/10] Batch 256/510               Loss D: 0.0232, loss G: 3.7753\n",
      "Epoch [7/10] Batch 257/510               Loss D: 0.1303, loss G: 3.6995\n",
      "Epoch [7/10] Batch 258/510               Loss D: 0.0278, loss G: 3.6206\n",
      "Epoch [7/10] Batch 259/510               Loss D: 0.0305, loss G: 3.5629\n",
      "Epoch [7/10] Batch 260/510               Loss D: 0.0309, loss G: 3.5246\n",
      "Epoch [7/10] Batch 261/510               Loss D: 0.0294, loss G: 3.5817\n",
      "Epoch [7/10] Batch 262/510               Loss D: 0.0308, loss G: 3.5556\n",
      "Epoch [7/10] Batch 263/510               Loss D: 0.0344, loss G: 3.5105\n",
      "Epoch [7/10] Batch 264/510               Loss D: 0.0314, loss G: 3.5524\n",
      "Epoch [7/10] Batch 265/510               Loss D: 0.0284, loss G: 3.5993\n",
      "Epoch [7/10] Batch 266/510               Loss D: 0.0339, loss G: 3.5443\n",
      "Epoch [7/10] Batch 267/510               Loss D: 0.0316, loss G: 3.6082\n",
      "Epoch [7/10] Batch 268/510               Loss D: 0.0335, loss G: 3.5428\n",
      "Epoch [7/10] Batch 269/510               Loss D: 0.0268, loss G: 3.6812\n",
      "Epoch [7/10] Batch 270/510               Loss D: 0.0273, loss G: 3.6743\n",
      "Epoch [7/10] Batch 271/510               Loss D: 0.0285, loss G: 3.6768\n",
      "Epoch [7/10] Batch 272/510               Loss D: 0.0318, loss G: 3.6265\n",
      "Epoch [7/10] Batch 273/510               Loss D: 0.0314, loss G: 3.6985\n",
      "Epoch [7/10] Batch 274/510               Loss D: 0.0267, loss G: 3.7448\n",
      "Epoch [7/10] Batch 275/510               Loss D: 0.0257, loss G: 3.7537\n",
      "Epoch [7/10] Batch 276/510               Loss D: 0.0260, loss G: 3.7699\n",
      "Epoch [7/10] Batch 277/510               Loss D: 0.3500, loss G: 3.7973\n",
      "Epoch [7/10] Batch 278/510               Loss D: 0.0250, loss G: 3.6828\n",
      "Epoch [7/10] Batch 279/510               Loss D: 0.0278, loss G: 3.6129\n",
      "Epoch [7/10] Batch 280/510               Loss D: 0.0263, loss G: 3.6533\n",
      "Epoch [7/10] Batch 281/510               Loss D: 0.0283, loss G: 3.6625\n",
      "Epoch [7/10] Batch 282/510               Loss D: 0.0318, loss G: 3.5526\n",
      "Epoch [7/10] Batch 283/510               Loss D: 0.1565, loss G: 3.5382\n",
      "Epoch [7/10] Batch 284/510               Loss D: 0.0349, loss G: 3.4728\n",
      "Epoch [7/10] Batch 285/510               Loss D: 0.0334, loss G: 3.4531\n",
      "Epoch [7/10] Batch 286/510               Loss D: 0.0355, loss G: 3.4273\n",
      "Epoch [7/10] Batch 287/510               Loss D: 0.0359, loss G: 3.3972\n",
      "Epoch [7/10] Batch 288/510               Loss D: 0.0358, loss G: 3.4684\n",
      "Epoch [7/10] Batch 289/510               Loss D: 0.0358, loss G: 3.4540\n",
      "Epoch [7/10] Batch 290/510               Loss D: 0.0371, loss G: 3.4152\n",
      "Epoch [7/10] Batch 291/510               Loss D: 0.0331, loss G: 3.4795\n",
      "Epoch [7/10] Batch 292/510               Loss D: 0.0325, loss G: 3.6015\n",
      "Epoch [7/10] Batch 293/510               Loss D: 0.0309, loss G: 3.6101\n",
      "Epoch [7/10] Batch 294/510               Loss D: 0.0292, loss G: 3.6887\n",
      "Epoch [7/10] Batch 295/510               Loss D: 0.0294, loss G: 3.6590\n",
      "Epoch [7/10] Batch 296/510               Loss D: 0.0272, loss G: 3.7361\n",
      "Epoch [7/10] Batch 297/510               Loss D: 0.0247, loss G: 3.8014\n",
      "Epoch [7/10] Batch 298/510               Loss D: 0.0245, loss G: 3.8080\n",
      "Epoch [7/10] Batch 299/510               Loss D: 0.0247, loss G: 3.8117\n",
      "Epoch [7/10] Batch 300/510               Loss D: 0.0322, loss G: 3.8483\n",
      "Epoch [7/10] Batch 301/510               Loss D: 0.0236, loss G: 3.8532\n",
      "Epoch [7/10] Batch 302/510               Loss D: 0.0205, loss G: 4.0110\n",
      "Epoch [7/10] Batch 303/510               Loss D: 0.0194, loss G: 4.0569\n",
      "Epoch [7/10] Batch 304/510               Loss D: 0.0201, loss G: 4.0372\n",
      "Epoch [7/10] Batch 305/510               Loss D: 0.0206, loss G: 3.9675\n",
      "Epoch [7/10] Batch 306/510               Loss D: 0.0211, loss G: 3.9700\n",
      "Epoch [7/10] Batch 307/510               Loss D: 0.0454, loss G: 3.9848\n",
      "Epoch [7/10] Batch 308/510               Loss D: 0.0235, loss G: 4.0393\n",
      "Epoch [7/10] Batch 309/510               Loss D: 0.0185, loss G: 4.1222\n",
      "Epoch [7/10] Batch 310/510               Loss D: 0.0202, loss G: 4.0558\n",
      "Epoch [7/10] Batch 311/510               Loss D: 0.0209, loss G: 3.9488\n",
      "Epoch [7/10] Batch 312/510               Loss D: 0.0192, loss G: 4.0194\n",
      "Epoch [7/10] Batch 313/510               Loss D: 0.0207, loss G: 3.9665\n",
      "Epoch [7/10] Batch 314/510               Loss D: 0.0202, loss G: 3.9474\n",
      "Epoch [7/10] Batch 315/510               Loss D: 0.0213, loss G: 3.9392\n",
      "Epoch [7/10] Batch 316/510               Loss D: 0.0209, loss G: 4.0011\n",
      "Epoch [7/10] Batch 317/510               Loss D: 0.0211, loss G: 3.9817\n",
      "Epoch [7/10] Batch 318/510               Loss D: 0.0957, loss G: 3.9098\n",
      "Epoch [7/10] Batch 319/510               Loss D: 0.0257, loss G: 3.8848\n",
      "Epoch [7/10] Batch 320/510               Loss D: 0.0214, loss G: 3.9245\n",
      "Epoch [7/10] Batch 321/510               Loss D: 0.0565, loss G: 3.8424\n",
      "Epoch [7/10] Batch 322/510               Loss D: 0.0300, loss G: 3.7743\n",
      "Epoch [7/10] Batch 323/510               Loss D: 0.0256, loss G: 3.7093\n",
      "Epoch [7/10] Batch 324/510               Loss D: 0.2117, loss G: 3.6259\n",
      "Epoch [7/10] Batch 325/510               Loss D: 0.0272, loss G: 3.5562\n",
      "Epoch [7/10] Batch 326/510               Loss D: 0.0279, loss G: 3.5397\n",
      "Epoch [7/10] Batch 327/510               Loss D: 0.0348, loss G: 3.3937\n",
      "Epoch [7/10] Batch 328/510               Loss D: 0.0810, loss G: 3.3692\n",
      "Epoch [7/10] Batch 329/510               Loss D: 0.0466, loss G: 3.2685\n",
      "Epoch [7/10] Batch 330/510               Loss D: 0.1464, loss G: 3.2060\n",
      "Epoch [7/10] Batch 331/510               Loss D: 0.0397, loss G: 3.1853\n",
      "Epoch [7/10] Batch 332/510               Loss D: 0.0443, loss G: 3.2654\n",
      "Epoch [7/10] Batch 333/510               Loss D: 0.0374, loss G: 3.2724\n",
      "Epoch [7/10] Batch 334/510               Loss D: 0.0434, loss G: 3.1698\n",
      "Epoch [7/10] Batch 335/510               Loss D: 0.0393, loss G: 3.2441\n",
      "Epoch [7/10] Batch 336/510               Loss D: 0.0384, loss G: 3.2958\n",
      "Epoch [7/10] Batch 337/510               Loss D: 0.0372, loss G: 3.3170\n",
      "Epoch [7/10] Batch 338/510               Loss D: 0.0348, loss G: 3.3530\n",
      "Epoch [7/10] Batch 339/510               Loss D: 0.0338, loss G: 3.3758\n",
      "Epoch [7/10] Batch 340/510               Loss D: 0.0292, loss G: 3.5437\n",
      "Epoch [7/10] Batch 341/510               Loss D: 0.0302, loss G: 3.5566\n",
      "Epoch [7/10] Batch 342/510               Loss D: 0.0509, loss G: 3.6136\n",
      "Epoch [7/10] Batch 343/510               Loss D: 0.0252, loss G: 3.6786\n",
      "Epoch [7/10] Batch 344/510               Loss D: 0.0372, loss G: 3.6415\n",
      "Epoch [7/10] Batch 345/510               Loss D: 0.0227, loss G: 3.7622\n",
      "Epoch [7/10] Batch 346/510               Loss D: 0.0204, loss G: 3.8817\n",
      "Epoch [7/10] Batch 347/510               Loss D: 0.0200, loss G: 3.8514\n",
      "Epoch [7/10] Batch 348/510               Loss D: 0.3610, loss G: 3.8847\n",
      "Epoch [7/10] Batch 349/510               Loss D: 0.0207, loss G: 3.7826\n",
      "Epoch [7/10] Batch 350/510               Loss D: 0.0236, loss G: 3.7420\n",
      "Epoch [7/10] Batch 351/510               Loss D: 0.0229, loss G: 3.7326\n",
      "Epoch [7/10] Batch 352/510               Loss D: 0.0232, loss G: 3.6731\n",
      "Epoch [7/10] Batch 353/510               Loss D: 0.0254, loss G: 3.6528\n",
      "Epoch [7/10] Batch 354/510               Loss D: 0.0265, loss G: 3.6648\n",
      "Epoch [7/10] Batch 355/510               Loss D: 0.0263, loss G: 3.6619\n",
      "Epoch [7/10] Batch 356/510               Loss D: 0.0251, loss G: 3.6762\n",
      "Epoch [7/10] Batch 357/510               Loss D: 0.0282, loss G: 3.5784\n",
      "Epoch [7/10] Batch 358/510               Loss D: 0.0281, loss G: 3.6267\n",
      "Epoch [7/10] Batch 359/510               Loss D: 0.0277, loss G: 3.6427\n",
      "Epoch [7/10] Batch 360/510               Loss D: 0.0265, loss G: 3.6793\n",
      "Epoch [7/10] Batch 361/510               Loss D: 0.0274, loss G: 3.6518\n",
      "Epoch [7/10] Batch 362/510               Loss D: 0.0272, loss G: 3.6860\n",
      "Epoch [7/10] Batch 363/510               Loss D: 0.0289, loss G: 3.6003\n",
      "Epoch [7/10] Batch 364/510               Loss D: 0.0255, loss G: 3.7054\n",
      "Epoch [7/10] Batch 365/510               Loss D: 0.0281, loss G: 3.6646\n",
      "Epoch [7/10] Batch 366/510               Loss D: 0.0261, loss G: 3.7022\n",
      "Epoch [7/10] Batch 367/510               Loss D: 0.0259, loss G: 3.7333\n",
      "Epoch [7/10] Batch 368/510               Loss D: 0.0254, loss G: 3.7805\n",
      "Epoch [7/10] Batch 369/510               Loss D: 0.0263, loss G: 3.7715\n",
      "Epoch [7/10] Batch 370/510               Loss D: 0.0229, loss G: 3.8203\n",
      "Epoch [7/10] Batch 371/510               Loss D: 0.0256, loss G: 3.7622\n",
      "Epoch [7/10] Batch 372/510               Loss D: 0.0250, loss G: 3.7607\n",
      "Epoch [7/10] Batch 373/510               Loss D: 0.0246, loss G: 3.8446\n",
      "Epoch [7/10] Batch 374/510               Loss D: 0.0249, loss G: 3.7972\n",
      "Epoch [7/10] Batch 375/510               Loss D: 0.0253, loss G: 3.8389\n",
      "Epoch [7/10] Batch 376/510               Loss D: 0.0233, loss G: 3.8763\n",
      "Epoch [7/10] Batch 377/510               Loss D: 0.1441, loss G: 3.7700\n",
      "Epoch [7/10] Batch 378/510               Loss D: 0.0253, loss G: 3.7166\n",
      "Epoch [7/10] Batch 379/510               Loss D: 0.0261, loss G: 3.6133\n",
      "Epoch [7/10] Batch 380/510               Loss D: 0.0297, loss G: 3.6893\n",
      "Epoch [7/10] Batch 381/510               Loss D: 0.0266, loss G: 3.5654\n",
      "Epoch [7/10] Batch 382/510               Loss D: 0.0605, loss G: 3.5972\n",
      "Epoch [7/10] Batch 383/510               Loss D: 0.0732, loss G: 3.5615\n",
      "Epoch [7/10] Batch 384/510               Loss D: 0.1009, loss G: 3.4774\n",
      "Epoch [7/10] Batch 385/510               Loss D: 0.0282, loss G: 3.4286\n",
      "Epoch [7/10] Batch 386/510               Loss D: 0.0320, loss G: 3.3894\n",
      "Epoch [7/10] Batch 387/510               Loss D: 0.0300, loss G: 3.4019\n",
      "Epoch [7/10] Batch 388/510               Loss D: 0.0354, loss G: 3.2746\n",
      "Epoch [7/10] Batch 389/510               Loss D: 0.0334, loss G: 3.3105\n",
      "Epoch [7/10] Batch 390/510               Loss D: 0.0396, loss G: 3.2813\n",
      "Epoch [7/10] Batch 391/510               Loss D: 0.0342, loss G: 3.3258\n",
      "Epoch [7/10] Batch 392/510               Loss D: 0.0333, loss G: 3.3340\n",
      "Epoch [7/10] Batch 393/510               Loss D: 0.0320, loss G: 3.4210\n",
      "Epoch [7/10] Batch 394/510               Loss D: 0.0321, loss G: 3.3891\n",
      "Epoch [7/10] Batch 395/510               Loss D: 0.0299, loss G: 3.4396\n",
      "Epoch [7/10] Batch 396/510               Loss D: 0.0311, loss G: 3.4563\n",
      "Epoch [7/10] Batch 397/510               Loss D: 0.0278, loss G: 3.5663\n",
      "Epoch [7/10] Batch 398/510               Loss D: 0.0285, loss G: 3.5789\n",
      "Epoch [7/10] Batch 399/510               Loss D: 0.3813, loss G: 3.4742\n",
      "Epoch [7/10] Batch 400/510               Loss D: 0.0322, loss G: 3.3087\n",
      "Epoch [7/10] Batch 401/510               Loss D: 0.0314, loss G: 3.3483\n",
      "Epoch [7/10] Batch 402/510               Loss D: 0.0347, loss G: 3.2683\n",
      "Epoch [7/10] Batch 403/510               Loss D: 0.0351, loss G: 3.2444\n",
      "Epoch [7/10] Batch 404/510               Loss D: 0.0377, loss G: 3.2118\n",
      "Epoch [7/10] Batch 405/510               Loss D: 0.0386, loss G: 3.2030\n",
      "Epoch [7/10] Batch 406/510               Loss D: 0.0394, loss G: 3.2444\n",
      "Epoch [7/10] Batch 407/510               Loss D: 0.0382, loss G: 3.2273\n",
      "Epoch [7/10] Batch 408/510               Loss D: 0.0373, loss G: 3.3068\n",
      "Epoch [7/10] Batch 409/510               Loss D: 0.0351, loss G: 3.4100\n",
      "Epoch [7/10] Batch 410/510               Loss D: 0.0338, loss G: 3.3990\n",
      "Epoch [7/10] Batch 411/510               Loss D: 0.0332, loss G: 3.4598\n",
      "Epoch [7/10] Batch 412/510               Loss D: 0.0316, loss G: 3.5396\n",
      "Epoch [7/10] Batch 413/510               Loss D: 0.0310, loss G: 3.5737\n",
      "Epoch [7/10] Batch 414/510               Loss D: 1.0362, loss G: 3.3643\n",
      "Epoch [7/10] Batch 415/510               Loss D: 0.0308, loss G: 3.2935\n",
      "Epoch [7/10] Batch 416/510               Loss D: 0.0384, loss G: 3.1372\n",
      "Epoch [7/10] Batch 417/510               Loss D: 0.0415, loss G: 3.0818\n",
      "Epoch [7/10] Batch 418/510               Loss D: 0.0459, loss G: 2.9942\n",
      "Epoch [7/10] Batch 419/510               Loss D: 0.0453, loss G: 3.0522\n",
      "Epoch [7/10] Batch 420/510               Loss D: 0.0462, loss G: 3.0764\n",
      "Epoch [7/10] Batch 421/510               Loss D: 0.0624, loss G: 2.9674\n",
      "Epoch [7/10] Batch 422/510               Loss D: 0.0513, loss G: 3.0438\n",
      "Epoch [7/10] Batch 423/510               Loss D: 0.0464, loss G: 3.1141\n",
      "Epoch [7/10] Batch 424/510               Loss D: 0.0438, loss G: 3.1875\n",
      "Epoch [7/10] Batch 425/510               Loss D: 0.1333, loss G: 3.2678\n",
      "Epoch [7/10] Batch 426/510               Loss D: 0.0371, loss G: 3.3271\n",
      "Epoch [7/10] Batch 427/510               Loss D: 0.0376, loss G: 3.3582\n",
      "Epoch [7/10] Batch 428/510               Loss D: 0.0337, loss G: 3.4628\n",
      "Epoch [7/10] Batch 429/510               Loss D: 0.0331, loss G: 3.4856\n",
      "Epoch [7/10] Batch 430/510               Loss D: 0.0305, loss G: 3.5393\n",
      "Epoch [7/10] Batch 431/510               Loss D: 0.0306, loss G: 3.6324\n",
      "Epoch [7/10] Batch 432/510               Loss D: 0.0282, loss G: 3.7024\n",
      "Epoch [7/10] Batch 433/510               Loss D: 0.0262, loss G: 3.7627\n",
      "Epoch [7/10] Batch 434/510               Loss D: 0.6444, loss G: 3.6017\n",
      "Epoch [7/10] Batch 435/510               Loss D: 0.0321, loss G: 3.3792\n",
      "Epoch [7/10] Batch 436/510               Loss D: 0.0334, loss G: 3.3418\n",
      "Epoch [7/10] Batch 437/510               Loss D: 0.0367, loss G: 3.2983\n",
      "Epoch [7/10] Batch 438/510               Loss D: 0.0426, loss G: 3.1785\n",
      "Epoch [7/10] Batch 439/510               Loss D: 0.0497, loss G: 3.0775\n",
      "Epoch [7/10] Batch 440/510               Loss D: 0.0485, loss G: 3.1339\n",
      "Epoch [7/10] Batch 441/510               Loss D: 0.0537, loss G: 3.0319\n",
      "Epoch [7/10] Batch 442/510               Loss D: 0.0528, loss G: 3.1989\n",
      "Epoch [7/10] Batch 443/510               Loss D: 0.0567, loss G: 3.1519\n",
      "Epoch [7/10] Batch 444/510               Loss D: 0.0495, loss G: 3.2255\n",
      "Epoch [7/10] Batch 445/510               Loss D: 0.0541, loss G: 3.2071\n",
      "Epoch [7/10] Batch 446/510               Loss D: 0.0470, loss G: 3.3912\n",
      "Epoch [7/10] Batch 447/510               Loss D: 0.0406, loss G: 3.5405\n",
      "Epoch [7/10] Batch 448/510               Loss D: 0.0424, loss G: 3.4832\n",
      "Epoch [7/10] Batch 449/510               Loss D: 0.0345, loss G: 3.6824\n",
      "Epoch [7/10] Batch 450/510               Loss D: 0.0330, loss G: 3.7449\n",
      "Epoch [7/10] Batch 451/510               Loss D: 0.0310, loss G: 3.7506\n",
      "Epoch [7/10] Batch 452/510               Loss D: 0.0294, loss G: 3.8274\n",
      "Epoch [7/10] Batch 453/510               Loss D: 0.0246, loss G: 3.9803\n",
      "Epoch [7/10] Batch 454/510               Loss D: 0.0236, loss G: 4.0309\n",
      "Epoch [7/10] Batch 455/510               Loss D: 0.0217, loss G: 4.0799\n",
      "Epoch [7/10] Batch 456/510               Loss D: 0.0234, loss G: 3.9739\n",
      "Epoch [7/10] Batch 457/510               Loss D: 0.0212, loss G: 4.0221\n",
      "Epoch [7/10] Batch 458/510               Loss D: 0.0204, loss G: 4.0354\n",
      "Epoch [7/10] Batch 459/510               Loss D: 0.0208, loss G: 4.0411\n",
      "Epoch [7/10] Batch 460/510               Loss D: 0.0193, loss G: 4.0927\n",
      "Epoch [7/10] Batch 461/510               Loss D: 0.0225, loss G: 3.9414\n",
      "Epoch [7/10] Batch 462/510               Loss D: 0.0209, loss G: 4.0100\n",
      "Epoch [7/10] Batch 463/510               Loss D: 0.1585, loss G: 3.8604\n",
      "Epoch [7/10] Batch 464/510               Loss D: 0.0220, loss G: 3.8045\n",
      "Epoch [7/10] Batch 465/510               Loss D: 0.0274, loss G: 3.6669\n",
      "Epoch [7/10] Batch 466/510               Loss D: 0.0258, loss G: 3.6390\n",
      "Epoch [7/10] Batch 467/510               Loss D: 0.0268, loss G: 3.6320\n",
      "Epoch [7/10] Batch 468/510               Loss D: 0.0326, loss G: 3.4542\n",
      "Epoch [7/10] Batch 469/510               Loss D: 0.0343, loss G: 3.4106\n",
      "Epoch [7/10] Batch 470/510               Loss D: 0.1660, loss G: 3.3084\n",
      "Epoch [7/10] Batch 471/510               Loss D: 0.0378, loss G: 3.2174\n",
      "Epoch [7/10] Batch 472/510               Loss D: 0.0405, loss G: 3.1625\n",
      "Epoch [7/10] Batch 473/510               Loss D: 0.0420, loss G: 3.1265\n",
      "Epoch [7/10] Batch 474/510               Loss D: 0.0464, loss G: 3.0535\n",
      "Epoch [7/10] Batch 475/510               Loss D: 0.0476, loss G: 3.0769\n",
      "Epoch [7/10] Batch 476/510               Loss D: 0.0475, loss G: 3.1303\n",
      "Epoch [7/10] Batch 477/510               Loss D: 0.0418, loss G: 3.1528\n",
      "Epoch [7/10] Batch 478/510               Loss D: 0.0412, loss G: 3.2981\n",
      "Epoch [7/10] Batch 479/510               Loss D: 0.0352, loss G: 3.4161\n",
      "Epoch [7/10] Batch 480/510               Loss D: 0.0362, loss G: 3.4278\n",
      "Epoch [7/10] Batch 481/510               Loss D: 0.0312, loss G: 3.5357\n",
      "Epoch [7/10] Batch 482/510               Loss D: 0.0331, loss G: 3.5446\n",
      "Epoch [7/10] Batch 483/510               Loss D: 0.1427, loss G: 3.6030\n",
      "Epoch [7/10] Batch 484/510               Loss D: 0.0274, loss G: 3.6603\n",
      "Epoch [7/10] Batch 485/510               Loss D: 0.0260, loss G: 3.6145\n",
      "Epoch [7/10] Batch 486/510               Loss D: 0.0270, loss G: 3.6679\n",
      "Epoch [7/10] Batch 487/510               Loss D: 0.1304, loss G: 3.6107\n",
      "Epoch [7/10] Batch 488/510               Loss D: 0.4036, loss G: 3.3095\n",
      "Epoch [7/10] Batch 489/510               Loss D: 0.0350, loss G: 3.2057\n",
      "Epoch [7/10] Batch 490/510               Loss D: 0.0416, loss G: 3.0240\n",
      "Epoch [7/10] Batch 491/510               Loss D: 0.0493, loss G: 2.8861\n",
      "Epoch [7/10] Batch 492/510               Loss D: 0.0549, loss G: 2.8143\n",
      "Epoch [7/10] Batch 493/510               Loss D: 0.0551, loss G: 2.8426\n",
      "Epoch [7/10] Batch 494/510               Loss D: 0.0581, loss G: 2.9498\n",
      "Epoch [7/10] Batch 495/510               Loss D: 0.0590, loss G: 2.8635\n",
      "Epoch [7/10] Batch 496/510               Loss D: 0.0542, loss G: 3.0219\n",
      "Epoch [7/10] Batch 497/510               Loss D: 0.0519, loss G: 3.0905\n",
      "Epoch [7/10] Batch 498/510               Loss D: 0.0449, loss G: 3.2556\n",
      "Epoch [7/10] Batch 499/510               Loss D: 0.0818, loss G: 3.2468\n",
      "Epoch [7/10] Batch 500/510               Loss D: 0.0395, loss G: 3.3683\n",
      "Epoch [7/10] Batch 501/510               Loss D: 0.0351, loss G: 3.5214\n",
      "Epoch [7/10] Batch 502/510               Loss D: 0.0327, loss G: 3.5919\n",
      "Epoch [7/10] Batch 503/510               Loss D: 0.0305, loss G: 3.7541\n",
      "Epoch [7/10] Batch 504/510               Loss D: 0.0276, loss G: 3.8314\n",
      "Epoch [7/10] Batch 505/510               Loss D: 0.0259, loss G: 3.8782\n",
      "Epoch [7/10] Batch 506/510               Loss D: 0.0228, loss G: 4.0579\n",
      "Epoch [7/10] Batch 507/510               Loss D: 0.0209, loss G: 4.1376\n",
      "Epoch [7/10] Batch 508/510               Loss D: 0.4705, loss G: 3.8883\n",
      "Epoch [7/10] Batch 509/510               Loss D: 0.0271, loss G: 3.7664\n",
      "Epoch [8/10] Batch 0/510               Loss D: 0.0317, loss G: 3.5516\n",
      "Epoch [8/10] Batch 1/510               Loss D: 0.5718, loss G: 3.2235\n",
      "Epoch [8/10] Batch 2/510               Loss D: 0.0476, loss G: 3.0251\n",
      "Epoch [8/10] Batch 3/510               Loss D: 0.0666, loss G: 2.7638\n",
      "Epoch [8/10] Batch 4/510               Loss D: 0.0896, loss G: 2.6875\n",
      "Epoch [8/10] Batch 5/510               Loss D: 0.1006, loss G: 2.6343\n",
      "Epoch [8/10] Batch 6/510               Loss D: 0.1147, loss G: 2.6465\n",
      "Epoch [8/10] Batch 7/510               Loss D: 0.0981, loss G: 2.9248\n",
      "Epoch [8/10] Batch 8/510               Loss D: 0.0989, loss G: 2.9444\n",
      "Epoch [8/10] Batch 9/510               Loss D: 0.1013, loss G: 2.9797\n",
      "Epoch [8/10] Batch 10/510               Loss D: 0.0836, loss G: 3.3056\n",
      "Epoch [8/10] Batch 11/510               Loss D: 0.0748, loss G: 3.4369\n",
      "Epoch [8/10] Batch 12/510               Loss D: 0.0590, loss G: 3.6300\n",
      "Epoch [8/10] Batch 13/510               Loss D: 0.0488, loss G: 3.9090\n",
      "Epoch [8/10] Batch 14/510               Loss D: 0.0401, loss G: 4.1489\n",
      "Epoch [8/10] Batch 15/510               Loss D: 0.0361, loss G: 4.2913\n",
      "Epoch [8/10] Batch 16/510               Loss D: 0.0268, loss G: 4.4758\n",
      "Epoch [8/10] Batch 17/510               Loss D: 0.0249, loss G: 4.7288\n",
      "Epoch [8/10] Batch 18/510               Loss D: 0.0212, loss G: 4.7722\n",
      "Epoch [8/10] Batch 19/510               Loss D: 0.0219, loss G: 4.8832\n",
      "Epoch [8/10] Batch 20/510               Loss D: 0.0169, loss G: 5.1722\n",
      "Epoch [8/10] Batch 21/510               Loss D: 0.0208, loss G: 5.0211\n",
      "Epoch [8/10] Batch 22/510               Loss D: 0.9480, loss G: 4.8356\n",
      "Epoch [8/10] Batch 23/510               Loss D: 0.0161, loss G: 4.8827\n",
      "Epoch [8/10] Batch 24/510               Loss D: 0.0230, loss G: 4.6099\n",
      "Epoch [8/10] Batch 25/510               Loss D: 0.0289, loss G: 4.3879\n",
      "Epoch [8/10] Batch 26/510               Loss D: 0.0330, loss G: 4.1518\n",
      "Epoch [8/10] Batch 27/510               Loss D: 0.0370, loss G: 4.1833\n",
      "Epoch [8/10] Batch 28/510               Loss D: 0.0397, loss G: 4.0177\n",
      "Epoch [8/10] Batch 29/510               Loss D: 0.0430, loss G: 3.9211\n",
      "Epoch [8/10] Batch 30/510               Loss D: 0.0435, loss G: 3.8320\n",
      "Epoch [8/10] Batch 31/510               Loss D: 0.0480, loss G: 3.8677\n",
      "Epoch [8/10] Batch 32/510               Loss D: 0.0550, loss G: 3.6656\n",
      "Epoch [8/10] Batch 33/510               Loss D: 0.0512, loss G: 3.7163\n",
      "Epoch [8/10] Batch 34/510               Loss D: 0.0484, loss G: 3.7825\n",
      "Epoch [8/10] Batch 35/510               Loss D: 0.0479, loss G: 3.7774\n",
      "Epoch [8/10] Batch 36/510               Loss D: 0.0472, loss G: 3.8596\n",
      "Epoch [8/10] Batch 37/510               Loss D: 0.0422, loss G: 3.8593\n",
      "Epoch [8/10] Batch 38/510               Loss D: 0.0424, loss G: 3.8391\n",
      "Epoch [8/10] Batch 39/510               Loss D: 0.8600, loss G: 3.7676\n",
      "Epoch [8/10] Batch 40/510               Loss D: 0.0439, loss G: 3.6118\n",
      "Epoch [8/10] Batch 41/510               Loss D: 0.0436, loss G: 3.5959\n",
      "Epoch [8/10] Batch 42/510               Loss D: 0.0477, loss G: 3.4505\n",
      "Epoch [8/10] Batch 43/510               Loss D: 0.0479, loss G: 3.4482\n",
      "Epoch [8/10] Batch 44/510               Loss D: 0.0492, loss G: 3.4339\n",
      "Epoch [8/10] Batch 45/510               Loss D: 0.0472, loss G: 3.4884\n",
      "Epoch [8/10] Batch 46/510               Loss D: 0.0437, loss G: 3.5155\n",
      "Epoch [8/10] Batch 47/510               Loss D: 0.0455, loss G: 3.5096\n",
      "Epoch [8/10] Batch 48/510               Loss D: 0.0383, loss G: 3.6649\n",
      "Epoch [8/10] Batch 49/510               Loss D: 0.0384, loss G: 3.6395\n",
      "Epoch [8/10] Batch 50/510               Loss D: 0.0346, loss G: 3.6755\n",
      "Epoch [8/10] Batch 51/510               Loss D: 0.0392, loss G: 3.6971\n",
      "Epoch [8/10] Batch 52/510               Loss D: 0.0328, loss G: 3.7225\n",
      "Epoch [8/10] Batch 53/510               Loss D: 0.0319, loss G: 3.7822\n",
      "Epoch [8/10] Batch 54/510               Loss D: 0.0280, loss G: 3.8460\n",
      "Epoch [8/10] Batch 55/510               Loss D: 0.0295, loss G: 3.8112\n",
      "Epoch [8/10] Batch 56/510               Loss D: 0.0256, loss G: 3.9063\n",
      "Epoch [8/10] Batch 57/510               Loss D: 0.0261, loss G: 3.8943\n",
      "Epoch [8/10] Batch 58/510               Loss D: 0.0249, loss G: 3.8692\n",
      "Epoch [8/10] Batch 59/510               Loss D: 0.0250, loss G: 3.8582\n",
      "Epoch [8/10] Batch 60/510               Loss D: 0.0251, loss G: 3.8366\n",
      "Epoch [8/10] Batch 61/510               Loss D: 0.0261, loss G: 3.8274\n",
      "Epoch [8/10] Batch 62/510               Loss D: 0.0229, loss G: 3.9036\n",
      "Epoch [8/10] Batch 63/510               Loss D: 0.1857, loss G: 3.8252\n",
      "Epoch [8/10] Batch 64/510               Loss D: 0.0305, loss G: 3.7336\n",
      "Epoch [8/10] Batch 65/510               Loss D: 0.0258, loss G: 3.6458\n",
      "Epoch [8/10] Batch 66/510               Loss D: 0.0275, loss G: 3.5887\n",
      "Epoch [8/10] Batch 67/510               Loss D: 0.0281, loss G: 3.5545\n",
      "Epoch [8/10] Batch 68/510               Loss D: 0.0296, loss G: 3.4971\n",
      "Epoch [8/10] Batch 69/510               Loss D: 0.0303, loss G: 3.4537\n",
      "Epoch [8/10] Batch 70/510               Loss D: 0.0346, loss G: 3.3574\n",
      "Epoch [8/10] Batch 71/510               Loss D: 0.0331, loss G: 3.3369\n",
      "Epoch [8/10] Batch 72/510               Loss D: 0.0352, loss G: 3.2867\n",
      "Epoch [8/10] Batch 73/510               Loss D: 0.0333, loss G: 3.3259\n",
      "Epoch [8/10] Batch 74/510               Loss D: 0.0341, loss G: 3.3408\n",
      "Epoch [8/10] Batch 75/510               Loss D: 0.0313, loss G: 3.3913\n",
      "Epoch [8/10] Batch 76/510               Loss D: 0.0315, loss G: 3.3751\n",
      "Epoch [8/10] Batch 77/510               Loss D: 0.2647, loss G: 3.2961\n",
      "Epoch [8/10] Batch 78/510               Loss D: 0.0324, loss G: 3.2028\n",
      "Epoch [8/10] Batch 79/510               Loss D: 0.0366, loss G: 3.0899\n",
      "Epoch [8/10] Batch 80/510               Loss D: 0.0415, loss G: 2.9960\n",
      "Epoch [8/10] Batch 81/510               Loss D: 0.0415, loss G: 3.0026\n",
      "Epoch [8/10] Batch 82/510               Loss D: 0.0431, loss G: 2.9728\n",
      "Epoch [8/10] Batch 83/510               Loss D: 0.0427, loss G: 3.0029\n",
      "Epoch [8/10] Batch 84/510               Loss D: 0.0446, loss G: 3.0382\n",
      "Epoch [8/10] Batch 85/510               Loss D: 0.0419, loss G: 3.0426\n",
      "Epoch [8/10] Batch 86/510               Loss D: 0.0388, loss G: 3.1569\n",
      "Epoch [8/10] Batch 87/510               Loss D: 0.0356, loss G: 3.2501\n",
      "Epoch [8/10] Batch 88/510               Loss D: 0.0326, loss G: 3.3427\n",
      "Epoch [8/10] Batch 89/510               Loss D: 0.0330, loss G: 3.3548\n",
      "Epoch [8/10] Batch 90/510               Loss D: 0.0289, loss G: 3.4710\n",
      "Epoch [8/10] Batch 91/510               Loss D: 0.0284, loss G: 3.5165\n",
      "Epoch [8/10] Batch 92/510               Loss D: 0.0254, loss G: 3.6024\n",
      "Epoch [8/10] Batch 93/510               Loss D: 0.0239, loss G: 3.6875\n",
      "Epoch [8/10] Batch 94/510               Loss D: 0.0252, loss G: 3.7573\n",
      "Epoch [8/10] Batch 95/510               Loss D: 0.0436, loss G: 3.7631\n",
      "Epoch [8/10] Batch 96/510               Loss D: 0.0217, loss G: 3.7727\n",
      "Epoch [8/10] Batch 97/510               Loss D: 0.0209, loss G: 3.7882\n",
      "Epoch [8/10] Batch 98/510               Loss D: 0.0205, loss G: 3.8333\n",
      "Epoch [8/10] Batch 99/510               Loss D: 0.0213, loss G: 3.7887\n",
      "Epoch [8/10] Batch 100/510               Loss D: 0.0222, loss G: 3.7597\n",
      "Epoch [8/10] Batch 101/510               Loss D: 1.2878, loss G: 3.4626\n",
      "Epoch [8/10] Batch 102/510               Loss D: 0.0327, loss G: 3.0558\n",
      "Epoch [8/10] Batch 103/510               Loss D: 0.0429, loss G: 2.8081\n",
      "Epoch [8/10] Batch 104/510               Loss D: 0.0564, loss G: 2.5947\n",
      "Epoch [8/10] Batch 105/510               Loss D: 0.0695, loss G: 2.4732\n",
      "Epoch [8/10] Batch 106/510               Loss D: 0.0817, loss G: 2.4079\n",
      "Epoch [8/10] Batch 107/510               Loss D: 0.0835, loss G: 2.4440\n",
      "Epoch [8/10] Batch 108/510               Loss D: 0.0879, loss G: 2.4645\n",
      "Epoch [8/10] Batch 109/510               Loss D: 0.0845, loss G: 2.5582\n",
      "Epoch [8/10] Batch 110/510               Loss D: 0.0767, loss G: 2.7358\n",
      "Epoch [8/10] Batch 111/510               Loss D: 0.0686, loss G: 2.8762\n",
      "Epoch [8/10] Batch 112/510               Loss D: 0.0590, loss G: 3.0410\n",
      "Epoch [8/10] Batch 113/510               Loss D: 0.1134, loss G: 3.2425\n",
      "Epoch [8/10] Batch 114/510               Loss D: 0.0436, loss G: 3.4057\n",
      "Epoch [8/10] Batch 115/510               Loss D: 0.0333, loss G: 3.6656\n",
      "Epoch [8/10] Batch 116/510               Loss D: 0.3014, loss G: 3.6107\n",
      "Epoch [8/10] Batch 117/510               Loss D: 0.0337, loss G: 3.5636\n",
      "Epoch [8/10] Batch 118/510               Loss D: 0.0318, loss G: 3.6691\n",
      "Epoch [8/10] Batch 119/510               Loss D: 0.0330, loss G: 3.7067\n",
      "Epoch [8/10] Batch 120/510               Loss D: 0.0269, loss G: 3.8024\n",
      "Epoch [8/10] Batch 121/510               Loss D: 0.0299, loss G: 3.8218\n",
      "Epoch [8/10] Batch 122/510               Loss D: 0.0349, loss G: 3.8021\n",
      "Epoch [8/10] Batch 123/510               Loss D: 0.0275, loss G: 3.8470\n",
      "Epoch [8/10] Batch 124/510               Loss D: 0.0299, loss G: 3.8434\n",
      "Epoch [8/10] Batch 125/510               Loss D: 0.0277, loss G: 3.8736\n",
      "Epoch [8/10] Batch 126/510               Loss D: 0.0295, loss G: 3.8452\n",
      "Epoch [8/10] Batch 127/510               Loss D: 0.0305, loss G: 3.8761\n",
      "Epoch [8/10] Batch 128/510               Loss D: 0.0307, loss G: 3.8278\n",
      "Epoch [8/10] Batch 129/510               Loss D: 0.0312, loss G: 3.8249\n",
      "Epoch [8/10] Batch 130/510               Loss D: 0.0315, loss G: 3.8822\n",
      "Epoch [8/10] Batch 131/510               Loss D: 0.0370, loss G: 3.7744\n",
      "Epoch [8/10] Batch 132/510               Loss D: 0.0314, loss G: 3.7621\n",
      "Epoch [8/10] Batch 133/510               Loss D: 0.0326, loss G: 3.7400\n",
      "Epoch [8/10] Batch 134/510               Loss D: 0.0324, loss G: 3.7907\n",
      "Epoch [8/10] Batch 135/510               Loss D: 0.0321, loss G: 3.7967\n",
      "Epoch [8/10] Batch 136/510               Loss D: 0.0305, loss G: 3.8056\n",
      "Epoch [8/10] Batch 137/510               Loss D: 0.0322, loss G: 3.8214\n",
      "Epoch [8/10] Batch 138/510               Loss D: 0.0322, loss G: 3.8064\n",
      "Epoch [8/10] Batch 139/510               Loss D: 0.0398, loss G: 3.8407\n",
      "Epoch [8/10] Batch 140/510               Loss D: 0.0337, loss G: 3.8735\n",
      "Epoch [8/10] Batch 141/510               Loss D: 0.0294, loss G: 3.8045\n",
      "Epoch [8/10] Batch 142/510               Loss D: 0.0303, loss G: 3.8441\n",
      "Epoch [8/10] Batch 143/510               Loss D: 0.0497, loss G: 3.8323\n",
      "Epoch [8/10] Batch 144/510               Loss D: 0.0385, loss G: 3.8442\n",
      "Epoch [8/10] Batch 145/510               Loss D: 0.0308, loss G: 3.8421\n",
      "Epoch [8/10] Batch 146/510               Loss D: 0.0425, loss G: 3.9035\n",
      "Epoch [8/10] Batch 147/510               Loss D: 0.2861, loss G: 3.6935\n",
      "Epoch [8/10] Batch 148/510               Loss D: 0.0363, loss G: 3.4684\n",
      "Epoch [8/10] Batch 149/510               Loss D: 0.0400, loss G: 3.3829\n",
      "Epoch [8/10] Batch 150/510               Loss D: 0.0447, loss G: 3.3249\n",
      "Epoch [8/10] Batch 151/510               Loss D: 0.0459, loss G: 3.2794\n",
      "Epoch [8/10] Batch 152/510               Loss D: 0.0494, loss G: 3.2497\n",
      "Epoch [8/10] Batch 153/510               Loss D: 0.0501, loss G: 3.2472\n",
      "Epoch [8/10] Batch 154/510               Loss D: 0.0595, loss G: 3.2666\n",
      "Epoch [8/10] Batch 155/510               Loss D: 0.0441, loss G: 3.3921\n",
      "Epoch [8/10] Batch 156/510               Loss D: 0.4969, loss G: 3.1798\n",
      "Epoch [8/10] Batch 157/510               Loss D: 0.0501, loss G: 3.0339\n",
      "Epoch [8/10] Batch 158/510               Loss D: 0.0554, loss G: 2.9549\n",
      "Epoch [8/10] Batch 159/510               Loss D: 0.1063, loss G: 2.9446\n",
      "Epoch [8/10] Batch 160/510               Loss D: 0.0642, loss G: 2.9020\n",
      "Epoch [8/10] Batch 161/510               Loss D: 0.0696, loss G: 2.8471\n",
      "Epoch [8/10] Batch 162/510               Loss D: 0.4198, loss G: 2.7803\n",
      "Epoch [8/10] Batch 163/510               Loss D: 0.0754, loss G: 2.6687\n",
      "Epoch [8/10] Batch 164/510               Loss D: 0.0702, loss G: 2.7528\n",
      "Epoch [8/10] Batch 165/510               Loss D: 0.0680, loss G: 2.8373\n",
      "Epoch [8/10] Batch 166/510               Loss D: 0.0700, loss G: 2.8690\n",
      "Epoch [8/10] Batch 167/510               Loss D: 0.0607, loss G: 3.0728\n",
      "Epoch [8/10] Batch 168/510               Loss D: 0.8018, loss G: 3.1255\n",
      "Epoch [8/10] Batch 169/510               Loss D: 0.0517, loss G: 3.1161\n",
      "Epoch [8/10] Batch 170/510               Loss D: 0.0478, loss G: 3.2285\n",
      "Epoch [8/10] Batch 171/510               Loss D: 0.0460, loss G: 3.3869\n",
      "Epoch [8/10] Batch 172/510               Loss D: 0.1716, loss G: 3.3944\n",
      "Epoch [8/10] Batch 173/510               Loss D: 0.0403, loss G: 3.4551\n",
      "Epoch [8/10] Batch 174/510               Loss D: 0.0390, loss G: 3.4850\n",
      "Epoch [8/10] Batch 175/510               Loss D: 0.0393, loss G: 3.4928\n",
      "Epoch [8/10] Batch 176/510               Loss D: 0.0355, loss G: 3.6225\n",
      "Epoch [8/10] Batch 177/510               Loss D: 0.0343, loss G: 3.6738\n",
      "Epoch [8/10] Batch 178/510               Loss D: 0.0298, loss G: 3.8410\n",
      "Epoch [8/10] Batch 179/510               Loss D: 0.0300, loss G: 3.8759\n",
      "Epoch [8/10] Batch 180/510               Loss D: 0.0313, loss G: 3.8024\n",
      "Epoch [8/10] Batch 181/510               Loss D: 0.0304, loss G: 3.9412\n",
      "Epoch [8/10] Batch 182/510               Loss D: 0.0289, loss G: 3.9194\n",
      "Epoch [8/10] Batch 183/510               Loss D: 0.0289, loss G: 3.9785\n",
      "Epoch [8/10] Batch 184/510               Loss D: 0.0313, loss G: 3.8972\n",
      "Epoch [8/10] Batch 185/510               Loss D: 0.0299, loss G: 3.9720\n",
      "Epoch [8/10] Batch 186/510               Loss D: 0.0281, loss G: 3.9803\n",
      "Epoch [8/10] Batch 187/510               Loss D: 0.0316, loss G: 3.9505\n",
      "Epoch [8/10] Batch 188/510               Loss D: 0.0365, loss G: 3.9669\n",
      "Epoch [8/10] Batch 189/510               Loss D: 0.0309, loss G: 3.9900\n",
      "Epoch [8/10] Batch 190/510               Loss D: 0.0348, loss G: 3.8521\n",
      "Epoch [8/10] Batch 191/510               Loss D: 0.0308, loss G: 3.9902\n",
      "Epoch [8/10] Batch 192/510               Loss D: 0.0315, loss G: 3.9238\n",
      "Epoch [8/10] Batch 193/510               Loss D: 0.0356, loss G: 3.9302\n",
      "Epoch [8/10] Batch 194/510               Loss D: 0.0316, loss G: 3.9629\n",
      "Epoch [8/10] Batch 195/510               Loss D: 0.0370, loss G: 3.9150\n",
      "Epoch [8/10] Batch 196/510               Loss D: 0.0360, loss G: 3.8937\n",
      "Epoch [8/10] Batch 197/510               Loss D: 0.0358, loss G: 3.9024\n",
      "Epoch [8/10] Batch 198/510               Loss D: 0.0343, loss G: 3.9484\n",
      "Epoch [8/10] Batch 199/510               Loss D: 0.0310, loss G: 3.9646\n",
      "Epoch [8/10] Batch 200/510               Loss D: 0.0363, loss G: 3.9357\n",
      "Epoch [8/10] Batch 201/510               Loss D: 0.2420, loss G: 3.7599\n",
      "Epoch [8/10] Batch 202/510               Loss D: 0.0383, loss G: 3.7460\n",
      "Epoch [8/10] Batch 203/510               Loss D: 0.0429, loss G: 3.6709\n",
      "Epoch [8/10] Batch 204/510               Loss D: 0.0474, loss G: 3.5616\n",
      "Epoch [8/10] Batch 205/510               Loss D: 0.0473, loss G: 3.5299\n",
      "Epoch [8/10] Batch 206/510               Loss D: 0.0468, loss G: 3.5979\n",
      "Epoch [8/10] Batch 207/510               Loss D: 0.0531, loss G: 3.5652\n",
      "Epoch [8/10] Batch 208/510               Loss D: 0.0481, loss G: 3.6884\n",
      "Epoch [8/10] Batch 209/510               Loss D: 0.0444, loss G: 3.7245\n",
      "Epoch [8/10] Batch 210/510               Loss D: 0.0414, loss G: 3.8477\n",
      "Epoch [8/10] Batch 211/510               Loss D: 0.0474, loss G: 3.7711\n",
      "Epoch [8/10] Batch 212/510               Loss D: 0.0449, loss G: 3.8821\n",
      "Epoch [8/10] Batch 213/510               Loss D: 0.0394, loss G: 3.9957\n",
      "Epoch [8/10] Batch 214/510               Loss D: 0.0364, loss G: 4.1300\n",
      "Epoch [8/10] Batch 215/510               Loss D: 0.0335, loss G: 4.1599\n",
      "Epoch [8/10] Batch 216/510               Loss D: 0.0320, loss G: 4.1884\n",
      "Epoch [8/10] Batch 217/510               Loss D: 0.0344, loss G: 4.1608\n",
      "Epoch [8/10] Batch 218/510               Loss D: 0.0431, loss G: 4.3803\n",
      "Epoch [8/10] Batch 219/510               Loss D: 0.0556, loss G: 4.2073\n",
      "Epoch [8/10] Batch 220/510               Loss D: 0.0290, loss G: 4.3620\n",
      "Epoch [8/10] Batch 221/510               Loss D: 0.0301, loss G: 4.2889\n",
      "Epoch [8/10] Batch 222/510               Loss D: 2.9990, loss G: 3.9810\n",
      "Epoch [8/10] Batch 223/510               Loss D: 1.0382, loss G: 3.2675\n",
      "Epoch [8/10] Batch 224/510               Loss D: 0.0634, loss G: 2.7588\n",
      "Epoch [8/10] Batch 225/510               Loss D: 0.0940, loss G: 2.3968\n",
      "Epoch [8/10] Batch 226/510               Loss D: 0.1155, loss G: 2.2011\n",
      "Epoch [8/10] Batch 227/510               Loss D: 0.1431, loss G: 2.1085\n",
      "Epoch [8/10] Batch 228/510               Loss D: 0.1527, loss G: 2.1709\n",
      "Epoch [8/10] Batch 229/510               Loss D: 0.1425, loss G: 2.3462\n",
      "Epoch [8/10] Batch 230/510               Loss D: 0.1181, loss G: 2.6146\n",
      "Epoch [8/10] Batch 231/510               Loss D: 0.0979, loss G: 2.9122\n",
      "Epoch [8/10] Batch 232/510               Loss D: 0.0720, loss G: 3.2137\n",
      "Epoch [8/10] Batch 233/510               Loss D: 0.0574, loss G: 3.5559\n",
      "Epoch [8/10] Batch 234/510               Loss D: 0.0426, loss G: 3.9215\n",
      "Epoch [8/10] Batch 235/510               Loss D: 0.0314, loss G: 4.2424\n",
      "Epoch [8/10] Batch 236/510               Loss D: 0.0235, loss G: 4.5482\n",
      "Epoch [8/10] Batch 237/510               Loss D: 0.0210, loss G: 4.7317\n",
      "Epoch [8/10] Batch 238/510               Loss D: 0.0255, loss G: 4.9274\n",
      "Epoch [8/10] Batch 239/510               Loss D: 0.0131, loss G: 5.1094\n",
      "Epoch [8/10] Batch 240/510               Loss D: 0.0143, loss G: 5.1397\n",
      "Epoch [8/10] Batch 241/510               Loss D: 0.0126, loss G: 5.0746\n",
      "Epoch [8/10] Batch 242/510               Loss D: 0.0127, loss G: 5.1840\n",
      "Epoch [8/10] Batch 243/510               Loss D: 0.0142, loss G: 5.2251\n",
      "Epoch [8/10] Batch 244/510               Loss D: 0.0176, loss G: 5.0601\n",
      "Epoch [8/10] Batch 245/510               Loss D: 0.0172, loss G: 5.0458\n",
      "Epoch [8/10] Batch 246/510               Loss D: 0.0615, loss G: 4.8720\n",
      "Epoch [8/10] Batch 247/510               Loss D: 0.0175, loss G: 4.7991\n",
      "Epoch [8/10] Batch 248/510               Loss D: 0.1666, loss G: 4.5475\n",
      "Epoch [8/10] Batch 249/510               Loss D: 0.0209, loss G: 4.5072\n",
      "Epoch [8/10] Batch 250/510               Loss D: 0.0252, loss G: 4.1926\n",
      "Epoch [8/10] Batch 251/510               Loss D: 0.0268, loss G: 4.0762\n",
      "Epoch [8/10] Batch 252/510               Loss D: 0.0406, loss G: 3.9789\n",
      "Epoch [8/10] Batch 253/510               Loss D: 0.0358, loss G: 3.8287\n",
      "Epoch [8/10] Batch 254/510               Loss D: 0.0388, loss G: 3.7296\n",
      "Epoch [8/10] Batch 255/510               Loss D: 0.0412, loss G: 3.6175\n",
      "Epoch [8/10] Batch 256/510               Loss D: 0.0461, loss G: 3.5799\n",
      "Epoch [8/10] Batch 257/510               Loss D: 0.0461, loss G: 3.5730\n",
      "Epoch [8/10] Batch 258/510               Loss D: 0.0478, loss G: 3.4984\n",
      "Epoch [8/10] Batch 259/510               Loss D: 0.0444, loss G: 3.5547\n",
      "Epoch [8/10] Batch 260/510               Loss D: 0.0397, loss G: 3.6746\n",
      "Epoch [8/10] Batch 261/510               Loss D: 0.0386, loss G: 3.7325\n",
      "Epoch [8/10] Batch 262/510               Loss D: 0.0349, loss G: 3.8373\n",
      "Epoch [8/10] Batch 263/510               Loss D: 0.0348, loss G: 3.8878\n",
      "Epoch [8/10] Batch 264/510               Loss D: 0.0303, loss G: 3.9419\n",
      "Epoch [8/10] Batch 265/510               Loss D: 0.0334, loss G: 3.8945\n",
      "Epoch [8/10] Batch 266/510               Loss D: 0.0279, loss G: 4.1089\n",
      "Epoch [8/10] Batch 267/510               Loss D: 0.0263, loss G: 4.1825\n",
      "Epoch [8/10] Batch 268/510               Loss D: 0.0231, loss G: 4.2271\n",
      "Epoch [8/10] Batch 269/510               Loss D: 0.0244, loss G: 4.1775\n",
      "Epoch [8/10] Batch 270/510               Loss D: 0.0319, loss G: 4.2270\n",
      "Epoch [8/10] Batch 271/510               Loss D: 0.0202, loss G: 4.4162\n",
      "Epoch [8/10] Batch 272/510               Loss D: 0.0201, loss G: 4.4085\n",
      "Epoch [8/10] Batch 273/510               Loss D: 0.0260, loss G: 4.3243\n",
      "Epoch [8/10] Batch 274/510               Loss D: 0.0238, loss G: 4.3790\n",
      "Epoch [8/10] Batch 275/510               Loss D: 0.0194, loss G: 4.4406\n",
      "Epoch [8/10] Batch 276/510               Loss D: 0.0243, loss G: 4.3933\n",
      "Epoch [8/10] Batch 277/510               Loss D: 0.0836, loss G: 4.4268\n",
      "Epoch [8/10] Batch 278/510               Loss D: 0.0223, loss G: 4.2855\n",
      "Epoch [8/10] Batch 279/510               Loss D: 0.0213, loss G: 4.2247\n",
      "Epoch [8/10] Batch 280/510               Loss D: 0.0226, loss G: 4.2182\n",
      "Epoch [8/10] Batch 281/510               Loss D: 0.0510, loss G: 4.1740\n",
      "Epoch [8/10] Batch 282/510               Loss D: 0.0235, loss G: 4.2192\n",
      "Epoch [8/10] Batch 283/510               Loss D: 0.0342, loss G: 4.1572\n",
      "Epoch [8/10] Batch 284/510               Loss D: 0.0250, loss G: 4.1321\n",
      "Epoch [8/10] Batch 285/510               Loss D: 0.0266, loss G: 4.0741\n",
      "Epoch [8/10] Batch 286/510               Loss D: 0.0269, loss G: 4.1415\n",
      "Epoch [8/10] Batch 287/510               Loss D: 0.0889, loss G: 3.9818\n",
      "Epoch [8/10] Batch 288/510               Loss D: 0.0248, loss G: 4.0913\n",
      "Epoch [8/10] Batch 289/510               Loss D: 0.0261, loss G: 4.0203\n",
      "Epoch [8/10] Batch 290/510               Loss D: 0.0272, loss G: 3.9022\n",
      "Epoch [8/10] Batch 291/510               Loss D: 0.0334, loss G: 3.9134\n",
      "Epoch [8/10] Batch 292/510               Loss D: 0.0267, loss G: 3.9557\n",
      "Epoch [8/10] Batch 293/510               Loss D: 0.0855, loss G: 3.8958\n",
      "Epoch [8/10] Batch 294/510               Loss D: 0.0308, loss G: 3.9597\n",
      "Epoch [8/10] Batch 295/510               Loss D: 0.0276, loss G: 3.9259\n",
      "Epoch [8/10] Batch 296/510               Loss D: 0.0239, loss G: 3.9983\n",
      "Epoch [8/10] Batch 297/510               Loss D: 0.0269, loss G: 3.8729\n",
      "Epoch [8/10] Batch 298/510               Loss D: 0.0278, loss G: 3.8838\n",
      "Epoch [8/10] Batch 299/510               Loss D: 0.3002, loss G: 3.9090\n",
      "Epoch [8/10] Batch 300/510               Loss D: 0.0251, loss G: 3.8448\n",
      "Epoch [8/10] Batch 301/510               Loss D: 0.0289, loss G: 3.7270\n",
      "Epoch [8/10] Batch 302/510               Loss D: 0.0632, loss G: 3.7177\n",
      "Epoch [8/10] Batch 303/510               Loss D: 0.0904, loss G: 3.7018\n",
      "Epoch [8/10] Batch 304/510               Loss D: 0.1504, loss G: 3.6202\n",
      "Epoch [8/10] Batch 305/510               Loss D: 0.0318, loss G: 3.5347\n",
      "Epoch [8/10] Batch 306/510               Loss D: 0.0383, loss G: 3.4761\n",
      "Epoch [8/10] Batch 307/510               Loss D: 0.0348, loss G: 3.4440\n",
      "Epoch [8/10] Batch 308/510               Loss D: 0.0356, loss G: 3.4556\n",
      "Epoch [8/10] Batch 309/510               Loss D: 0.0334, loss G: 3.4925\n",
      "Epoch [8/10] Batch 310/510               Loss D: 0.0343, loss G: 3.5395\n",
      "Epoch [8/10] Batch 311/510               Loss D: 0.0336, loss G: 3.5742\n",
      "Epoch [8/10] Batch 312/510               Loss D: 0.0322, loss G: 3.5946\n",
      "Epoch [8/10] Batch 313/510               Loss D: 0.0411, loss G: 3.6486\n",
      "Epoch [8/10] Batch 314/510               Loss D: 0.0291, loss G: 3.7393\n",
      "Epoch [8/10] Batch 315/510               Loss D: 0.0272, loss G: 3.7748\n",
      "Epoch [8/10] Batch 316/510               Loss D: 0.0259, loss G: 3.8568\n",
      "Epoch [8/10] Batch 317/510               Loss D: 0.0234, loss G: 3.9843\n",
      "Epoch [8/10] Batch 318/510               Loss D: 0.0225, loss G: 4.0091\n",
      "Epoch [8/10] Batch 319/510               Loss D: 0.0255, loss G: 4.1653\n",
      "Epoch [8/10] Batch 320/510               Loss D: 0.0182, loss G: 4.2537\n",
      "Epoch [8/10] Batch 321/510               Loss D: 0.0167, loss G: 4.2944\n",
      "Epoch [8/10] Batch 322/510               Loss D: 0.0159, loss G: 4.3596\n",
      "Epoch [8/10] Batch 323/510               Loss D: 0.0173, loss G: 4.3326\n",
      "Epoch [8/10] Batch 324/510               Loss D: 0.0158, loss G: 4.3976\n",
      "Epoch [8/10] Batch 325/510               Loss D: 0.1117, loss G: 4.3875\n",
      "Epoch [8/10] Batch 326/510               Loss D: 0.0139, loss G: 4.3990\n",
      "Epoch [8/10] Batch 327/510               Loss D: 0.0158, loss G: 4.3116\n",
      "Epoch [8/10] Batch 328/510               Loss D: 0.2539, loss G: 4.1947\n",
      "Epoch [8/10] Batch 329/510               Loss D: 0.0174, loss G: 4.0439\n",
      "Epoch [8/10] Batch 330/510               Loss D: 0.0225, loss G: 3.9311\n",
      "Epoch [8/10] Batch 331/510               Loss D: 0.0231, loss G: 3.7896\n",
      "Epoch [8/10] Batch 332/510               Loss D: 0.0244, loss G: 3.7565\n",
      "Epoch [8/10] Batch 333/510               Loss D: 0.0265, loss G: 3.6825\n",
      "Epoch [8/10] Batch 334/510               Loss D: 0.0279, loss G: 3.6758\n",
      "Epoch [8/10] Batch 335/510               Loss D: 0.0316, loss G: 3.5212\n",
      "Epoch [8/10] Batch 336/510               Loss D: 0.0312, loss G: 3.5474\n",
      "Epoch [8/10] Batch 337/510               Loss D: 0.0305, loss G: 3.5754\n",
      "Epoch [8/10] Batch 338/510               Loss D: 0.0295, loss G: 3.6180\n",
      "Epoch [8/10] Batch 339/510               Loss D: 0.0294, loss G: 3.6603\n",
      "Epoch [8/10] Batch 340/510               Loss D: 0.0253, loss G: 3.7874\n",
      "Epoch [8/10] Batch 341/510               Loss D: 0.0266, loss G: 3.7523\n",
      "Epoch [8/10] Batch 342/510               Loss D: 0.0257, loss G: 3.7902\n",
      "Epoch [8/10] Batch 343/510               Loss D: 0.0224, loss G: 3.9126\n",
      "Epoch [8/10] Batch 344/510               Loss D: 0.0210, loss G: 3.9899\n",
      "Epoch [8/10] Batch 345/510               Loss D: 0.0228, loss G: 3.9770\n",
      "Epoch [8/10] Batch 346/510               Loss D: 0.0186, loss G: 4.0968\n",
      "Epoch [8/10] Batch 347/510               Loss D: 0.0188, loss G: 4.0965\n",
      "Epoch [8/10] Batch 348/510               Loss D: 0.0170, loss G: 4.2230\n",
      "Epoch [8/10] Batch 349/510               Loss D: 0.0156, loss G: 4.2883\n",
      "Epoch [8/10] Batch 350/510               Loss D: 0.0216, loss G: 4.2425\n",
      "Epoch [8/10] Batch 351/510               Loss D: 0.0159, loss G: 4.2531\n",
      "Epoch [8/10] Batch 352/510               Loss D: 2.2343, loss G: 3.8325\n",
      "Epoch [8/10] Batch 353/510               Loss D: 0.0237, loss G: 3.5670\n",
      "Epoch [8/10] Batch 354/510               Loss D: 0.0285, loss G: 3.3515\n",
      "Epoch [8/10] Batch 355/510               Loss D: 0.0359, loss G: 3.1528\n",
      "Epoch [8/10] Batch 356/510               Loss D: 0.1473, loss G: 2.9398\n",
      "Epoch [8/10] Batch 357/510               Loss D: 0.0515, loss G: 2.8342\n",
      "Epoch [8/10] Batch 358/510               Loss D: 0.0599, loss G: 2.7839\n",
      "Epoch [8/10] Batch 359/510               Loss D: 0.0681, loss G: 2.7091\n",
      "Epoch [8/10] Batch 360/510               Loss D: 0.0662, loss G: 2.7974\n",
      "Epoch [8/10] Batch 361/510               Loss D: 0.0654, loss G: 2.9082\n",
      "Epoch [8/10] Batch 362/510               Loss D: 0.0618, loss G: 2.9503\n",
      "Epoch [8/10] Batch 363/510               Loss D: 0.0491, loss G: 3.2622\n",
      "Epoch [8/10] Batch 364/510               Loss D: 0.0441, loss G: 3.3672\n",
      "Epoch [8/10] Batch 365/510               Loss D: 0.0350, loss G: 3.5913\n",
      "Epoch [8/10] Batch 366/510               Loss D: 0.0311, loss G: 3.7703\n",
      "Epoch [8/10] Batch 367/510               Loss D: 0.0257, loss G: 3.9401\n",
      "Epoch [8/10] Batch 368/510               Loss D: 0.0218, loss G: 4.1411\n",
      "Epoch [8/10] Batch 369/510               Loss D: 0.0161, loss G: 4.3707\n",
      "Epoch [8/10] Batch 370/510               Loss D: 0.0148, loss G: 4.4118\n",
      "Epoch [8/10] Batch 371/510               Loss D: 0.0281, loss G: 4.6087\n",
      "Epoch [8/10] Batch 372/510               Loss D: 0.0125, loss G: 4.5872\n",
      "Epoch [8/10] Batch 373/510               Loss D: 0.0283, loss G: 4.5615\n",
      "Epoch [8/10] Batch 374/510               Loss D: 0.0125, loss G: 4.5683\n",
      "Epoch [8/10] Batch 375/510               Loss D: 0.0103, loss G: 4.7029\n",
      "Epoch [8/10] Batch 376/510               Loss D: 0.0105, loss G: 4.7012\n",
      "Epoch [8/10] Batch 377/510               Loss D: 0.0104, loss G: 4.6963\n",
      "Epoch [8/10] Batch 378/510               Loss D: 0.0265, loss G: 4.6648\n",
      "Epoch [8/10] Batch 379/510               Loss D: 0.0103, loss G: 4.5892\n",
      "Epoch [8/10] Batch 380/510               Loss D: 0.0107, loss G: 4.5443\n",
      "Epoch [8/10] Batch 381/510               Loss D: 0.0834, loss G: 4.4622\n",
      "Epoch [8/10] Batch 382/510               Loss D: 0.0129, loss G: 4.2630\n",
      "Epoch [8/10] Batch 383/510               Loss D: 0.0142, loss G: 4.1845\n",
      "Epoch [8/10] Batch 384/510               Loss D: 0.0163, loss G: 4.0595\n",
      "Epoch [8/10] Batch 385/510               Loss D: 0.0165, loss G: 4.0039\n",
      "Epoch [8/10] Batch 386/510               Loss D: 0.0183, loss G: 3.8904\n",
      "Epoch [8/10] Batch 387/510               Loss D: 0.0185, loss G: 3.9048\n",
      "Epoch [8/10] Batch 388/510               Loss D: 0.0203, loss G: 3.8392\n",
      "Epoch [8/10] Batch 389/510               Loss D: 0.0251, loss G: 3.6599\n",
      "Epoch [8/10] Batch 390/510               Loss D: 0.0220, loss G: 3.6804\n",
      "Epoch [8/10] Batch 391/510               Loss D: 0.0231, loss G: 3.6575\n",
      "Epoch [8/10] Batch 392/510               Loss D: 0.0382, loss G: 3.6147\n",
      "Epoch [8/10] Batch 393/510               Loss D: 0.0229, loss G: 3.6466\n",
      "Epoch [8/10] Batch 394/510               Loss D: 0.0216, loss G: 3.6848\n",
      "Epoch [8/10] Batch 395/510               Loss D: 0.0219, loss G: 3.6988\n",
      "Epoch [8/10] Batch 396/510               Loss D: 0.0215, loss G: 3.7012\n",
      "Epoch [8/10] Batch 397/510               Loss D: 0.0207, loss G: 3.7258\n",
      "Epoch [8/10] Batch 398/510               Loss D: 0.0214, loss G: 3.7017\n",
      "Epoch [8/10] Batch 399/510               Loss D: 0.0201, loss G: 3.7558\n",
      "Epoch [8/10] Batch 400/510               Loss D: 0.0186, loss G: 3.8277\n",
      "Epoch [8/10] Batch 401/510               Loss D: 0.0176, loss G: 3.8811\n",
      "Epoch [8/10] Batch 402/510               Loss D: 0.0165, loss G: 3.9200\n",
      "Epoch [8/10] Batch 403/510               Loss D: 0.0191, loss G: 3.9010\n",
      "Epoch [8/10] Batch 404/510               Loss D: 0.0210, loss G: 3.9455\n",
      "Epoch [8/10] Batch 405/510               Loss D: 0.0379, loss G: 3.9023\n",
      "Epoch [8/10] Batch 406/510               Loss D: 0.0151, loss G: 3.9853\n",
      "Epoch [8/10] Batch 407/510               Loss D: 0.0152, loss G: 3.9637\n",
      "Epoch [8/10] Batch 408/510               Loss D: 0.0148, loss G: 3.9827\n",
      "Epoch [8/10] Batch 409/510               Loss D: 0.0473, loss G: 3.9748\n",
      "Epoch [8/10] Batch 410/510               Loss D: 0.0331, loss G: 3.9167\n",
      "Epoch [8/10] Batch 411/510               Loss D: 0.1313, loss G: 3.8643\n",
      "Epoch [8/10] Batch 412/510               Loss D: 0.0164, loss G: 3.7779\n",
      "Epoch [8/10] Batch 413/510               Loss D: 0.0493, loss G: 3.7122\n",
      "Epoch [8/10] Batch 414/510               Loss D: 0.0319, loss G: 3.6682\n",
      "Epoch [8/10] Batch 415/510               Loss D: 0.0190, loss G: 3.6204\n",
      "Epoch [8/10] Batch 416/510               Loss D: 0.0232, loss G: 3.5428\n",
      "Epoch [8/10] Batch 417/510               Loss D: 0.0236, loss G: 3.5859\n",
      "Epoch [8/10] Batch 418/510               Loss D: 0.0204, loss G: 3.5923\n",
      "Epoch [8/10] Batch 419/510               Loss D: 0.0211, loss G: 3.5592\n",
      "Epoch [8/10] Batch 420/510               Loss D: 0.0217, loss G: 3.5591\n",
      "Epoch [8/10] Batch 421/510               Loss D: 0.0208, loss G: 3.5709\n",
      "Epoch [8/10] Batch 422/510               Loss D: 0.0205, loss G: 3.6384\n",
      "Epoch [8/10] Batch 423/510               Loss D: 0.0197, loss G: 3.6694\n",
      "Epoch [8/10] Batch 424/510               Loss D: 0.0224, loss G: 3.7171\n",
      "Epoch [8/10] Batch 425/510               Loss D: 0.0182, loss G: 3.7346\n",
      "Epoch [8/10] Batch 426/510               Loss D: 0.0177, loss G: 3.7840\n",
      "Epoch [8/10] Batch 427/510               Loss D: 0.0174, loss G: 3.8961\n",
      "Epoch [8/10] Batch 428/510               Loss D: 0.0159, loss G: 3.8888\n",
      "Epoch [8/10] Batch 429/510               Loss D: 0.0146, loss G: 3.9605\n",
      "Epoch [8/10] Batch 430/510               Loss D: 0.0144, loss G: 3.9937\n",
      "Epoch [8/10] Batch 431/510               Loss D: 0.0135, loss G: 4.0391\n",
      "Epoch [8/10] Batch 432/510               Loss D: 0.0542, loss G: 4.0286\n",
      "Epoch [8/10] Batch 433/510               Loss D: 0.0127, loss G: 4.0677\n",
      "Epoch [8/10] Batch 434/510               Loss D: 0.0132, loss G: 4.0416\n",
      "Epoch [8/10] Batch 435/510               Loss D: 0.0126, loss G: 4.1067\n",
      "Epoch [8/10] Batch 436/510               Loss D: 0.0125, loss G: 4.0701\n",
      "Epoch [8/10] Batch 437/510               Loss D: 0.0136, loss G: 4.0406\n",
      "Epoch [8/10] Batch 438/510               Loss D: 0.0122, loss G: 4.1097\n",
      "Epoch [8/10] Batch 439/510               Loss D: 0.0128, loss G: 4.0725\n",
      "Epoch [8/10] Batch 440/510               Loss D: 0.0175, loss G: 4.0696\n",
      "Epoch [8/10] Batch 441/510               Loss D: 0.0119, loss G: 4.1472\n",
      "Epoch [8/10] Batch 442/510               Loss D: 0.0113, loss G: 4.1810\n",
      "Epoch [8/10] Batch 443/510               Loss D: 0.1018, loss G: 4.1263\n",
      "Epoch [8/10] Batch 444/510               Loss D: 0.0138, loss G: 4.0418\n",
      "Epoch [8/10] Batch 445/510               Loss D: 0.0769, loss G: 3.9897\n",
      "Epoch [8/10] Batch 446/510               Loss D: 0.0140, loss G: 3.8802\n",
      "Epoch [8/10] Batch 447/510               Loss D: 0.0149, loss G: 3.8417\n",
      "Epoch [8/10] Batch 448/510               Loss D: 0.1210, loss G: 3.7448\n",
      "Epoch [8/10] Batch 449/510               Loss D: 0.0178, loss G: 3.6095\n",
      "Epoch [8/10] Batch 450/510               Loss D: 0.0198, loss G: 3.5147\n",
      "Epoch [8/10] Batch 451/510               Loss D: 0.0222, loss G: 3.4761\n",
      "Epoch [8/10] Batch 452/510               Loss D: 0.0214, loss G: 3.4745\n",
      "Epoch [8/10] Batch 453/510               Loss D: 0.0238, loss G: 3.4693\n",
      "Epoch [8/10] Batch 454/510               Loss D: 0.0216, loss G: 3.5089\n",
      "Epoch [8/10] Batch 455/510               Loss D: 0.0216, loss G: 3.5258\n",
      "Epoch [8/10] Batch 456/510               Loss D: 0.2435, loss G: 3.3723\n",
      "Epoch [8/10] Batch 457/510               Loss D: 0.0233, loss G: 3.3425\n",
      "Epoch [8/10] Batch 458/510               Loss D: 0.0252, loss G: 3.2972\n",
      "Epoch [8/10] Batch 459/510               Loss D: 0.0709, loss G: 3.2582\n",
      "Epoch [8/10] Batch 460/510               Loss D: 0.0276, loss G: 3.2532\n",
      "Epoch [8/10] Batch 461/510               Loss D: 0.0279, loss G: 3.2528\n",
      "Epoch [8/10] Batch 462/510               Loss D: 0.0283, loss G: 3.2589\n",
      "Epoch [8/10] Batch 463/510               Loss D: 0.0263, loss G: 3.3501\n",
      "Epoch [8/10] Batch 464/510               Loss D: 0.0265, loss G: 3.4050\n",
      "Epoch [8/10] Batch 465/510               Loss D: 0.0246, loss G: 3.4751\n",
      "Epoch [8/10] Batch 466/510               Loss D: 0.0472, loss G: 3.5965\n",
      "Epoch [8/10] Batch 467/510               Loss D: 0.0200, loss G: 3.6955\n",
      "Epoch [8/10] Batch 468/510               Loss D: 0.0200, loss G: 3.7066\n",
      "Epoch [8/10] Batch 469/510               Loss D: 0.0185, loss G: 3.8046\n",
      "Epoch [8/10] Batch 470/510               Loss D: 0.0167, loss G: 3.8929\n",
      "Epoch [8/10] Batch 471/510               Loss D: 0.0219, loss G: 3.9650\n",
      "Epoch [8/10] Batch 472/510               Loss D: 0.0148, loss G: 4.0369\n",
      "Epoch [8/10] Batch 473/510               Loss D: 0.0143, loss G: 4.0864\n",
      "Epoch [8/10] Batch 474/510               Loss D: 0.0136, loss G: 4.1282\n",
      "Epoch [8/10] Batch 475/510               Loss D: 2.8373, loss G: 3.8949\n",
      "Epoch [8/10] Batch 476/510               Loss D: 0.0165, loss G: 3.6691\n",
      "Epoch [8/10] Batch 477/510               Loss D: 0.0211, loss G: 3.4895\n",
      "Epoch [8/10] Batch 478/510               Loss D: 0.0258, loss G: 3.3290\n",
      "Epoch [8/10] Batch 479/510               Loss D: 0.0312, loss G: 3.1646\n",
      "Epoch [8/10] Batch 480/510               Loss D: 0.0337, loss G: 3.1477\n",
      "Epoch [8/10] Batch 481/510               Loss D: 0.0402, loss G: 3.0427\n",
      "Epoch [8/10] Batch 482/510               Loss D: 0.0416, loss G: 3.0479\n",
      "Epoch [8/10] Batch 483/510               Loss D: 0.0411, loss G: 3.0741\n",
      "Epoch [8/10] Batch 484/510               Loss D: 0.0398, loss G: 3.1420\n",
      "Epoch [8/10] Batch 485/510               Loss D: 0.0404, loss G: 3.1914\n",
      "Epoch [8/10] Batch 486/510               Loss D: 0.0365, loss G: 3.3120\n",
      "Epoch [8/10] Batch 487/510               Loss D: 0.0361, loss G: 3.3846\n",
      "Epoch [8/10] Batch 488/510               Loss D: 0.0330, loss G: 3.4465\n",
      "Epoch [8/10] Batch 489/510               Loss D: 0.0291, loss G: 3.6147\n",
      "Epoch [8/10] Batch 490/510               Loss D: 0.0260, loss G: 3.7112\n",
      "Epoch [8/10] Batch 491/510               Loss D: 0.0255, loss G: 3.8186\n",
      "Epoch [8/10] Batch 492/510               Loss D: 0.0730, loss G: 3.8552\n",
      "Epoch [8/10] Batch 493/510               Loss D: 0.0231, loss G: 3.8755\n",
      "Epoch [8/10] Batch 494/510               Loss D: 0.0213, loss G: 3.9551\n",
      "Epoch [8/10] Batch 495/510               Loss D: 0.0214, loss G: 4.0150\n",
      "Epoch [8/10] Batch 496/510               Loss D: 0.0213, loss G: 4.0165\n",
      "Epoch [8/10] Batch 497/510               Loss D: 0.0182, loss G: 4.1276\n",
      "Epoch [8/10] Batch 498/510               Loss D: 0.0182, loss G: 4.1893\n",
      "Epoch [8/10] Batch 499/510               Loss D: 0.0209, loss G: 4.1249\n",
      "Epoch [8/10] Batch 500/510               Loss D: 0.0213, loss G: 4.1787\n",
      "Epoch [8/10] Batch 501/510               Loss D: 0.0184, loss G: 4.1652\n",
      "Epoch [8/10] Batch 502/510               Loss D: 0.0203, loss G: 4.1193\n",
      "Epoch [8/10] Batch 503/510               Loss D: 0.0206, loss G: 4.1097\n",
      "Epoch [8/10] Batch 504/510               Loss D: 0.0203, loss G: 4.1827\n",
      "Epoch [8/10] Batch 505/510               Loss D: 0.0176, loss G: 4.2604\n",
      "Epoch [8/10] Batch 506/510               Loss D: 0.0187, loss G: 4.2128\n",
      "Epoch [8/10] Batch 507/510               Loss D: 0.0181, loss G: 4.2129\n",
      "Epoch [8/10] Batch 508/510               Loss D: 0.0185, loss G: 4.1826\n",
      "Epoch [8/10] Batch 509/510               Loss D: 0.0185, loss G: 4.1713\n",
      "Epoch [9/10] Batch 0/510               Loss D: 0.0174, loss G: 4.2089\n",
      "Epoch [9/10] Batch 1/510               Loss D: 0.0197, loss G: 4.1662\n",
      "Epoch [9/10] Batch 2/510               Loss D: 0.0204, loss G: 4.1289\n",
      "Epoch [9/10] Batch 3/510               Loss D: 0.0193, loss G: 4.1345\n",
      "Epoch [9/10] Batch 4/510               Loss D: 0.0183, loss G: 4.1819\n",
      "Epoch [9/10] Batch 5/510               Loss D: 0.0198, loss G: 4.1000\n",
      "Epoch [9/10] Batch 6/510               Loss D: 0.0204, loss G: 4.0961\n",
      "Epoch [9/10] Batch 7/510               Loss D: 0.0200, loss G: 4.0894\n",
      "Epoch [9/10] Batch 8/510               Loss D: 0.0181, loss G: 4.1264\n",
      "Epoch [9/10] Batch 9/510               Loss D: 0.0201, loss G: 4.1171\n",
      "Epoch [9/10] Batch 10/510               Loss D: 0.0184, loss G: 4.2486\n",
      "Epoch [9/10] Batch 11/510               Loss D: 0.0389, loss G: 4.1443\n",
      "Epoch [9/10] Batch 12/510               Loss D: 0.0200, loss G: 4.1536\n",
      "Epoch [9/10] Batch 13/510               Loss D: 0.0198, loss G: 4.1164\n",
      "Epoch [9/10] Batch 14/510               Loss D: 0.0201, loss G: 4.0656\n",
      "Epoch [9/10] Batch 15/510               Loss D: 0.0196, loss G: 4.1270\n",
      "Epoch [9/10] Batch 16/510               Loss D: 0.0220, loss G: 4.0669\n",
      "Epoch [9/10] Batch 17/510               Loss D: 0.2566, loss G: 3.9191\n",
      "Epoch [9/10] Batch 18/510               Loss D: 0.0230, loss G: 3.8146\n",
      "Epoch [9/10] Batch 19/510               Loss D: 0.0271, loss G: 3.7079\n",
      "Epoch [9/10] Batch 20/510               Loss D: 0.0261, loss G: 3.6847\n",
      "Epoch [9/10] Batch 21/510               Loss D: 0.0272, loss G: 3.6737\n",
      "Epoch [9/10] Batch 22/510               Loss D: 0.0407, loss G: 3.5883\n",
      "Epoch [9/10] Batch 23/510               Loss D: 0.0334, loss G: 3.5613\n",
      "Epoch [9/10] Batch 24/510               Loss D: 0.0354, loss G: 3.5264\n",
      "Epoch [9/10] Batch 25/510               Loss D: 0.0342, loss G: 3.5359\n",
      "Epoch [9/10] Batch 26/510               Loss D: 0.0329, loss G: 3.5651\n",
      "Epoch [9/10] Batch 27/510               Loss D: 0.0287, loss G: 3.6966\n",
      "Epoch [9/10] Batch 28/510               Loss D: 0.0267, loss G: 3.8201\n",
      "Epoch [9/10] Batch 29/510               Loss D: 0.0247, loss G: 3.8947\n",
      "Epoch [9/10] Batch 30/510               Loss D: 0.0275, loss G: 3.8514\n",
      "Epoch [9/10] Batch 31/510               Loss D: 0.0260, loss G: 3.9375\n",
      "Epoch [9/10] Batch 32/510               Loss D: 0.0217, loss G: 4.0606\n",
      "Epoch [9/10] Batch 33/510               Loss D: 0.0211, loss G: 4.0942\n",
      "Epoch [9/10] Batch 34/510               Loss D: 0.0196, loss G: 4.2116\n",
      "Epoch [9/10] Batch 35/510               Loss D: 0.0181, loss G: 4.1972\n",
      "Epoch [9/10] Batch 36/510               Loss D: 0.0186, loss G: 4.2424\n",
      "Epoch [9/10] Batch 37/510               Loss D: 0.0448, loss G: 4.2842\n",
      "Epoch [9/10] Batch 38/510               Loss D: 0.0161, loss G: 4.3217\n",
      "Epoch [9/10] Batch 39/510               Loss D: 0.0156, loss G: 4.3587\n",
      "Epoch [9/10] Batch 40/510               Loss D: 0.0163, loss G: 4.3422\n",
      "Epoch [9/10] Batch 41/510               Loss D: 0.0159, loss G: 4.3462\n",
      "Epoch [9/10] Batch 42/510               Loss D: 0.0159, loss G: 4.4391\n",
      "Epoch [9/10] Batch 43/510               Loss D: 0.0151, loss G: 4.4285\n",
      "Epoch [9/10] Batch 44/510               Loss D: 0.0166, loss G: 4.3950\n",
      "Epoch [9/10] Batch 45/510               Loss D: 0.0157, loss G: 4.4112\n",
      "Epoch [9/10] Batch 46/510               Loss D: 0.0151, loss G: 4.3890\n",
      "Epoch [9/10] Batch 47/510               Loss D: 0.0169, loss G: 4.3622\n",
      "Epoch [9/10] Batch 48/510               Loss D: 0.1427, loss G: 4.4057\n",
      "Epoch [9/10] Batch 49/510               Loss D: 0.0489, loss G: 4.3169\n",
      "Epoch [9/10] Batch 50/510               Loss D: 0.0189, loss G: 4.2460\n",
      "Epoch [9/10] Batch 51/510               Loss D: 0.0180, loss G: 4.1828\n",
      "Epoch [9/10] Batch 52/510               Loss D: 0.0189, loss G: 4.1488\n",
      "Epoch [9/10] Batch 53/510               Loss D: 0.0221, loss G: 4.0950\n",
      "Epoch [9/10] Batch 54/510               Loss D: 0.0215, loss G: 4.0630\n",
      "Epoch [9/10] Batch 55/510               Loss D: 0.0210, loss G: 4.0490\n",
      "Epoch [9/10] Batch 56/510               Loss D: 0.0217, loss G: 3.9936\n",
      "Epoch [9/10] Batch 57/510               Loss D: 0.0250, loss G: 3.9404\n",
      "Epoch [9/10] Batch 58/510               Loss D: 0.0247, loss G: 3.9450\n",
      "Epoch [9/10] Batch 59/510               Loss D: 0.0243, loss G: 3.9955\n",
      "Epoch [9/10] Batch 60/510               Loss D: 0.0254, loss G: 3.9511\n",
      "Epoch [9/10] Batch 61/510               Loss D: 0.3252, loss G: 3.8743\n",
      "Epoch [9/10] Batch 62/510               Loss D: 0.0274, loss G: 3.7584\n",
      "Epoch [9/10] Batch 63/510               Loss D: 0.0264, loss G: 3.7320\n",
      "Epoch [9/10] Batch 64/510               Loss D: 0.0282, loss G: 3.6559\n",
      "Epoch [9/10] Batch 65/510               Loss D: 0.0291, loss G: 3.6568\n",
      "Epoch [9/10] Batch 66/510               Loss D: 0.0291, loss G: 3.6542\n",
      "Epoch [9/10] Batch 67/510               Loss D: 0.0295, loss G: 3.6413\n",
      "Epoch [9/10] Batch 68/510               Loss D: 0.0284, loss G: 3.7239\n",
      "Epoch [9/10] Batch 69/510               Loss D: 0.0282, loss G: 3.6760\n",
      "Epoch [9/10] Batch 70/510               Loss D: 0.0285, loss G: 3.6749\n",
      "Epoch [9/10] Batch 71/510               Loss D: 0.0273, loss G: 3.7576\n",
      "Epoch [9/10] Batch 72/510               Loss D: 0.0282, loss G: 3.8200\n",
      "Epoch [9/10] Batch 73/510               Loss D: 0.0221, loss G: 3.8616\n",
      "Epoch [9/10] Batch 74/510               Loss D: 0.0226, loss G: 3.8773\n",
      "Epoch [9/10] Batch 75/510               Loss D: 0.0210, loss G: 3.9340\n",
      "Epoch [9/10] Batch 76/510               Loss D: 0.0186, loss G: 3.9896\n",
      "Epoch [9/10] Batch 77/510               Loss D: 0.0200, loss G: 3.9629\n",
      "Epoch [9/10] Batch 78/510               Loss D: 0.0181, loss G: 4.0574\n",
      "Epoch [9/10] Batch 79/510               Loss D: 0.0204, loss G: 4.0866\n",
      "Epoch [9/10] Batch 80/510               Loss D: 0.0164, loss G: 4.1212\n",
      "Epoch [9/10] Batch 81/510               Loss D: 0.0169, loss G: 4.1040\n",
      "Epoch [9/10] Batch 82/510               Loss D: 0.4054, loss G: 3.9928\n",
      "Epoch [9/10] Batch 83/510               Loss D: 0.0182, loss G: 3.8620\n",
      "Epoch [9/10] Batch 84/510               Loss D: 0.0210, loss G: 3.7192\n",
      "Epoch [9/10] Batch 85/510               Loss D: 0.0233, loss G: 3.5773\n",
      "Epoch [9/10] Batch 86/510               Loss D: 0.0246, loss G: 3.5313\n",
      "Epoch [9/10] Batch 87/510               Loss D: 0.0406, loss G: 3.4723\n",
      "Epoch [9/10] Batch 88/510               Loss D: 0.0306, loss G: 3.3872\n",
      "Epoch [9/10] Batch 89/510               Loss D: 0.0308, loss G: 3.3888\n",
      "Epoch [9/10] Batch 90/510               Loss D: 0.0301, loss G: 3.3950\n",
      "Epoch [9/10] Batch 91/510               Loss D: 0.0330, loss G: 3.3348\n",
      "Epoch [9/10] Batch 92/510               Loss D: 0.0317, loss G: 3.3753\n",
      "Epoch [9/10] Batch 93/510               Loss D: 0.0323, loss G: 3.3834\n",
      "Epoch [9/10] Batch 94/510               Loss D: 0.0329, loss G: 3.3834\n",
      "Epoch [9/10] Batch 95/510               Loss D: 0.0324, loss G: 3.4232\n",
      "Epoch [9/10] Batch 96/510               Loss D: 0.0279, loss G: 3.5648\n",
      "Epoch [9/10] Batch 97/510               Loss D: 0.0286, loss G: 3.5213\n",
      "Epoch [9/10] Batch 98/510               Loss D: 0.0295, loss G: 3.5396\n",
      "Epoch [9/10] Batch 99/510               Loss D: 0.0257, loss G: 3.6413\n",
      "Epoch [9/10] Batch 100/510               Loss D: 0.0254, loss G: 3.7256\n",
      "Epoch [9/10] Batch 101/510               Loss D: 0.0236, loss G: 3.7863\n",
      "Epoch [9/10] Batch 102/510               Loss D: 0.0224, loss G: 3.8261\n",
      "Epoch [9/10] Batch 103/510               Loss D: 0.0240, loss G: 3.8024\n",
      "Epoch [9/10] Batch 104/510               Loss D: 0.0212, loss G: 3.9322\n",
      "Epoch [9/10] Batch 105/510               Loss D: 0.0244, loss G: 3.8433\n",
      "Epoch [9/10] Batch 106/510               Loss D: 0.0237, loss G: 3.8488\n",
      "Epoch [9/10] Batch 107/510               Loss D: 0.0214, loss G: 3.9906\n",
      "Epoch [9/10] Batch 108/510               Loss D: 0.1814, loss G: 3.9060\n",
      "Epoch [9/10] Batch 109/510               Loss D: 0.0222, loss G: 3.8742\n",
      "Epoch [9/10] Batch 110/510               Loss D: 0.0242, loss G: 3.9306\n",
      "Epoch [9/10] Batch 111/510               Loss D: 0.0219, loss G: 3.9346\n",
      "Epoch [9/10] Batch 112/510               Loss D: 0.0253, loss G: 3.8913\n",
      "Epoch [9/10] Batch 113/510               Loss D: 0.0261, loss G: 3.9145\n",
      "Epoch [9/10] Batch 114/510               Loss D: 0.0319, loss G: 3.8834\n",
      "Epoch [9/10] Batch 115/510               Loss D: 0.0278, loss G: 3.8996\n",
      "Epoch [9/10] Batch 116/510               Loss D: 0.0473, loss G: 3.8922\n",
      "Epoch [9/10] Batch 117/510               Loss D: 0.0300, loss G: 3.8133\n",
      "Epoch [9/10] Batch 118/510               Loss D: 0.0307, loss G: 3.8656\n",
      "Epoch [9/10] Batch 119/510               Loss D: 0.0284, loss G: 3.8827\n",
      "Epoch [9/10] Batch 120/510               Loss D: 0.0286, loss G: 3.8925\n",
      "Epoch [9/10] Batch 121/510               Loss D: 0.0272, loss G: 3.9548\n",
      "Epoch [9/10] Batch 122/510               Loss D: 0.0316, loss G: 3.9291\n",
      "Epoch [9/10] Batch 123/510               Loss D: 0.0741, loss G: 3.9011\n",
      "Epoch [9/10] Batch 124/510               Loss D: 0.0263, loss G: 3.9903\n",
      "Epoch [9/10] Batch 125/510               Loss D: 0.0286, loss G: 4.0235\n",
      "Epoch [9/10] Batch 126/510               Loss D: 0.0305, loss G: 4.0025\n",
      "Epoch [9/10] Batch 127/510               Loss D: 0.0256, loss G: 4.0676\n",
      "Epoch [9/10] Batch 128/510               Loss D: 0.0298, loss G: 4.0607\n",
      "Epoch [9/10] Batch 129/510               Loss D: 0.0262, loss G: 4.1229\n",
      "Epoch [9/10] Batch 130/510               Loss D: 0.0293, loss G: 4.0118\n",
      "Epoch [9/10] Batch 131/510               Loss D: 0.0272, loss G: 4.1060\n",
      "Epoch [9/10] Batch 132/510               Loss D: 0.0260, loss G: 4.1863\n",
      "Epoch [9/10] Batch 133/510               Loss D: 0.0284, loss G: 4.0798\n",
      "Epoch [9/10] Batch 134/510               Loss D: 0.0280, loss G: 4.2386\n",
      "Epoch [9/10] Batch 135/510               Loss D: 0.0254, loss G: 4.2628\n",
      "Epoch [9/10] Batch 136/510               Loss D: 0.0215, loss G: 4.3355\n",
      "Epoch [9/10] Batch 137/510               Loss D: 0.0239, loss G: 4.2716\n",
      "Epoch [9/10] Batch 138/510               Loss D: 0.0616, loss G: 4.3364\n",
      "Epoch [9/10] Batch 139/510               Loss D: 0.0230, loss G: 4.2327\n",
      "Epoch [9/10] Batch 140/510               Loss D: 0.0229, loss G: 4.2598\n",
      "Epoch [9/10] Batch 141/510               Loss D: 0.0212, loss G: 4.3650\n",
      "Epoch [9/10] Batch 142/510               Loss D: 0.0280, loss G: 4.3208\n",
      "Epoch [9/10] Batch 143/510               Loss D: 0.1934, loss G: 4.2382\n",
      "Epoch [9/10] Batch 144/510               Loss D: 0.0240, loss G: 4.0940\n",
      "Epoch [9/10] Batch 145/510               Loss D: 0.0229, loss G: 4.1824\n",
      "Epoch [9/10] Batch 146/510               Loss D: 0.0237, loss G: 4.1287\n",
      "Epoch [9/10] Batch 147/510               Loss D: 0.0231, loss G: 4.1357\n",
      "Epoch [9/10] Batch 148/510               Loss D: 0.0228, loss G: 4.1123\n",
      "Epoch [9/10] Batch 149/510               Loss D: 0.0253, loss G: 4.0510\n",
      "Epoch [9/10] Batch 150/510               Loss D: 0.0267, loss G: 4.0349\n",
      "Epoch [9/10] Batch 151/510               Loss D: 0.0272, loss G: 4.0269\n",
      "Epoch [9/10] Batch 152/510               Loss D: 0.0264, loss G: 4.0471\n",
      "Epoch [9/10] Batch 153/510               Loss D: 0.0262, loss G: 4.0308\n",
      "Epoch [9/10] Batch 154/510               Loss D: 0.0250, loss G: 4.1035\n",
      "Epoch [9/10] Batch 155/510               Loss D: 0.1236, loss G: 4.1384\n",
      "Epoch [9/10] Batch 156/510               Loss D: 0.0365, loss G: 4.0124\n",
      "Epoch [9/10] Batch 157/510               Loss D: 0.0258, loss G: 3.9667\n",
      "Epoch [9/10] Batch 158/510               Loss D: 0.0258, loss G: 3.9904\n",
      "Epoch [9/10] Batch 159/510               Loss D: 0.0235, loss G: 4.0716\n",
      "Epoch [9/10] Batch 160/510               Loss D: 0.0252, loss G: 3.9917\n",
      "Epoch [9/10] Batch 161/510               Loss D: 0.0364, loss G: 4.0018\n",
      "Epoch [9/10] Batch 162/510               Loss D: 0.0255, loss G: 3.9840\n",
      "Epoch [9/10] Batch 163/510               Loss D: 0.0245, loss G: 4.0672\n",
      "Epoch [9/10] Batch 164/510               Loss D: 0.0244, loss G: 3.9647\n",
      "Epoch [9/10] Batch 165/510               Loss D: 0.0242, loss G: 3.9359\n",
      "Epoch [9/10] Batch 166/510               Loss D: 0.0205, loss G: 4.1320\n",
      "Epoch [9/10] Batch 167/510               Loss D: 0.0223, loss G: 4.1580\n",
      "Epoch [9/10] Batch 168/510               Loss D: 0.0215, loss G: 4.0851\n",
      "Epoch [9/10] Batch 169/510               Loss D: 0.0203, loss G: 4.1430\n",
      "Epoch [9/10] Batch 170/510               Loss D: 0.0227, loss G: 4.0775\n",
      "Epoch [9/10] Batch 171/510               Loss D: 0.0207, loss G: 4.1493\n",
      "Epoch [9/10] Batch 172/510               Loss D: 0.0206, loss G: 4.1574\n",
      "Epoch [9/10] Batch 173/510               Loss D: 0.0195, loss G: 4.2016\n",
      "Epoch [9/10] Batch 174/510               Loss D: 0.0196, loss G: 4.1527\n",
      "Epoch [9/10] Batch 175/510               Loss D: 0.0247, loss G: 4.1985\n",
      "Epoch [9/10] Batch 176/510               Loss D: 0.0184, loss G: 4.1936\n",
      "Epoch [9/10] Batch 177/510               Loss D: 2.1951, loss G: 3.8934\n",
      "Epoch [9/10] Batch 178/510               Loss D: 0.0404, loss G: 3.7453\n",
      "Epoch [9/10] Batch 179/510               Loss D: 0.9785, loss G: 3.2917\n",
      "Epoch [9/10] Batch 180/510               Loss D: 0.0410, loss G: 2.9932\n",
      "Epoch [9/10] Batch 181/510               Loss D: 0.0531, loss G: 2.7615\n",
      "Epoch [9/10] Batch 182/510               Loss D: 0.0636, loss G: 2.6488\n",
      "Epoch [9/10] Batch 183/510               Loss D: 0.0757, loss G: 2.5243\n",
      "Epoch [9/10] Batch 184/510               Loss D: 0.0846, loss G: 2.4666\n",
      "Epoch [9/10] Batch 185/510               Loss D: 0.0791, loss G: 2.5660\n",
      "Epoch [9/10] Batch 186/510               Loss D: 0.0856, loss G: 2.4963\n",
      "Epoch [9/10] Batch 187/510               Loss D: 0.0868, loss G: 2.5746\n",
      "Epoch [9/10] Batch 188/510               Loss D: 0.0793, loss G: 2.7261\n",
      "Epoch [9/10] Batch 189/510               Loss D: 0.1036, loss G: 2.8610\n",
      "Epoch [9/10] Batch 190/510               Loss D: 0.0640, loss G: 2.9785\n",
      "Epoch [9/10] Batch 191/510               Loss D: 0.0560, loss G: 3.1930\n",
      "Epoch [9/10] Batch 192/510               Loss D: 0.0449, loss G: 3.4157\n",
      "Epoch [9/10] Batch 193/510               Loss D: 0.0413, loss G: 3.5695\n",
      "Epoch [9/10] Batch 194/510               Loss D: 0.0378, loss G: 3.6716\n",
      "Epoch [9/10] Batch 195/510               Loss D: 0.0348, loss G: 3.8125\n",
      "Epoch [9/10] Batch 196/510               Loss D: 0.0320, loss G: 3.8783\n",
      "Epoch [9/10] Batch 197/510               Loss D: 0.0269, loss G: 4.0846\n",
      "Epoch [9/10] Batch 198/510               Loss D: 0.0276, loss G: 4.0838\n",
      "Epoch [9/10] Batch 199/510               Loss D: 0.0256, loss G: 4.1824\n",
      "Epoch [9/10] Batch 200/510               Loss D: 0.0231, loss G: 4.2062\n",
      "Epoch [9/10] Batch 201/510               Loss D: 0.0220, loss G: 4.3418\n",
      "Epoch [9/10] Batch 202/510               Loss D: 0.0203, loss G: 4.3718\n",
      "Epoch [9/10] Batch 203/510               Loss D: 0.0231, loss G: 4.3359\n",
      "Epoch [9/10] Batch 204/510               Loss D: 0.0223, loss G: 4.2986\n",
      "Epoch [9/10] Batch 205/510               Loss D: 0.0236, loss G: 4.2942\n",
      "Epoch [9/10] Batch 206/510               Loss D: 0.0208, loss G: 4.3607\n",
      "Epoch [9/10] Batch 207/510               Loss D: 0.0237, loss G: 4.1573\n",
      "Epoch [9/10] Batch 208/510               Loss D: 0.0257, loss G: 4.1991\n",
      "Epoch [9/10] Batch 209/510               Loss D: 0.0275, loss G: 4.1696\n",
      "Epoch [9/10] Batch 210/510               Loss D: 0.0286, loss G: 4.1061\n",
      "Epoch [9/10] Batch 211/510               Loss D: 0.0293, loss G: 4.0694\n",
      "Epoch [9/10] Batch 212/510               Loss D: 0.0304, loss G: 3.9985\n",
      "Epoch [9/10] Batch 213/510               Loss D: 0.0284, loss G: 4.0793\n",
      "Epoch [9/10] Batch 214/510               Loss D: 0.0306, loss G: 4.0019\n",
      "Epoch [9/10] Batch 215/510               Loss D: 0.0341, loss G: 3.9192\n",
      "Epoch [9/10] Batch 216/510               Loss D: 0.0385, loss G: 3.9031\n",
      "Epoch [9/10] Batch 217/510               Loss D: 0.0360, loss G: 3.8994\n",
      "Epoch [9/10] Batch 218/510               Loss D: 0.0353, loss G: 3.9043\n",
      "Epoch [9/10] Batch 219/510               Loss D: 0.0367, loss G: 3.8979\n",
      "Epoch [9/10] Batch 220/510               Loss D: 0.0355, loss G: 3.8925\n",
      "Epoch [9/10] Batch 221/510               Loss D: 0.0339, loss G: 3.9654\n",
      "Epoch [9/10] Batch 222/510               Loss D: 0.0393, loss G: 3.8630\n",
      "Epoch [9/10] Batch 223/510               Loss D: 0.0354, loss G: 3.9489\n",
      "Epoch [9/10] Batch 224/510               Loss D: 0.0350, loss G: 3.9627\n",
      "Epoch [9/10] Batch 225/510               Loss D: 0.0359, loss G: 3.9693\n",
      "Epoch [9/10] Batch 226/510               Loss D: 0.0333, loss G: 3.9433\n",
      "Epoch [9/10] Batch 227/510               Loss D: 0.0304, loss G: 4.0432\n",
      "Epoch [9/10] Batch 228/510               Loss D: 0.0293, loss G: 4.1286\n",
      "Epoch [9/10] Batch 229/510               Loss D: 0.0308, loss G: 4.0769\n",
      "Epoch [9/10] Batch 230/510               Loss D: 0.0264, loss G: 4.1567\n",
      "Epoch [9/10] Batch 231/510               Loss D: 0.0290, loss G: 4.1500\n",
      "Epoch [9/10] Batch 232/510               Loss D: 0.0271, loss G: 4.1653\n",
      "Epoch [9/10] Batch 233/510               Loss D: 0.0283, loss G: 4.1815\n",
      "Epoch [9/10] Batch 234/510               Loss D: 0.0242, loss G: 4.2446\n",
      "Epoch [9/10] Batch 235/510               Loss D: 0.0268, loss G: 4.1281\n",
      "Epoch [9/10] Batch 236/510               Loss D: 0.0241, loss G: 4.2484\n",
      "Epoch [9/10] Batch 237/510               Loss D: 0.0253, loss G: 4.1029\n",
      "Epoch [9/10] Batch 238/510               Loss D: 0.0238, loss G: 4.2932\n",
      "Epoch [9/10] Batch 239/510               Loss D: 0.0278, loss G: 4.2135\n",
      "Epoch [9/10] Batch 240/510               Loss D: 0.0216, loss G: 4.2844\n",
      "Epoch [9/10] Batch 241/510               Loss D: 0.0227, loss G: 4.2413\n",
      "Epoch [9/10] Batch 242/510               Loss D: 0.0217, loss G: 4.2501\n",
      "Epoch [9/10] Batch 243/510               Loss D: 0.0212, loss G: 4.2445\n",
      "Epoch [9/10] Batch 244/510               Loss D: 0.0221, loss G: 4.1967\n",
      "Epoch [9/10] Batch 245/510               Loss D: 0.0226, loss G: 4.1116\n",
      "Epoch [9/10] Batch 246/510               Loss D: 0.0242, loss G: 4.1502\n",
      "Epoch [9/10] Batch 247/510               Loss D: 0.0223, loss G: 4.1389\n",
      "Epoch [9/10] Batch 248/510               Loss D: 0.0242, loss G: 4.1086\n",
      "Epoch [9/10] Batch 249/510               Loss D: 0.0268, loss G: 4.1484\n",
      "Epoch [9/10] Batch 250/510               Loss D: 0.0225, loss G: 4.1116\n",
      "Epoch [9/10] Batch 251/510               Loss D: 0.0318, loss G: 4.0517\n",
      "Epoch [9/10] Batch 252/510               Loss D: 0.0247, loss G: 4.0274\n",
      "Epoch [9/10] Batch 253/510               Loss D: 0.0232, loss G: 4.0543\n",
      "Epoch [9/10] Batch 254/510               Loss D: 0.0236, loss G: 4.0256\n",
      "Epoch [9/10] Batch 255/510               Loss D: 0.0236, loss G: 4.0015\n",
      "Epoch [9/10] Batch 256/510               Loss D: 0.0494, loss G: 3.9075\n",
      "Epoch [9/10] Batch 257/510               Loss D: 0.0240, loss G: 3.9508\n",
      "Epoch [9/10] Batch 258/510               Loss D: 0.0234, loss G: 3.8965\n",
      "Epoch [9/10] Batch 259/510               Loss D: 0.0267, loss G: 3.8053\n",
      "Epoch [9/10] Batch 260/510               Loss D: 0.0255, loss G: 3.8353\n",
      "Epoch [9/10] Batch 261/510               Loss D: 0.0266, loss G: 3.8375\n",
      "Epoch [9/10] Batch 262/510               Loss D: 0.0256, loss G: 3.8111\n",
      "Epoch [9/10] Batch 263/510               Loss D: 0.0241, loss G: 3.8545\n",
      "Epoch [9/10] Batch 264/510               Loss D: 0.0249, loss G: 3.8132\n",
      "Epoch [9/10] Batch 265/510               Loss D: 0.0250, loss G: 3.8455\n",
      "Epoch [9/10] Batch 266/510               Loss D: 0.0252, loss G: 3.7899\n",
      "Epoch [9/10] Batch 267/510               Loss D: 0.0235, loss G: 3.8273\n",
      "Epoch [9/10] Batch 268/510               Loss D: 0.0240, loss G: 3.7949\n",
      "Epoch [9/10] Batch 269/510               Loss D: 0.0236, loss G: 3.7795\n",
      "Epoch [9/10] Batch 270/510               Loss D: 0.4039, loss G: 3.5633\n",
      "Epoch [9/10] Batch 271/510               Loss D: 0.0322, loss G: 3.3669\n",
      "Epoch [9/10] Batch 272/510               Loss D: 0.0352, loss G: 3.2266\n",
      "Epoch [9/10] Batch 273/510               Loss D: 0.0402, loss G: 3.1572\n",
      "Epoch [9/10] Batch 274/510               Loss D: 0.0476, loss G: 3.0700\n",
      "Epoch [9/10] Batch 275/510               Loss D: 0.0472, loss G: 3.0829\n",
      "Epoch [9/10] Batch 276/510               Loss D: 0.0446, loss G: 3.1709\n",
      "Epoch [9/10] Batch 277/510               Loss D: 0.0432, loss G: 3.1606\n",
      "Epoch [9/10] Batch 278/510               Loss D: 0.0432, loss G: 3.1690\n",
      "Epoch [9/10] Batch 279/510               Loss D: 0.0392, loss G: 3.3257\n",
      "Epoch [9/10] Batch 280/510               Loss D: 0.0405, loss G: 3.2952\n",
      "Epoch [9/10] Batch 281/510               Loss D: 0.0363, loss G: 3.4316\n",
      "Epoch [9/10] Batch 282/510               Loss D: 0.0337, loss G: 3.5120\n",
      "Epoch [9/10] Batch 283/510               Loss D: 0.0269, loss G: 3.7228\n",
      "Epoch [9/10] Batch 284/510               Loss D: 0.0283, loss G: 3.6792\n",
      "Epoch [9/10] Batch 285/510               Loss D: 0.0255, loss G: 3.7833\n",
      "Epoch [9/10] Batch 286/510               Loss D: 0.0238, loss G: 3.8738\n",
      "Epoch [9/10] Batch 287/510               Loss D: 0.0223, loss G: 3.9329\n",
      "Epoch [9/10] Batch 288/510               Loss D: 0.0216, loss G: 4.0398\n",
      "Epoch [9/10] Batch 289/510               Loss D: 0.1050, loss G: 3.9532\n",
      "Epoch [9/10] Batch 290/510               Loss D: 0.0209, loss G: 4.0488\n",
      "Epoch [9/10] Batch 291/510               Loss D: 0.0211, loss G: 3.9959\n",
      "Epoch [9/10] Batch 292/510               Loss D: 0.0209, loss G: 3.9253\n",
      "Epoch [9/10] Batch 293/510               Loss D: 0.0213, loss G: 3.9937\n",
      "Epoch [9/10] Batch 294/510               Loss D: 0.0216, loss G: 4.0196\n",
      "Epoch [9/10] Batch 295/510               Loss D: 0.0206, loss G: 4.0505\n",
      "Epoch [9/10] Batch 296/510               Loss D: 0.0205, loss G: 3.9733\n",
      "Epoch [9/10] Batch 297/510               Loss D: 0.0206, loss G: 4.0832\n",
      "Epoch [9/10] Batch 298/510               Loss D: 0.0198, loss G: 4.0447\n",
      "Epoch [9/10] Batch 299/510               Loss D: 0.0234, loss G: 3.9647\n",
      "Epoch [9/10] Batch 300/510               Loss D: 0.0273, loss G: 3.9809\n",
      "Epoch [9/10] Batch 301/510               Loss D: 0.0205, loss G: 4.1097\n",
      "Epoch [9/10] Batch 302/510               Loss D: 0.0442, loss G: 3.9244\n",
      "Epoch [9/10] Batch 303/510               Loss D: 0.0250, loss G: 3.9216\n",
      "Epoch [9/10] Batch 304/510               Loss D: 0.0236, loss G: 3.9521\n",
      "Epoch [9/10] Batch 305/510               Loss D: 0.0271, loss G: 3.9241\n",
      "Epoch [9/10] Batch 306/510               Loss D: 0.0238, loss G: 3.9381\n",
      "Epoch [9/10] Batch 307/510               Loss D: 0.0249, loss G: 3.9393\n",
      "Epoch [9/10] Batch 308/510               Loss D: 0.0317, loss G: 3.9262\n",
      "Epoch [9/10] Batch 309/510               Loss D: 0.0228, loss G: 3.9635\n",
      "Epoch [9/10] Batch 310/510               Loss D: 0.0233, loss G: 3.9871\n",
      "Epoch [9/10] Batch 311/510               Loss D: 0.0237, loss G: 4.0475\n",
      "Epoch [9/10] Batch 312/510               Loss D: 0.0210, loss G: 4.1254\n",
      "Epoch [9/10] Batch 313/510               Loss D: 0.0193, loss G: 4.2142\n",
      "Epoch [9/10] Batch 314/510               Loss D: 0.0183, loss G: 4.2449\n",
      "Epoch [9/10] Batch 315/510               Loss D: 0.0206, loss G: 4.2029\n",
      "Epoch [9/10] Batch 316/510               Loss D: 0.1542, loss G: 4.1740\n",
      "Epoch [9/10] Batch 317/510               Loss D: 0.2048, loss G: 4.1232\n",
      "Epoch [9/10] Batch 318/510               Loss D: 0.0211, loss G: 4.0564\n",
      "Epoch [9/10] Batch 319/510               Loss D: 0.0263, loss G: 4.0424\n",
      "Epoch [9/10] Batch 320/510               Loss D: 0.6045, loss G: 3.8468\n",
      "Epoch [9/10] Batch 321/510               Loss D: 0.0285, loss G: 3.6936\n",
      "Epoch [9/10] Batch 322/510               Loss D: 0.0340, loss G: 3.5551\n",
      "Epoch [9/10] Batch 323/510               Loss D: 0.0357, loss G: 3.5214\n",
      "Epoch [9/10] Batch 324/510               Loss D: 0.0374, loss G: 3.4614\n",
      "Epoch [9/10] Batch 325/510               Loss D: 0.0396, loss G: 3.4669\n",
      "Epoch [9/10] Batch 326/510               Loss D: 0.0411, loss G: 3.4466\n",
      "Epoch [9/10] Batch 327/510               Loss D: 3.0449, loss G: 3.3820\n",
      "Epoch [9/10] Batch 328/510               Loss D: 0.0425, loss G: 3.2713\n",
      "Epoch [9/10] Batch 329/510               Loss D: 0.0410, loss G: 3.2268\n",
      "Epoch [9/10] Batch 330/510               Loss D: 0.0452, loss G: 3.1797\n",
      "Epoch [9/10] Batch 331/510               Loss D: 0.0494, loss G: 3.2355\n",
      "Epoch [9/10] Batch 332/510               Loss D: 0.2896, loss G: 3.1754\n",
      "Epoch [9/10] Batch 333/510               Loss D: 0.0416, loss G: 3.1458\n",
      "Epoch [9/10] Batch 334/510               Loss D: 0.0437, loss G: 3.1059\n",
      "Epoch [9/10] Batch 335/510               Loss D: 0.0442, loss G: 3.1407\n",
      "Epoch [9/10] Batch 336/510               Loss D: 0.0438, loss G: 3.1664\n",
      "Epoch [9/10] Batch 337/510               Loss D: 0.0403, loss G: 3.2912\n",
      "Epoch [9/10] Batch 338/510               Loss D: 0.0397, loss G: 3.2709\n",
      "Epoch [9/10] Batch 339/510               Loss D: 0.0366, loss G: 3.4087\n",
      "Epoch [9/10] Batch 340/510               Loss D: 0.0384, loss G: 3.3390\n",
      "Epoch [9/10] Batch 341/510               Loss D: 0.0334, loss G: 3.5824\n",
      "Epoch [9/10] Batch 342/510               Loss D: 0.0302, loss G: 3.6512\n",
      "Epoch [9/10] Batch 343/510               Loss D: 0.0292, loss G: 3.6801\n",
      "Epoch [9/10] Batch 344/510               Loss D: 0.0307, loss G: 3.6697\n",
      "Epoch [9/10] Batch 345/510               Loss D: 0.0282, loss G: 3.8010\n",
      "Epoch [9/10] Batch 346/510               Loss D: 0.0250, loss G: 3.8790\n",
      "Epoch [9/10] Batch 347/510               Loss D: 0.0260, loss G: 3.8382\n",
      "Epoch [9/10] Batch 348/510               Loss D: 0.0257, loss G: 3.8820\n",
      "Epoch [9/10] Batch 349/510               Loss D: 0.0230, loss G: 3.9829\n",
      "Epoch [9/10] Batch 350/510               Loss D: 0.0224, loss G: 3.9514\n",
      "Epoch [9/10] Batch 351/510               Loss D: 0.0229, loss G: 3.9434\n",
      "Epoch [9/10] Batch 352/510               Loss D: 0.0228, loss G: 3.9381\n",
      "Epoch [9/10] Batch 353/510               Loss D: 0.0231, loss G: 3.9080\n",
      "Epoch [9/10] Batch 354/510               Loss D: 0.0236, loss G: 3.9232\n",
      "Epoch [9/10] Batch 355/510               Loss D: 0.0236, loss G: 3.8594\n",
      "Epoch [9/10] Batch 356/510               Loss D: 0.0240, loss G: 3.9093\n",
      "Epoch [9/10] Batch 357/510               Loss D: 0.0267, loss G: 3.8321\n",
      "Epoch [9/10] Batch 358/510               Loss D: 0.0279, loss G: 3.7906\n",
      "Epoch [9/10] Batch 359/510               Loss D: 0.0282, loss G: 3.7343\n",
      "Epoch [9/10] Batch 360/510               Loss D: 0.0251, loss G: 3.8301\n",
      "Epoch [9/10] Batch 361/510               Loss D: 0.0280, loss G: 3.7622\n",
      "Epoch [9/10] Batch 362/510               Loss D: 0.0286, loss G: 3.6938\n",
      "Epoch [9/10] Batch 363/510               Loss D: 0.0288, loss G: 3.7081\n",
      "Epoch [9/10] Batch 364/510               Loss D: 0.0285, loss G: 3.7009\n",
      "Epoch [9/10] Batch 365/510               Loss D: 0.0283, loss G: 3.7965\n",
      "Epoch [9/10] Batch 366/510               Loss D: 0.0294, loss G: 3.7229\n",
      "Epoch [9/10] Batch 367/510               Loss D: 0.0282, loss G: 3.7756\n",
      "Epoch [9/10] Batch 368/510               Loss D: 0.0279, loss G: 3.7740\n",
      "Epoch [9/10] Batch 369/510               Loss D: 0.0275, loss G: 3.7724\n",
      "Epoch [9/10] Batch 370/510               Loss D: 0.0268, loss G: 3.8251\n",
      "Epoch [9/10] Batch 371/510               Loss D: 0.0246, loss G: 3.8541\n",
      "Epoch [9/10] Batch 372/510               Loss D: 0.0242, loss G: 3.8947\n",
      "Epoch [9/10] Batch 373/510               Loss D: 0.0237, loss G: 3.9549\n",
      "Epoch [9/10] Batch 374/510               Loss D: 0.0244, loss G: 3.9299\n",
      "Epoch [9/10] Batch 375/510               Loss D: 0.0248, loss G: 3.8959\n",
      "Epoch [9/10] Batch 376/510               Loss D: 0.0225, loss G: 3.9916\n",
      "Epoch [9/10] Batch 377/510               Loss D: 0.0226, loss G: 4.0009\n",
      "Epoch [9/10] Batch 378/510               Loss D: 0.0231, loss G: 4.0052\n",
      "Epoch [9/10] Batch 379/510               Loss D: 0.0298, loss G: 4.0270\n",
      "Epoch [9/10] Batch 380/510               Loss D: 0.0227, loss G: 3.9986\n",
      "Epoch [9/10] Batch 381/510               Loss D: 0.0203, loss G: 4.0571\n",
      "Epoch [9/10] Batch 382/510               Loss D: 0.0206, loss G: 4.0510\n",
      "Epoch [9/10] Batch 383/510               Loss D: 0.0198, loss G: 4.0697\n",
      "Epoch [9/10] Batch 384/510               Loss D: 0.0251, loss G: 4.0910\n",
      "Epoch [9/10] Batch 385/510               Loss D: 0.0189, loss G: 4.0965\n",
      "Epoch [9/10] Batch 386/510               Loss D: 0.0204, loss G: 4.0465\n",
      "Epoch [9/10] Batch 387/510               Loss D: 0.0215, loss G: 4.0083\n",
      "Epoch [9/10] Batch 388/510               Loss D: 0.0204, loss G: 4.0572\n",
      "Epoch [9/10] Batch 389/510               Loss D: 0.0190, loss G: 4.1178\n",
      "Epoch [9/10] Batch 390/510               Loss D: 0.0213, loss G: 3.9855\n",
      "Epoch [9/10] Batch 391/510               Loss D: 0.0216, loss G: 4.0127\n",
      "Epoch [9/10] Batch 392/510               Loss D: 0.0198, loss G: 4.0899\n",
      "Epoch [9/10] Batch 393/510               Loss D: 0.0209, loss G: 4.0471\n",
      "Epoch [9/10] Batch 394/510               Loss D: 0.0198, loss G: 4.0564\n",
      "Epoch [9/10] Batch 395/510               Loss D: 0.0206, loss G: 4.0776\n",
      "Epoch [9/10] Batch 396/510               Loss D: 0.0208, loss G: 4.0247\n",
      "Epoch [9/10] Batch 397/510               Loss D: 0.0979, loss G: 3.9843\n",
      "Epoch [9/10] Batch 398/510               Loss D: 0.0224, loss G: 3.8676\n",
      "Epoch [9/10] Batch 399/510               Loss D: 0.0350, loss G: 3.8196\n",
      "Epoch [9/10] Batch 400/510               Loss D: 0.0239, loss G: 3.8524\n",
      "Epoch [9/10] Batch 401/510               Loss D: 0.0270, loss G: 3.7518\n",
      "Epoch [9/10] Batch 402/510               Loss D: 0.0283, loss G: 3.6910\n",
      "Epoch [9/10] Batch 403/510               Loss D: 0.0273, loss G: 3.7248\n",
      "Epoch [9/10] Batch 404/510               Loss D: 0.0280, loss G: 3.7566\n",
      "Epoch [9/10] Batch 405/510               Loss D: 0.0292, loss G: 3.7553\n",
      "Epoch [9/10] Batch 406/510               Loss D: 0.0256, loss G: 3.8510\n",
      "Epoch [9/10] Batch 407/510               Loss D: 0.0256, loss G: 3.8336\n",
      "Epoch [9/10] Batch 408/510               Loss D: 0.3044, loss G: 3.7840\n",
      "Epoch [9/10] Batch 409/510               Loss D: 0.0285, loss G: 3.6204\n",
      "Epoch [9/10] Batch 410/510               Loss D: 0.0344, loss G: 3.5500\n",
      "Epoch [9/10] Batch 411/510               Loss D: 0.0345, loss G: 3.4587\n",
      "Epoch [9/10] Batch 412/510               Loss D: 0.0360, loss G: 3.4916\n",
      "Epoch [9/10] Batch 413/510               Loss D: 0.0366, loss G: 3.4859\n",
      "Epoch [9/10] Batch 414/510               Loss D: 0.0327, loss G: 3.5693\n",
      "Epoch [9/10] Batch 415/510               Loss D: 0.0325, loss G: 3.5431\n",
      "Epoch [9/10] Batch 416/510               Loss D: 0.0599, loss G: 3.6354\n",
      "Epoch [9/10] Batch 417/510               Loss D: 0.0311, loss G: 3.7387\n",
      "Epoch [9/10] Batch 418/510               Loss D: 0.0638, loss G: 3.7845\n",
      "Epoch [9/10] Batch 419/510               Loss D: 0.0296, loss G: 3.7473\n",
      "Epoch [9/10] Batch 420/510               Loss D: 0.0314, loss G: 3.7296\n",
      "Epoch [9/10] Batch 421/510               Loss D: 0.0259, loss G: 3.7938\n",
      "Epoch [9/10] Batch 422/510               Loss D: 0.0243, loss G: 3.8534\n",
      "Epoch [9/10] Batch 423/510               Loss D: 0.0238, loss G: 3.8883\n",
      "Epoch [9/10] Batch 424/510               Loss D: 0.0288, loss G: 4.0183\n",
      "Epoch [9/10] Batch 425/510               Loss D: 0.0232, loss G: 3.9525\n",
      "Epoch [9/10] Batch 426/510               Loss D: 0.0228, loss G: 4.0408\n",
      "Epoch [9/10] Batch 427/510               Loss D: 0.0219, loss G: 4.0967\n",
      "Epoch [9/10] Batch 428/510               Loss D: 0.1830, loss G: 4.0667\n",
      "Epoch [9/10] Batch 429/510               Loss D: 0.0185, loss G: 4.0755\n",
      "Epoch [9/10] Batch 430/510               Loss D: 0.0207, loss G: 3.9545\n",
      "Epoch [9/10] Batch 431/510               Loss D: 0.0225, loss G: 3.9585\n",
      "Epoch [9/10] Batch 432/510               Loss D: 0.0200, loss G: 3.9916\n",
      "Epoch [9/10] Batch 433/510               Loss D: 0.0202, loss G: 3.9790\n",
      "Epoch [9/10] Batch 434/510               Loss D: 0.0224, loss G: 3.8984\n",
      "Epoch [9/10] Batch 435/510               Loss D: 0.0202, loss G: 3.9885\n",
      "Epoch [9/10] Batch 436/510               Loss D: 0.0209, loss G: 3.9086\n",
      "Epoch [9/10] Batch 437/510               Loss D: 0.0229, loss G: 3.9200\n",
      "Epoch [9/10] Batch 438/510               Loss D: 0.0212, loss G: 3.9540\n",
      "Epoch [9/10] Batch 439/510               Loss D: 0.0211, loss G: 3.9399\n",
      "Epoch [9/10] Batch 440/510               Loss D: 0.0893, loss G: 4.0031\n",
      "Epoch [9/10] Batch 441/510               Loss D: 0.0328, loss G: 3.9234\n",
      "Epoch [9/10] Batch 442/510               Loss D: 0.0220, loss G: 3.8924\n",
      "Epoch [9/10] Batch 443/510               Loss D: 0.0312, loss G: 3.9180\n",
      "Epoch [9/10] Batch 444/510               Loss D: 0.0215, loss G: 3.8665\n",
      "Epoch [9/10] Batch 445/510               Loss D: 0.0237, loss G: 3.8526\n",
      "Epoch [9/10] Batch 446/510               Loss D: 0.0210, loss G: 3.9587\n",
      "Epoch [9/10] Batch 447/510               Loss D: 0.1861, loss G: 3.7706\n",
      "Epoch [9/10] Batch 448/510               Loss D: 0.0237, loss G: 3.7532\n",
      "Epoch [9/10] Batch 449/510               Loss D: 0.0231, loss G: 3.7553\n",
      "Epoch [9/10] Batch 450/510               Loss D: 0.0244, loss G: 3.6976\n",
      "Epoch [9/10] Batch 451/510               Loss D: 0.0248, loss G: 3.6719\n",
      "Epoch [9/10] Batch 452/510               Loss D: 0.0269, loss G: 3.6089\n",
      "Epoch [9/10] Batch 453/510               Loss D: 0.0265, loss G: 3.6833\n",
      "Epoch [9/10] Batch 454/510               Loss D: 0.0581, loss G: 3.6260\n",
      "Epoch [9/10] Batch 455/510               Loss D: 0.0510, loss G: 3.6899\n",
      "Epoch [9/10] Batch 456/510               Loss D: 0.0235, loss G: 3.8507\n",
      "Epoch [9/10] Batch 457/510               Loss D: 0.0255, loss G: 3.7347\n",
      "Epoch [9/10] Batch 458/510               Loss D: 0.0234, loss G: 3.8008\n",
      "Epoch [9/10] Batch 459/510               Loss D: 0.0221, loss G: 3.9175\n",
      "Epoch [9/10] Batch 460/510               Loss D: 0.0296, loss G: 3.9765\n",
      "Epoch [9/10] Batch 461/510               Loss D: 0.0200, loss G: 4.0301\n",
      "Epoch [9/10] Batch 462/510               Loss D: 0.0187, loss G: 4.0552\n",
      "Epoch [9/10] Batch 463/510               Loss D: 0.0190, loss G: 4.0501\n",
      "Epoch [9/10] Batch 464/510               Loss D: 0.0172, loss G: 4.1822\n",
      "Epoch [9/10] Batch 465/510               Loss D: 0.0179, loss G: 4.1217\n",
      "Epoch [9/10] Batch 466/510               Loss D: 0.0174, loss G: 4.1762\n",
      "Epoch [9/10] Batch 467/510               Loss D: 0.0165, loss G: 4.2043\n",
      "Epoch [9/10] Batch 468/510               Loss D: 0.0165, loss G: 4.1895\n",
      "Epoch [9/10] Batch 469/510               Loss D: 0.0156, loss G: 4.2770\n",
      "Epoch [9/10] Batch 470/510               Loss D: 0.0152, loss G: 4.2496\n",
      "Epoch [9/10] Batch 471/510               Loss D: 0.0175, loss G: 4.2104\n",
      "Epoch [9/10] Batch 472/510               Loss D: 0.0155, loss G: 4.2900\n",
      "Epoch [9/10] Batch 473/510               Loss D: 0.0158, loss G: 4.2493\n",
      "Epoch [9/10] Batch 474/510               Loss D: 0.0150, loss G: 4.2442\n",
      "Epoch [9/10] Batch 475/510               Loss D: 0.0140, loss G: 4.3032\n",
      "Epoch [9/10] Batch 476/510               Loss D: 0.0154, loss G: 4.2687\n",
      "Epoch [9/10] Batch 477/510               Loss D: 0.0166, loss G: 4.1934\n",
      "Epoch [9/10] Batch 478/510               Loss D: 0.0163, loss G: 4.2033\n",
      "Epoch [9/10] Batch 479/510               Loss D: 0.0163, loss G: 4.1833\n",
      "Epoch [9/10] Batch 480/510               Loss D: 0.4852, loss G: 3.9671\n",
      "Epoch [9/10] Batch 481/510               Loss D: 0.0284, loss G: 3.7611\n",
      "Epoch [9/10] Batch 482/510               Loss D: 0.0246, loss G: 3.5765\n",
      "Epoch [9/10] Batch 483/510               Loss D: 0.0262, loss G: 3.5167\n",
      "Epoch [9/10] Batch 484/510               Loss D: 0.0291, loss G: 3.4317\n",
      "Epoch [9/10] Batch 485/510               Loss D: 0.0333, loss G: 3.3589\n",
      "Epoch [9/10] Batch 486/510               Loss D: 0.0352, loss G: 3.3345\n",
      "Epoch [9/10] Batch 487/510               Loss D: 0.0360, loss G: 3.3085\n",
      "Epoch [9/10] Batch 488/510               Loss D: 0.0365, loss G: 3.3360\n",
      "Epoch [9/10] Batch 489/510               Loss D: 0.0336, loss G: 3.4132\n",
      "Epoch [9/10] Batch 490/510               Loss D: 0.0807, loss G: 3.4822\n",
      "Epoch [9/10] Batch 491/510               Loss D: 0.0334, loss G: 3.4921\n",
      "Epoch [9/10] Batch 492/510               Loss D: 0.0322, loss G: 3.5293\n",
      "Epoch [9/10] Batch 493/510               Loss D: 0.0320, loss G: 3.5758\n",
      "Epoch [9/10] Batch 494/510               Loss D: 0.0281, loss G: 3.6810\n",
      "Epoch [9/10] Batch 495/510               Loss D: 0.0279, loss G: 3.7868\n",
      "Epoch [9/10] Batch 496/510               Loss D: 0.0266, loss G: 3.8540\n",
      "Epoch [9/10] Batch 497/510               Loss D: 0.0252, loss G: 3.9511\n",
      "Epoch [9/10] Batch 498/510               Loss D: 0.0218, loss G: 4.0429\n",
      "Epoch [9/10] Batch 499/510               Loss D: 0.0221, loss G: 4.0251\n",
      "Epoch [9/10] Batch 500/510               Loss D: 0.0203, loss G: 4.1298\n",
      "Epoch [9/10] Batch 501/510               Loss D: 0.0187, loss G: 4.2975\n",
      "Epoch [9/10] Batch 502/510               Loss D: 0.0180, loss G: 4.3269\n",
      "Epoch [9/10] Batch 503/510               Loss D: 0.0189, loss G: 4.2764\n",
      "Epoch [9/10] Batch 504/510               Loss D: 0.0178, loss G: 4.3196\n",
      "Epoch [9/10] Batch 505/510               Loss D: 0.0175, loss G: 4.3474\n",
      "Epoch [9/10] Batch 506/510               Loss D: 0.0172, loss G: 4.2595\n",
      "Epoch [9/10] Batch 507/510               Loss D: 0.0668, loss G: 4.2698\n",
      "Epoch [9/10] Batch 508/510               Loss D: 0.0184, loss G: 4.2189\n",
      "Epoch [9/10] Batch 509/510               Loss D: 0.0184, loss G: 4.1932\n"
     ]
    }
   ],
   "source": [
    "# Now, create the DataLoader using the dataset\n",
    "batch_size = 64  # Or any other batch size you wish to use\n",
    "loader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# Optimizers\n",
    "opt_disc = optim.Adam(disc.parameters(), lr=lr)\n",
    "opt_gen = optim.Adam(gen.parameters(), lr=lr)\n",
    "\n",
    "# Loss function\n",
    "criterion = nn.BCELoss()\n",
    "\n",
    "# TensorBoard writers\n",
    "writer_fake = SummaryWriter(f\"logs/fake\")\n",
    "writer_real = SummaryWriter(f\"logs/real\")\n",
    "step = 0\n",
    "\n",
    "# Assuming the generator (gen), discriminator (disc), and their optimizers (opt_gen, opt_disc) are defined\n",
    "# Also assuming a loss function (criterion) is defined\n",
    "# z_dim is the dimensionality of the latent space (noise vector)\n",
    "\n",
    "num_epochs = 10  # Number of epochs to train for\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    for batch_idx, real_embeddings in enumerate(loader):\n",
    "        batch_size = real_embeddings[0].size(0)\n",
    "        real_embeddings = real_embeddings[0].to(device)\n",
    "\n",
    "        # Train Discriminator\n",
    "        # Generate fake embeddings\n",
    "        noise = torch.randn(batch_size, z_dim, device=device)\n",
    "        fake_embeddings = gen(noise)\n",
    "\n",
    "        # Get discriminator predictions on real and fake data\n",
    "        disc_real = disc(real_embeddings).view(-1)\n",
    "        disc_fake = disc(fake_embeddings.detach()).view(-1)\n",
    "\n",
    "        # Calculate loss on real and fake\n",
    "        lossD_real = criterion(disc_real, torch.ones_like(disc_real))\n",
    "        lossD_fake = criterion(disc_fake, torch.zeros_like(disc_fake))\n",
    "        lossD = (lossD_real + lossD_fake) / 2\n",
    "\n",
    "        # Update discriminator\n",
    "        opt_disc.zero_grad()\n",
    "        lossD.backward()\n",
    "        opt_disc.step()\n",
    "\n",
    "        # Train Generator\n",
    "        # Generate fake embeddings\n",
    "        output = disc(fake_embeddings).view(-1)\n",
    "        lossG = criterion(output, torch.ones_like(output))\n",
    "\n",
    "        # Update generator\n",
    "        opt_gen.zero_grad()\n",
    "        lossG.backward()\n",
    "        opt_gen.step()\n",
    "\n",
    "        # Optional: Print out loss values or save models/checkpoints here\n",
    "\n",
    "       \n",
    "        print(f\"Epoch [{epoch}/{num_epochs}] Batch {batch_idx}/{len(loader)} \\\n",
    "              Loss D: {lossD:.4f}, loss G: {lossG:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ee8895e-1c24-4198-91bf-c24e69b5e8f4",
   "metadata": {},
   "source": [
    "# Generating a Fake CLS Token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "0d463e18-db86-480f-abdb-1aaac6fb3e01",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 768])"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_examples = 1\n",
    "noise = torch.randn(num_examples, z_dim)\n",
    "\n",
    "with torch.no_grad():  # We don't need to track gradients for generation\n",
    "    fake_data = gen(noise) \n",
    "    \n",
    "fake_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "242875ec-7b04-4a47-9a65-98d95421b215",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.7973e-01,  1.2505e-01,  9.1502e-02,  5.6659e-01,  2.0206e-01,\n",
       "          6.0736e-01, -5.7118e-01,  2.9765e-01,  7.5517e-01,  3.0931e-02,\n",
       "         -5.6140e-01, -8.9618e-02, -1.2270e-01,  4.5628e-01,  7.5275e-01,\n",
       "         -5.9645e-01,  2.0139e-01,  8.1219e-01, -3.6552e-01, -5.2813e-01,\n",
       "          3.5227e-01, -5.0198e-01,  3.6557e-01, -7.6460e-01, -1.9204e-01,\n",
       "          2.6984e-01,  2.6896e-01,  5.2190e-01,  2.2123e-01, -8.2898e-01,\n",
       "          5.6847e-01, -1.0157e-01, -3.3123e-01,  5.6599e-01,  5.3486e-01,\n",
       "          5.4067e-02, -3.4344e-01,  1.1928e-01, -2.4428e-01,  6.4715e-01,\n",
       "         -2.9957e-01, -8.0467e-01,  1.9973e-01, -2.2332e-01,  3.4915e-01,\n",
       "         -2.8261e-01, -6.6266e-01, -6.2377e-01, -2.1833e-01,  2.2443e-01,\n",
       "         -5.7208e-01,  3.7842e-01,  7.2347e-01, -3.9107e-02, -2.1125e-01,\n",
       "          3.3803e-01, -4.7618e-01, -4.0101e-01,  4.7798e-01, -2.7365e-01,\n",
       "         -4.0838e-01,  7.3872e-01, -5.5772e-01, -4.5435e-01, -3.2097e-01,\n",
       "          1.2993e-01, -4.8096e-01,  3.8967e-01,  4.2497e-01,  4.8345e-01,\n",
       "          4.7206e-01,  7.2473e-01, -2.6751e-01, -4.2703e-01,  3.0647e-02,\n",
       "          2.5555e-01,  2.1819e-01,  5.5530e-01, -6.9420e-01, -3.8784e-02,\n",
       "          1.9452e-01,  1.7098e-01, -8.3756e-01, -4.2242e-01,  1.0405e-01,\n",
       "          7.2329e-01, -4.7311e-01,  1.5962e-01,  5.8303e-01, -6.0930e-01,\n",
       "          1.9818e-01,  1.9956e-02,  2.9574e-01, -3.5633e-01, -3.6633e-01,\n",
       "          1.1867e-01,  2.9582e-01,  6.0272e-01,  4.1427e-01, -5.5049e-01,\n",
       "          3.3157e-01, -3.2954e-01, -4.3902e-01,  2.9221e-01, -1.2823e-01,\n",
       "         -8.9795e-01,  3.6511e-01,  2.2417e-01, -7.2326e-02, -5.8438e-01,\n",
       "         -5.2756e-01, -9.9727e-01, -7.0028e-01, -4.3713e-01, -7.6866e-03,\n",
       "         -6.2483e-01, -4.5493e-01, -7.6546e-01, -4.2564e-01, -1.0193e-01,\n",
       "         -5.7947e-01,  4.6115e-02, -2.8745e-01,  6.6850e-02, -5.2388e-01,\n",
       "         -4.6622e-02,  6.1546e-01,  2.8070e-01,  6.4999e-02, -3.0453e-01,\n",
       "          4.4763e-01,  1.0881e-01, -4.6869e-01,  4.9450e-01, -5.5917e-01,\n",
       "         -6.0072e-01,  2.0487e-01,  2.2772e-01, -2.4368e-01,  6.7037e-01,\n",
       "         -1.0602e-01,  1.4145e-01, -3.2622e-01, -5.8940e-03,  6.8355e-03,\n",
       "          4.8331e-01, -2.2302e-01,  4.6321e-01,  2.1580e-01, -3.0441e-01,\n",
       "          4.8763e-01, -4.4965e-01, -3.1543e-01, -2.7108e-01,  6.1403e-01,\n",
       "          5.7979e-01, -5.9853e-01,  5.1581e-01,  5.9504e-01, -9.9999e-01,\n",
       "          2.8865e-02, -3.3749e-01, -4.9511e-01, -3.1503e-01,  3.5925e-01,\n",
       "          4.2819e-01, -4.7063e-01, -6.2683e-01,  5.0376e-01, -5.7908e-01,\n",
       "          1.9101e-01,  5.6432e-01, -5.0928e-01,  4.3323e-01,  6.3283e-01,\n",
       "          1.3320e-02,  9.6244e-01,  9.7552e-01,  7.0642e-01, -6.0508e-01,\n",
       "         -7.4372e-01, -1.7256e-01,  4.0378e-01, -9.1768e-02, -4.7399e-01,\n",
       "          2.8958e-01,  5.8189e-01, -6.6465e-01,  9.0147e-01,  7.3787e-03,\n",
       "         -2.4920e-01,  3.5306e-01,  2.3776e-01,  6.8785e-01, -2.1946e-01,\n",
       "          5.7969e-01, -5.8108e-01, -7.4636e-01,  2.4038e-01, -1.7351e-01,\n",
       "         -6.2404e-01,  4.9026e-01, -2.3979e-01, -1.4213e-01,  7.3173e-01,\n",
       "          1.1492e-01,  1.6679e-01,  1.3391e-01,  3.0357e-01,  5.8521e-01,\n",
       "         -6.8845e-01,  4.4675e-01,  6.6610e-01,  2.3697e-01,  6.4308e-01,\n",
       "          4.3441e-01, -4.7654e-01,  8.0372e-01,  2.0848e-01,  3.5958e-01,\n",
       "          1.1487e-01, -2.9581e-01,  6.4311e-01, -2.5295e-01, -6.4601e-01,\n",
       "         -8.1918e-01, -8.6792e-02,  6.0899e-01,  1.4328e-01,  2.7478e-01,\n",
       "          6.7226e-03,  5.6368e-01,  1.0324e-01,  7.9794e-01,  2.6464e-01,\n",
       "         -7.0655e-02, -7.0522e-01,  1.2273e-01,  5.7644e-01,  3.3092e-01,\n",
       "          1.1189e-01,  3.4876e-01,  6.1711e-01,  2.5005e-01,  1.0679e-01,\n",
       "         -3.8449e-01, -4.0577e-01,  3.1331e-01, -7.2351e-01, -4.7938e-01,\n",
       "          4.1331e-01,  1.7818e-01, -8.1688e-01,  2.2927e-01, -6.2749e-01,\n",
       "          6.1674e-03,  4.9722e-01,  2.0610e-01,  1.7447e-01,  1.7916e-01,\n",
       "         -4.6877e-01, -3.8412e-01,  7.9022e-01, -2.0312e-01, -8.5465e-04,\n",
       "          9.0196e-02, -2.5858e-01, -2.1611e-01, -6.0114e-01,  7.7253e-02,\n",
       "          6.0695e-01, -6.7854e-02, -7.1015e-02, -4.5774e-01, -4.8607e-01,\n",
       "         -3.0692e-01,  6.1407e-01, -1.5755e-01,  7.3851e-01,  2.3176e-01,\n",
       "         -6.2925e-01, -7.6486e-02,  2.6045e-01, -5.3918e-01,  5.0357e-01,\n",
       "         -2.9140e-01, -5.6288e-01,  4.1866e-02,  2.4273e-01, -4.1102e-01,\n",
       "          1.3066e-01,  6.7068e-01,  3.3645e-01,  5.1221e-01,  3.3685e-01,\n",
       "          8.0508e-02,  3.5927e-01,  1.0999e-02, -1.8582e-01,  5.4321e-01,\n",
       "         -3.0804e-01, -4.9721e-02, -1.6958e-01,  3.5558e-01,  7.9813e-01,\n",
       "          2.3154e-01,  2.8850e-01, -3.5405e-01,  3.1215e-01, -3.2253e-01,\n",
       "         -5.1794e-01,  5.3441e-02, -2.7302e-01, -5.6061e-02,  3.5920e-01,\n",
       "         -4.2412e-01, -3.4800e-01, -4.0789e-01,  2.5362e-01, -7.1761e-02,\n",
       "          3.2359e-01, -3.7436e-01, -2.8537e-01, -2.7421e-01,  7.9881e-02,\n",
       "         -5.1293e-01, -5.8725e-01,  2.8131e-01, -3.7932e-01,  5.5766e-02,\n",
       "         -6.5126e-02,  3.6663e-02, -7.5302e-01,  7.0489e-02,  6.2794e-01,\n",
       "          1.6046e-01,  1.4538e-01, -6.8632e-02, -3.3688e-01,  7.3593e-02,\n",
       "         -5.1226e-01, -4.0711e-01,  5.0574e-01, -7.3846e-01,  7.0906e-01,\n",
       "         -1.7553e-01, -5.7283e-01,  2.5573e-01,  6.8042e-02,  4.4403e-01,\n",
       "         -3.1893e-02, -2.4025e-01,  2.9267e-02, -5.4918e-01, -3.8750e-01,\n",
       "          5.7909e-01,  3.7974e-01, -3.2201e-01, -9.9250e-02,  3.4204e-01,\n",
       "         -9.1342e-02,  6.5520e-01, -2.5673e-01,  6.5498e-02,  6.4966e-01,\n",
       "          2.5758e-01, -4.4333e-01,  6.5378e-01,  3.3255e-01, -5.3631e-01,\n",
       "          1.2599e-01, -7.0356e-01, -3.8049e-01,  2.6065e-01,  5.8956e-01,\n",
       "          2.0813e-01,  1.6502e-01,  8.6193e-02,  2.3711e-01, -7.2882e-03,\n",
       "          1.9444e-01,  4.7813e-01, -3.2475e-01, -5.6913e-02,  4.4951e-01,\n",
       "         -1.7582e-01,  6.6626e-01,  6.1991e-01,  3.7018e-01,  4.6219e-02,\n",
       "          6.3040e-01, -1.7458e-02, -5.6435e-01,  3.2566e-01,  6.9715e-01,\n",
       "          3.4622e-01, -1.2466e-01,  2.2216e-01,  6.1250e-01, -1.9203e-01,\n",
       "          5.7798e-01, -5.3428e-01, -3.7181e-01, -5.1940e-02,  2.1344e-01,\n",
       "          3.3271e-01, -4.6050e-01, -1.1107e-01, -9.0714e-02, -4.1712e-01,\n",
       "          3.9032e-01,  1.2647e-01,  2.1718e-01, -3.8790e-01, -2.3833e-01,\n",
       "         -7.6401e-01,  3.1697e-01, -4.3593e-01, -4.4593e-01,  1.7504e-01,\n",
       "          4.1172e-01,  4.4491e-01,  4.7031e-02,  4.1057e-01,  6.4103e-01,\n",
       "         -6.5276e-01, -6.0959e-01, -2.9612e-01,  3.0406e-01, -5.6113e-01,\n",
       "         -4.4052e-01, -9.5524e-02,  5.0554e-01, -5.3764e-01, -6.6443e-01,\n",
       "         -3.4242e-01,  6.6223e-01,  1.3457e-01, -1.9843e-01,  4.6561e-01,\n",
       "         -1.5927e-01,  1.6545e-01,  5.2828e-01, -3.9229e-01, -2.6420e-01,\n",
       "         -8.2508e-01,  1.6244e-01,  5.2977e-01,  5.1690e-03, -4.8714e-01,\n",
       "         -4.2040e-01, -1.7770e-01,  7.9072e-01, -7.5477e-01,  3.3812e-01,\n",
       "          3.7536e-01, -2.3695e-01, -7.1160e-02,  1.8610e-01, -9.9998e-01,\n",
       "          1.6298e-01,  5.1800e-01, -1.8596e-01, -2.8647e-01,  7.0905e-01,\n",
       "          2.2927e-01,  3.6586e-01,  4.1382e-01,  1.3806e-01,  4.5765e-01,\n",
       "         -7.4436e-01, -1.8552e-02, -1.0167e-01,  4.1036e-01,  1.9957e-02,\n",
       "         -7.4938e-01,  5.6413e-01, -4.0311e-01,  6.0260e-01, -7.4911e-01,\n",
       "         -4.8112e-01, -5.5968e-01, -5.8892e-01,  3.5823e-02,  6.0311e-01,\n",
       "         -2.8883e-01, -5.0553e-01, -3.3559e-01, -2.4570e-01, -5.0017e-01,\n",
       "          3.5662e-01,  3.0403e-01, -2.1090e-01,  3.9049e-01,  5.1181e-01,\n",
       "         -5.8201e-01, -5.5344e-01,  6.8077e-01,  2.9652e-02, -1.6314e-01,\n",
       "          3.9072e-01, -4.1405e-01, -3.5042e-01, -3.1538e-01,  8.6275e-02,\n",
       "          6.1690e-01,  5.5481e-01,  8.0122e-01,  6.5673e-02, -3.0060e-01,\n",
       "          7.0183e-01, -3.3013e-01,  6.1762e-01,  3.9423e-01,  3.7363e-01,\n",
       "         -4.1003e-01,  3.2684e-01, -5.6321e-01,  1.1301e-02, -3.8606e-01,\n",
       "         -2.8546e-01, -6.7390e-01, -5.5070e-01, -6.2147e-01,  4.0515e-01,\n",
       "          3.5387e-01,  4.6442e-01, -1.4871e-01,  3.7199e-01, -3.1856e-01,\n",
       "          9.2596e-01, -1.2863e-01, -2.7296e-01,  1.0517e-01, -2.2637e-01,\n",
       "         -4.5116e-03, -2.4006e-02, -2.6191e-01, -9.1049e-02, -3.5584e-01,\n",
       "         -1.2167e-01, -3.1151e-02, -1.0725e-01,  1.7586e-01, -3.0822e-01,\n",
       "         -6.9133e-01,  8.6540e-01, -3.7321e-01, -3.4368e-01,  6.8241e-01,\n",
       "         -1.6153e-01, -8.8537e-02, -5.8486e-01,  1.9283e-01,  1.5036e-01,\n",
       "          1.3514e-01, -6.4434e-01,  5.9412e-01,  4.5961e-01,  2.2662e-01,\n",
       "         -4.0041e-01,  1.8975e-02,  2.3929e-01, -2.0727e-02,  5.2369e-01,\n",
       "         -2.1322e-01, -5.9078e-02,  2.7050e-01, -6.0665e-01, -5.1223e-02,\n",
       "          6.7326e-01, -4.4272e-01,  2.1577e-01,  5.1425e-01, -4.1485e-01,\n",
       "         -1.2820e-01, -1.9517e-01, -5.2544e-01, -2.0424e-01, -4.7880e-01,\n",
       "         -3.8943e-01, -3.0762e-01,  5.3111e-01,  6.0155e-01,  1.9524e-01,\n",
       "         -2.4680e-01,  6.8769e-01,  8.8590e-02, -7.1189e-01,  1.1352e-02,\n",
       "         -4.1302e-01, -5.9097e-01,  1.5271e-01, -2.1496e-01,  5.0814e-01,\n",
       "          1.9360e-01,  3.4951e-01,  2.4482e-02, -7.5462e-01,  3.1777e-01,\n",
       "          4.5188e-01, -4.8309e-01,  5.5380e-01, -7.9585e-01,  6.6321e-01,\n",
       "         -2.6008e-01, -4.9462e-01, -4.4151e-01, -6.2717e-01,  1.2987e-01,\n",
       "          3.7547e-01,  4.4491e-01,  2.6090e-01,  4.5785e-01,  2.3826e-01,\n",
       "         -4.2311e-01, -6.5217e-01,  6.1347e-01, -4.9639e-01, -1.4386e-01,\n",
       "          3.0287e-01,  2.3011e-01, -9.8280e-02,  5.6946e-01,  1.9721e-01,\n",
       "         -3.7435e-01,  5.4754e-01, -3.6570e-01,  3.6249e-01,  3.0080e-01,\n",
       "          1.9859e-01,  5.9593e-01,  3.5370e-01,  6.8994e-01, -1.8893e-01,\n",
       "          5.6598e-01, -5.5707e-01, -8.6772e-02, -4.0316e-01,  2.9027e-01,\n",
       "         -5.7157e-01, -1.8817e-01,  4.2705e-01, -3.8553e-01, -1.3447e-02,\n",
       "         -4.6846e-01, -2.0069e-01,  5.1821e-01, -3.9158e-01, -1.3980e-01,\n",
       "          5.5546e-01, -4.7161e-01, -3.9914e-01,  8.2942e-01,  1.0110e-01,\n",
       "         -9.9120e-02, -7.5673e-01,  2.3647e-01, -2.9611e-01,  3.1721e-01,\n",
       "         -1.1956e-01,  3.5772e-01,  4.1211e-01, -3.2808e-01,  5.4745e-01,\n",
       "         -7.5272e-01,  3.8905e-01,  6.4190e-01, -2.4192e-01,  5.6408e-01,\n",
       "         -4.7973e-01,  2.6351e-01, -7.8824e-01,  7.6553e-01,  4.4770e-01,\n",
       "         -1.1424e-01, -2.0876e-01, -2.4041e-01, -8.2860e-02, -4.9228e-01,\n",
       "          4.8421e-01,  4.8832e-01, -5.1190e-01, -1.3487e-02,  6.6269e-01,\n",
       "          1.2012e-01,  8.4556e-02,  5.2505e-01, -4.4641e-01,  5.8879e-01,\n",
       "          7.1989e-01, -1.0360e-02, -7.7997e-01,  9.0250e-02,  2.7045e-01,\n",
       "         -5.9052e-01, -3.2945e-01,  2.8679e-01,  4.9852e-01,  1.5404e-01,\n",
       "          9.9791e-03,  3.0557e-01,  2.0950e-01,  3.8398e-01,  4.9577e-02,\n",
       "          6.7340e-01, -3.9042e-01,  3.8361e-01,  2.9244e-01,  3.6515e-01,\n",
       "         -8.1221e-01,  5.4941e-01, -5.4635e-01,  2.0055e-01, -4.1062e-01,\n",
       "         -1.5347e-01,  1.6176e-01, -7.3879e-01, -1.0010e-01,  6.0469e-01,\n",
       "         -1.5142e-01, -4.4842e-01, -4.0109e-01, -2.2919e-01, -2.8565e-02,\n",
       "         -1.8062e-01, -5.0641e-01, -5.5497e-01,  5.5823e-01,  3.4040e-01,\n",
       "          6.6603e-01,  8.8253e-01,  2.9779e-01,  4.2680e-01, -4.9991e-01,\n",
       "         -5.8824e-01,  6.1012e-01,  1.1861e-01, -4.6124e-01,  2.2072e-01,\n",
       "          4.6514e-01,  4.5259e-02, -5.8251e-01, -3.3544e-01,  2.7143e-01,\n",
       "          6.7325e-02, -3.2809e-01, -7.5217e-01,  6.1612e-01,  8.9986e-02,\n",
       "          5.7436e-01,  5.3885e-01, -6.0674e-01, -3.3342e-01, -4.8071e-01,\n",
       "         -6.4063e-01, -7.6916e-02,  4.2714e-01,  6.9311e-01, -6.1319e-01,\n",
       "          9.2644e-02, -8.3156e-02,  1.6161e-02,  2.4447e-01,  7.8343e-02,\n",
       "          3.4906e-01,  2.8593e-01, -3.5730e-01]])"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fake_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e62d1b5-8641-4569-a46b-0c4c5ae7fc37",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
