{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b26b6e1b-a96d-4b8f-b69c-97caad5edb75",
   "metadata": {},
   "source": [
    "# Training a Simple GAN Model for Sentence Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f13714e-a0d7-40f6-adce-bbdc125592ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.datasets as datasets\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.tensorboard import SummaryWriter  # to print to tensorboard\n",
    "\n",
    "MAX_LENGTH = 768\n",
    "\n",
    "\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, in_features):\n",
    "        super().__init__()\n",
    "        self.disc = nn.Sequential(\n",
    "            nn.Linear(in_features, 128),\n",
    "            nn.LeakyReLU(0.01),\n",
    "            nn.Linear(128, 1),\n",
    "            nn.Sigmoid(),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.disc(x)\n",
    "\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self, z_dim, emb_dim):\n",
    "        super().__init__()\n",
    "        self.gen = nn.Sequential(\n",
    "            nn.Linear(z_dim, 256),\n",
    "            nn.LeakyReLU(0.01),\n",
    "            nn.Linear(256, emb_dim),\n",
    "            nn.Tanh(),  # Assuming you want to normalize the outputs\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.gen(x)\n",
    "\n",
    "class SentenceEmbeddingsDataset(Dataset):\n",
    "    def __init__(self, embeddings):\n",
    "        self.embeddings = embeddings\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.embeddings)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.embeddings[idx]\n",
    "\n",
    "\n",
    "# Hyperparameters etc.\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "lr = 3e-4\n",
    "z_dim = 64\n",
    "image_dim = 28 * 28 * 1  # 784\n",
    "batch_size = 32\n",
    "num_epochs = 50\n",
    "\n",
    "disc = Discriminator(image_dim).to(device)\n",
    "gen = Generator(z_dim, image_dim).to(device)\n",
    "fixed_noise = torch.randn((batch_size, z_dim)).to(device)\n",
    "transforms = transforms.Compose(\n",
    "    [\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.5,), (0.5,)),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Assuming embeddings is a 768 x 768 tensor of sentence embeddings (768 examples of 768-dimensional sentence embeddings)\n",
    "embeddings = torch.randn(768, 768)  # Placeholder for actual sentence embeddings\n",
    "labels = torch.zeros(768, 1)  # Dummy labels, not used in training but required to create the dataset\n",
    "\n",
    "dataset = TensorDataset(embeddings, labels)  # Create a dataset of embeddings\n",
    "loader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# Optimizers\n",
    "opt_disc = optim.Adam(disc.parameters(), lr=lr)\n",
    "opt_gen = optim.Adam(gen.parameters(), lr=lr)\n",
    "\n",
    "# Loss function\n",
    "criterion = nn.BCELoss()\n",
    "\n",
    "# TensorBoard writers\n",
    "writer_fake = SummaryWriter(f\"logs/fake\")\n",
    "writer_real = SummaryWriter(f\"logs/real\")\n",
    "step = 0\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    for batch_idx, (real, _) in enumerate(loader):\n",
    "        # Adjust for embeddings size\n",
    "        real = real.to(device)\n",
    "        batch_size = real.shape[0]\n",
    "\n",
    "        ### Train Discriminator: max log(D(x)) + log(1 - D(G(z)))\n",
    "        noise = torch.randn(batch_size, z_dim).to(device)\n",
    "        fake = gen(noise)\n",
    "        disc_real = disc(real).view(-1)\n",
    "        lossD_real = criterion(disc_real, torch.ones_like(disc_real))\n",
    "        disc_fake = disc(fake).view(-1)\n",
    "        lossD_fake = criterion(disc_fake, torch.zeros_like(disc_fake))\n",
    "        lossD = (lossD_real + lossD_fake) / 2\n",
    "        disc.zero_grad()\n",
    "        lossD.backward(retain_graph=True)\n",
    "        opt_disc.step()\n",
    "\n",
    "        ### Train Generator: min log(1 - D(G(z))) <-> max log(D(G(z))\n",
    "        output = disc(fake).view(-1)\n",
    "        lossG = criterion(output, torch.ones_like(output))\n",
    "        gen.zero_grad()\n",
    "        lossG.backward()\n",
    "        opt_gen.step()\n",
    "\n",
    "        if batch_idx == 0:\n",
    "            print(\n",
    "                f\"Epoch [{epoch}/{num_epochs}] Batch {batch_idx}/{len(loader)} \\\n",
    "                      Loss D: {lossD:.4f}, loss G: {lossG:.4f}\"\n",
    "            )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4591df6-3976-4c17-972d-634a71e48eb1",
   "metadata": {},
   "source": [
    "## Tokenizing Ancient Greek Texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3ba24be9-67c2-4d0f-b08a-0c07d02724df",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertModel were not initialized from the model checkpoint at Jacobo/aristoBERTo and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "# Initialize the tokenizer and model\n",
    "aristoberto_tokenizer = AutoTokenizer.from_pretrained(\"Jacobo/aristoBERTo\")\n",
    "aristoberto_model = AutoModel.from_pretrained(\"Jacobo/aristoBERTo\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fa769728-bd9f-430f-917e-4984e2f3bc0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_text = \"\"\" οἳ μὲν γὰρ Δρακάνῳ σ᾽, οἳ δ᾽ Ἰκάρῳ ἠνεμοέσσῃ\n",
    "φάσ᾽, οἳ δ᾽ ἐν Νάξῳ, δῖον γένος, εἰραφιῶτα,\n",
    "οἳ δέ σ᾽ ἐπ᾽ Ἀλφειῷ ποταμῷ βαθυδινήεντι\n",
    "κυσαμένην Σεμέλην τεκέειν Διὶ τερπικεραύνῳ:\n",
    "5ἄλλοι δ᾽ ἐν Θήβῃσιν, ἄναξ, σε λέγουσι γενέσθαι,\n",
    "ψευδόμενοι: σὲ δ᾽ ἔτικτε πατὴρ ἀνδρῶν τε θεῶν τε\n",
    "πολλὸν ἀπ᾽ ἀνθρώπων, κρύπτων λευκώλενον Ἥρην.\n",
    "ἔστι δέ τις Νύση, ὕπατον ὄρος, ἀνθέον ὕλῃ,\n",
    "τηλοῦ Φοινίκης, σχεδὸν Αἰγύπτοιο ῥοάων,\n",
    "10... καί οἱ ἀναστήσουσιν ἀγάλματα πόλλ᾽ ἐνὶ νηοῖς.\n",
    "ὣς δὲ τὰ μὲν τρία, σοὶ πάντως τριετηρίσιν αἰεὶ\n",
    "ἄνθρωποι ῥέξουσι τεληέσσας ἑκατόμβας\"\"\"\n",
    "\n",
    "inputs = aristoberto_tokenizer(sample_text, return_tensors=\"pt\", padding=True, truncation=True, max_length = MAX_LENGTH)\n",
    "\n",
    "# Generate embeddings\n",
    "with torch.no_grad():\n",
    "    outputs = aristoberto_model(**inputs)\n",
    "\n",
    "# outputs.last_hidden_state will contain the token-level embeddings\n",
    "# For sentence-level embeddings, you can average the token embeddings\n",
    "sentence_embedding = outputs.last_hidden_state.mean(dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6c7008c6-3639-4dec-88c2-526a0636b60f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 768])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence_embedding.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5289e47b-f12c-4209-bb84-9d27e930382c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.1584e-01, -4.3876e-01,  7.6951e-02,  2.2028e-01, -7.1418e-01,\n",
       "          3.3325e-01,  1.9584e-01, -1.3508e-02,  1.1990e-01, -4.0512e-01,\n",
       "         -4.2947e-01, -3.4709e-01, -4.0955e-02,  4.4730e-02, -8.5780e-02,\n",
       "          2.2162e-01,  2.5557e-01, -6.5016e-01,  2.3086e-02,  3.0195e-01,\n",
       "          4.6583e-01, -4.7642e-01, -1.0109e-01,  1.0951e-01, -2.7022e-01,\n",
       "          3.3001e-01,  2.7636e-01, -1.5663e-01,  5.1859e-01, -2.0124e-01,\n",
       "          5.7697e-01,  1.5916e-01,  4.5630e-01, -7.6024e-02,  2.4529e-01,\n",
       "          4.8486e-01, -2.8936e-01,  3.5514e-01, -1.5313e-01,  2.5443e-01,\n",
       "         -1.4930e-01, -3.8183e-01,  8.2846e-02,  2.7116e-01,  1.7931e-01,\n",
       "          5.4954e-02, -1.2382e-01, -6.5090e-02, -3.3930e-01,  4.2462e-01,\n",
       "          8.6823e-02, -2.3818e-01,  3.4314e-01,  1.3457e-02, -9.2304e-02,\n",
       "          4.3891e-03, -1.6361e-01, -2.4262e-01,  3.0555e-01, -2.3122e-02,\n",
       "          1.2961e-02, -8.5956e-02,  3.4778e-01,  8.8530e-02,  4.4103e-02,\n",
       "         -2.1675e-01,  1.0899e-01, -2.2948e-02,  2.2065e-01,  3.7016e-02,\n",
       "         -7.6018e-03,  4.1202e-01, -9.3199e-03, -8.7688e-03, -9.1750e-03,\n",
       "          3.0694e-02,  2.5432e-01, -3.2224e-01, -5.1190e-01,  1.5572e-02,\n",
       "          3.0204e-01,  4.1583e-01, -3.8694e-01, -8.0263e-02,  1.9239e-02,\n",
       "          1.4683e-01,  3.8406e-01, -1.1117e-01,  2.1034e-01, -1.1074e-01,\n",
       "          5.8680e-01, -2.8805e-01, -1.0239e-01, -6.5436e-02,  3.7142e-02,\n",
       "          1.2502e-01, -8.5935e-01,  2.4403e-01, -1.7759e-01, -4.2981e-01,\n",
       "          1.7103e-01, -4.2033e-01,  5.1906e-01, -4.1951e-02,  8.8129e-01,\n",
       "         -6.7244e-01,  5.0435e-01,  4.2018e-01,  3.1236e-01, -4.6013e-01,\n",
       "         -6.1850e-02, -4.2478e-01,  9.9250e-01, -4.6255e-01,  3.4705e-01,\n",
       "          1.5826e-02,  6.0662e-01,  3.2921e-01, -5.6387e-01,  4.1942e-01,\n",
       "         -5.0039e-02, -1.6111e-01, -2.3616e-01,  3.2996e-01, -1.3857e-01,\n",
       "         -4.0860e-01, -1.5478e-01,  2.7637e-01, -7.6997e-01, -3.9270e-01,\n",
       "         -4.0475e-01, -5.6747e-01,  2.7112e-01, -2.6757e-01,  1.7712e-01,\n",
       "         -1.2399e-01, -3.3021e-02,  3.0162e-01,  2.5040e-01,  2.2265e-01,\n",
       "         -5.1580e-01,  5.7450e-01, -6.5564e-01,  1.3377e-01,  2.2599e-01,\n",
       "          1.5281e-01,  2.6436e-02, -1.5880e-01, -3.0071e-01,  7.6993e-01,\n",
       "         -4.0526e-01,  2.5065e-01, -1.4918e-02, -2.0591e-02, -9.9679e-03,\n",
       "          2.2546e-01, -2.3387e-01,  2.9744e-01, -4.9706e-01, -1.2936e+01,\n",
       "         -3.1593e-01, -4.1585e-05,  6.2540e-03, -2.9142e-01,  5.8256e-02,\n",
       "          3.0088e-01, -6.2957e-01, -4.2148e-01, -1.6146e-01, -6.6636e-01,\n",
       "         -9.4809e-02,  4.4767e-01, -4.7451e-01,  2.3314e-01,  2.1867e-01,\n",
       "         -2.8548e-01,  1.0451e+00, -5.1965e-01, -2.0834e-01,  6.8695e-01,\n",
       "          6.3661e-01, -6.1237e-01,  2.1960e-01, -2.1655e-01, -2.7282e-02,\n",
       "         -1.6971e-01,  1.7884e-02, -1.7899e-01,  6.8862e-01,  3.7020e-01,\n",
       "          1.6807e-01, -1.1089e-01, -7.1335e-01,  1.4290e+00,  1.0851e-01,\n",
       "         -1.6414e-01,  1.4188e-02, -2.0843e-01, -6.7663e-02,  1.5035e-01,\n",
       "          6.9755e-02, -1.7552e-01, -7.6595e-01,  3.2656e-01,  3.0763e-01,\n",
       "         -3.6412e-01, -3.2170e-01, -5.3241e-02,  1.1585e-01, -1.1245e-01,\n",
       "          5.2926e-02,  4.8443e-02, -1.6731e-01, -3.6602e-01, -1.4461e-01,\n",
       "          3.4124e-01,  2.0526e-01,  8.0548e-02,  4.1930e-01,  3.9833e-01,\n",
       "         -8.4741e-01, -3.9663e-01, -3.1829e-01,  1.2834e-01,  1.5786e-01,\n",
       "         -5.2231e-01,  3.3620e-01,  5.8395e-03, -2.1152e-01,  5.9187e-01,\n",
       "          1.0444e-01,  2.8979e-01, -5.6673e-01,  1.5857e-01,  5.5354e-01,\n",
       "          5.6804e-01, -6.0943e-01,  5.4004e-02,  6.1898e-01, -1.5308e-01,\n",
       "         -2.0953e-01,  5.0338e-01, -7.7254e-01, -6.6176e-01,  5.3831e-01,\n",
       "          3.7285e-01,  2.9517e-01, -1.2788e-01,  1.5187e-01,  1.9741e-01,\n",
       "          4.9143e-02,  2.6394e-01, -3.4939e-01, -5.1865e-02,  1.7701e-01,\n",
       "         -2.1374e-01, -2.4322e-02,  2.9360e-01,  1.3442e-01,  3.9279e-01,\n",
       "          2.1563e-01,  3.3383e-01,  7.4497e-01, -1.1664e-01, -2.6344e-01,\n",
       "         -1.0045e-01, -3.4187e-01,  8.2321e-02, -5.0118e-01, -1.5583e-01,\n",
       "         -4.3101e-01,  2.6092e-01, -3.5153e-02,  3.1724e-01,  6.4066e-01,\n",
       "         -5.2979e-02, -3.8738e-01, -6.0883e-02,  3.5193e-01, -2.3835e-01,\n",
       "         -1.5178e-01,  7.3011e-02, -4.5733e-01,  1.0143e-02,  1.4086e-01,\n",
       "         -5.7280e-02,  1.4317e-01, -6.1298e-01, -7.1691e-02,  1.3361e-01,\n",
       "          2.0886e-01, -1.2931e-01, -1.4225e-01, -3.8916e-01, -2.9183e-01,\n",
       "         -1.3975e-01, -2.0664e-01, -3.8408e-01, -6.7798e-01, -1.8107e-01,\n",
       "         -4.5214e-01, -6.4962e-02,  3.6218e-01, -4.2965e-01,  5.6385e-02,\n",
       "         -2.0762e-01, -6.3237e-01, -6.6917e-01,  8.6850e-01, -1.0761e-01,\n",
       "         -2.2064e-01,  2.4686e-01, -1.0342e-01, -1.4211e-01,  9.2078e-02,\n",
       "         -4.3183e-01, -2.4833e-01, -7.8091e-02,  2.7994e-01, -4.9792e-01,\n",
       "          5.5050e-02, -2.2555e-01, -2.6266e-01, -3.6017e-01, -4.0609e-01,\n",
       "         -2.9018e-01,  5.1543e-02,  4.9507e-03,  4.0979e-01, -1.4402e-01,\n",
       "         -4.7325e-01, -4.0096e-01, -1.4242e-01, -1.2198e-01, -5.1178e-01,\n",
       "         -6.1905e-01,  4.7028e-01,  3.0923e-01,  3.5426e-01,  3.1850e-01,\n",
       "         -3.0402e-01,  5.3728e-02, -2.0021e-01,  2.5308e-01, -3.6918e-01,\n",
       "         -6.1027e-01,  3.2993e-01, -1.1618e+00,  8.8900e-02,  5.7226e-01,\n",
       "         -2.9255e-02,  3.6753e-02, -2.3220e-01, -3.4754e-01,  3.1953e-01,\n",
       "          3.5256e-01,  2.6358e-01, -5.1269e-01, -3.3045e-02, -3.2508e-01,\n",
       "          1.1523e-01,  2.4546e-01, -6.2662e-01, -9.0700e-02,  2.7330e-01,\n",
       "          3.6779e-01, -2.1692e-01, -1.9779e-01,  4.5310e-01,  3.7266e-01,\n",
       "          1.0315e-01,  4.5774e-01, -1.3523e-02,  8.9746e-02,  4.5546e-01,\n",
       "          9.6954e-02, -3.8115e-01, -6.2432e-01, -6.4560e-01,  2.9781e-01,\n",
       "         -1.5385e-01,  5.4780e-01,  9.1750e-02, -1.7807e-01, -6.1667e-02,\n",
       "          8.2941e-01,  4.6154e-01,  9.4384e-02, -4.0700e-01, -6.1705e-01,\n",
       "         -1.4601e-01, -2.9062e-01, -6.0382e-01,  3.3736e-01,  7.4268e-01,\n",
       "          2.8996e-01, -4.6311e-01, -2.9632e-01,  2.8511e-01, -5.6240e-01,\n",
       "         -6.3173e-01,  2.1490e-01, -9.7864e-02,  6.7577e-01, -2.9528e-01,\n",
       "          1.1628e-01,  8.1474e-01,  3.3038e-02,  5.0275e-02, -1.3386e-01,\n",
       "          4.0868e-01, -6.4511e-01, -3.1668e-01,  3.0830e-01,  6.9127e-01,\n",
       "         -3.7529e-01,  1.2547e-01,  1.9111e-01,  2.3063e-01, -3.4764e-01,\n",
       "         -4.1659e-01, -3.5010e-01, -1.8017e-01, -1.6976e-02,  7.7540e-01,\n",
       "         -3.1661e-02,  6.3084e-01,  7.5423e-01,  2.5480e-01,  4.6932e-01,\n",
       "          4.0665e-01,  1.8530e-01,  4.1364e-02, -5.2884e-02,  1.6594e-01,\n",
       "         -1.8996e-01,  6.4211e-01, -7.0756e-01, -1.2934e-01, -6.9503e-02,\n",
       "         -1.4587e-01,  3.0325e-01,  2.0499e-01,  1.7703e-01, -1.4207e-01,\n",
       "         -6.8982e-01,  2.5539e-01,  2.8956e-01,  7.1309e-02,  3.0528e-01,\n",
       "         -3.5829e-02,  3.6401e-01,  9.2273e-01,  2.0173e-01,  7.8804e-01,\n",
       "          5.5653e-01, -2.3248e-01, -2.4243e-01, -5.3717e-02, -1.4418e+00,\n",
       "          7.3847e-01, -2.6189e-02, -4.2162e-01, -2.8655e-01,  8.0732e-01,\n",
       "          1.9147e-01,  1.5735e-01, -4.2216e-01, -2.0199e-01, -3.2903e-02,\n",
       "         -4.0769e-01, -3.7834e-01, -4.5914e-01,  3.0411e-01,  4.8595e-01,\n",
       "         -1.9343e-01, -3.0761e-01,  4.6671e-02, -3.2332e-01, -2.7683e-01,\n",
       "          1.1835e-02, -3.0653e-01, -8.5134e-02, -8.2021e-02, -6.1112e-02,\n",
       "         -4.9495e-01, -1.8596e-01,  2.6764e-02, -3.4990e-01,  9.9689e-02,\n",
       "          3.7077e-01,  2.3255e-01, -1.7435e-01, -9.2536e-02, -4.0074e-02,\n",
       "         -4.1200e-01, -3.1705e-01,  9.2175e-02,  3.3511e-01, -4.6215e-01,\n",
       "         -2.6831e-01, -9.5432e-02, -2.7277e-01, -5.0895e-01,  6.9898e-01,\n",
       "          6.4971e-01,  1.0143e+00,  5.1726e-01,  6.7369e-02, -3.8752e-01,\n",
       "         -3.9490e-01, -2.3649e-01, -2.4099e-01,  5.1270e-01,  1.5776e-01,\n",
       "          4.8538e-01,  3.8865e-01,  1.6148e-01, -1.1558e-01,  1.5274e-01,\n",
       "         -4.2597e-01, -3.8357e-01,  1.9350e-01, -4.8454e-01, -1.6737e-01,\n",
       "         -3.3527e-03,  5.3712e-02,  5.5047e-01,  1.2942e-01,  1.8501e-01,\n",
       "          4.1144e-01,  6.4646e-02,  1.6556e-01,  7.3738e-01,  2.4422e-01,\n",
       "         -1.3103e-02,  2.6056e-02, -4.3891e-01,  5.1394e-01,  7.7700e-03,\n",
       "         -1.1984e-01, -6.9904e-02,  3.8614e-01,  3.3308e-01, -5.1474e-02,\n",
       "         -2.7849e-02,  3.5198e-01,  3.9690e-02,  2.8324e-02, -6.1277e-01,\n",
       "          3.0609e-02,  1.3883e-01,  6.0484e-02, -2.0539e-01, -2.9968e-01,\n",
       "          5.6958e-02,  4.9966e-01,  2.4857e-01, -1.4983e+00, -6.0031e-02,\n",
       "          3.4710e-01,  4.2790e-01, -3.6331e-01, -1.1548e-01, -1.5294e-01,\n",
       "          1.0512e-01, -7.2574e-02,  1.4700e-01, -3.5053e-01,  2.4629e-01,\n",
       "          4.3623e-01,  6.1110e-01, -1.5800e-01, -1.0211e-01,  2.9726e-01,\n",
       "          4.0447e-03,  2.9960e-01,  4.6479e-01, -3.1933e-01, -4.2637e-01,\n",
       "          4.7289e-01, -5.0610e-01, -3.6666e-01, -2.6804e-02, -1.5145e-01,\n",
       "         -2.1766e-01, -5.2833e-02,  1.3199e-01,  6.9647e-02,  3.3285e-01,\n",
       "         -4.5671e-01, -8.0634e-01, -2.7336e-01, -6.7966e-02, -2.7704e-01,\n",
       "          3.4565e-01, -9.3812e-01,  3.4688e-01,  5.5106e-02,  3.1877e-01,\n",
       "         -1.1564e-01,  1.3983e-01, -5.6780e-01, -9.6469e-02,  3.3968e-01,\n",
       "          6.9364e-02, -2.8306e-01,  2.0313e-01,  7.5956e-01,  1.6435e-01,\n",
       "          2.9929e-01, -5.8537e-02,  2.7221e-01,  9.7131e-01, -2.8572e-01,\n",
       "          5.7567e-01,  2.8044e-01,  2.0862e-02, -1.4520e-02, -3.0248e-01,\n",
       "          1.1156e-01,  3.1099e-01,  5.5571e-01,  3.2181e-01,  4.9525e-01,\n",
       "          5.9372e-03,  4.2247e-02, -6.4482e-02, -2.2847e-01,  3.7182e-01,\n",
       "         -2.3949e-01, -3.9590e-02, -1.2879e-01,  1.2791e-01,  2.2348e-01,\n",
       "         -4.1955e-01,  2.4633e-01,  1.6208e-01, -1.2993e-01,  3.6214e-01,\n",
       "         -4.9390e-01,  1.7487e-01, -2.9239e-01,  1.2557e-01,  6.8066e-01,\n",
       "         -1.2616e-01,  4.4303e-01, -4.7694e-02,  1.3869e-01, -3.0635e-01,\n",
       "          5.6300e-01, -4.4337e-02,  5.7766e-02,  4.6555e-01,  2.8881e-01,\n",
       "          1.2299e+00,  4.5377e-01,  7.3589e-01,  4.4983e-02, -7.9878e-02,\n",
       "          5.3777e-02,  9.0426e-02,  1.2116e-01,  3.1691e-01, -3.2307e-01,\n",
       "         -3.8125e-01,  6.5228e-02,  1.8058e-01, -3.2641e-02,  2.5298e-01,\n",
       "         -3.5801e-01, -4.6998e-01,  7.2204e-01,  3.5401e-01,  9.0392e-02,\n",
       "          8.7330e-02, -3.0844e-01,  6.3049e-01,  1.2592e-01, -1.5738e-01,\n",
       "         -6.4292e-02,  3.6408e-01, -1.0505e-01,  5.1820e-01,  1.8411e-01,\n",
       "          2.3428e-01, -7.2458e-02, -2.8993e-01, -4.3100e-01,  5.7175e-01,\n",
       "          1.4881e-01, -2.1735e-01,  5.9980e-01, -5.7452e-01, -1.4447e-01,\n",
       "          5.3349e-01,  7.1141e-01, -2.9471e-01, -5.9746e-02, -5.3533e-01,\n",
       "         -3.5083e-01,  6.2551e-03,  3.5661e-01, -4.9305e-01, -3.4245e-01,\n",
       "         -1.0199e-01,  7.5127e-02,  5.2953e-01, -8.0445e-01, -3.4510e-01,\n",
       "         -4.6074e-03, -3.1347e-01, -4.7173e-01,  9.2312e-01, -7.2767e-01,\n",
       "          1.2427e-01,  1.8337e-01, -1.0984e-01,  1.4044e-01,  8.7221e-02,\n",
       "          3.9422e-01, -1.5537e-01,  9.8949e-03,  7.0803e-02,  2.7765e-01,\n",
       "         -1.7080e-01, -5.8525e-01, -4.3646e-01, -4.2071e-02, -5.0507e-01,\n",
       "          4.0704e-01, -2.4268e-01,  4.5605e-01, -1.2686e-01, -3.4000e-01,\n",
       "         -4.7791e-01, -1.5816e-01, -9.6192e-03, -3.2563e-02, -4.4732e-02,\n",
       "         -8.0465e-01,  3.7461e-01, -4.7697e-01,  2.6267e-02, -1.1183e-03,\n",
       "          1.5033e-01, -1.8458e-01, -5.0353e-01,  9.2344e-02, -1.7179e-01,\n",
       "          1.4152e-01,  4.7554e-02,  1.6844e-01,  1.4498e-02,  5.1739e-02,\n",
       "         -1.3626e-01,  7.7271e-02,  4.5335e-01,  6.2554e-01, -2.2301e-01,\n",
       "         -7.0691e-02,  3.0207e-01, -1.4265e-01, -4.0672e-01, -4.4315e-02,\n",
       "         -1.6107e-01,  1.4463e-01,  5.0508e-01]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence_embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eca3d371-d6f9-4728-a0f4-b2a9c099986c",
   "metadata": {},
   "source": [
    "# Using Ancient-Greek BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a1570f60-714f-4b0a-bee9-672e402f0364",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13 ['[CLS]', 'του', 'βιου', 'του', 'καθ', '΄', 'εαυτους', 'πολλα', 'γινε', '##σθαι', 'συγχ', '##ωρου', '##ν', '[MASK]', '[SEP]']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 15, 768])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokeniser = AutoTokenizer.from_pretrained(\"pranaydeeps/Ancient-Greek-BERT\")\n",
    "model = AutoModel.from_pretrained(\"pranaydeeps/Ancient-Greek-BERT\")\n",
    "\n",
    "input_ids = tokeniser.encode('τοῦ βίου τοῦ καθ ΄ εαυτοὺς πολλὰ γίνεσθαι συγχωροῦν [MASK]')\n",
    "tokens = tokeniser.convert_ids_to_tokens(input_ids)\n",
    "idx = tokens.index(\"[MASK]\")\n",
    "print(idx, tokens)\n",
    "outputs = model(torch.tensor([input_ids]))[0]\n",
    "outputs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "531c8bf2-b0e8-49b8-ac82-b304d58c1af2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
