{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b26b6e1b-a96d-4b8f-b69c-97caad5edb75",
   "metadata": {},
   "source": [
    "# Training a Simple GAN Model for Sentence Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9f13714e-a0d7-40f6-adce-bbdc125592ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [0/50] Batch 0/12               Loss D: 0.6893, loss G: 0.7260\n",
      "Epoch [0/50] Batch 1/12               Loss D: 0.6963, loss G: 0.7059\n",
      "Epoch [0/50] Batch 2/12               Loss D: 0.7004, loss G: 0.6890\n",
      "Epoch [0/50] Batch 3/12               Loss D: 0.7189, loss G: 0.6769\n",
      "Epoch [0/50] Batch 4/12               Loss D: 0.7106, loss G: 0.6648\n",
      "Epoch [0/50] Batch 5/12               Loss D: 0.7209, loss G: 0.6525\n",
      "Epoch [0/50] Batch 6/12               Loss D: 0.7221, loss G: 0.6407\n",
      "Epoch [0/50] Batch 7/12               Loss D: 0.7228, loss G: 0.6316\n",
      "Epoch [0/50] Batch 8/12               Loss D: 0.7365, loss G: 0.6242\n",
      "Epoch [0/50] Batch 9/12               Loss D: 0.7231, loss G: 0.6196\n",
      "Epoch [0/50] Batch 10/12               Loss D: 0.7342, loss G: 0.6086\n",
      "Epoch [0/50] Batch 11/12               Loss D: 0.7344, loss G: 0.6022\n",
      "Epoch [1/50] Batch 0/12               Loss D: 0.7154, loss G: 0.6035\n",
      "Epoch [1/50] Batch 1/12               Loss D: 0.7237, loss G: 0.5947\n",
      "Epoch [1/50] Batch 2/12               Loss D: 0.7254, loss G: 0.6013\n",
      "Epoch [1/50] Batch 3/12               Loss D: 0.7260, loss G: 0.5918\n",
      "Epoch [1/50] Batch 4/12               Loss D: 0.7154, loss G: 0.6028\n",
      "Epoch [1/50] Batch 5/12               Loss D: 0.7082, loss G: 0.6044\n",
      "Epoch [1/50] Batch 6/12               Loss D: 0.7012, loss G: 0.6105\n",
      "Epoch [1/50] Batch 7/12               Loss D: 0.7087, loss G: 0.6259\n",
      "Epoch [1/50] Batch 8/12               Loss D: 0.6993, loss G: 0.6331\n",
      "Epoch [1/50] Batch 9/12               Loss D: 0.6947, loss G: 0.6480\n",
      "Epoch [1/50] Batch 10/12               Loss D: 0.6754, loss G: 0.6657\n",
      "Epoch [1/50] Batch 11/12               Loss D: 0.6683, loss G: 0.6818\n",
      "Epoch [2/50] Batch 0/12               Loss D: 0.6513, loss G: 0.7091\n",
      "Epoch [2/50] Batch 1/12               Loss D: 0.6454, loss G: 0.7248\n",
      "Epoch [2/50] Batch 2/12               Loss D: 0.6311, loss G: 0.7495\n",
      "Epoch [2/50] Batch 3/12               Loss D: 0.6079, loss G: 0.7759\n",
      "Epoch [2/50] Batch 4/12               Loss D: 0.6032, loss G: 0.7912\n",
      "Epoch [2/50] Batch 5/12               Loss D: 0.5884, loss G: 0.8246\n",
      "Epoch [2/50] Batch 6/12               Loss D: 0.5840, loss G: 0.8433\n",
      "Epoch [2/50] Batch 7/12               Loss D: 0.5800, loss G: 0.8732\n",
      "Epoch [2/50] Batch 8/12               Loss D: 0.5722, loss G: 0.8923\n",
      "Epoch [2/50] Batch 9/12               Loss D: 0.5594, loss G: 0.9115\n",
      "Epoch [2/50] Batch 10/12               Loss D: 0.5628, loss G: 0.9195\n",
      "Epoch [2/50] Batch 11/12               Loss D: 0.5502, loss G: 0.9291\n",
      "Epoch [3/50] Batch 0/12               Loss D: 0.5444, loss G: 0.9469\n",
      "Epoch [3/50] Batch 1/12               Loss D: 0.5424, loss G: 0.9531\n",
      "Epoch [3/50] Batch 2/12               Loss D: 0.5493, loss G: 0.9364\n",
      "Epoch [3/50] Batch 3/12               Loss D: 0.5400, loss G: 0.9384\n",
      "Epoch [3/50] Batch 4/12               Loss D: 0.5615, loss G: 0.9352\n",
      "Epoch [3/50] Batch 5/12               Loss D: 0.5463, loss G: 0.9490\n",
      "Epoch [3/50] Batch 6/12               Loss D: 0.5520, loss G: 0.9305\n",
      "Epoch [3/50] Batch 7/12               Loss D: 0.5538, loss G: 0.9155\n",
      "Epoch [3/50] Batch 8/12               Loss D: 0.5574, loss G: 0.8965\n",
      "Epoch [3/50] Batch 9/12               Loss D: 0.5614, loss G: 0.9092\n",
      "Epoch [3/50] Batch 10/12               Loss D: 0.5769, loss G: 0.8838\n",
      "Epoch [3/50] Batch 11/12               Loss D: 0.5927, loss G: 0.8614\n",
      "Epoch [4/50] Batch 0/12               Loss D: 0.5959, loss G: 0.8576\n",
      "Epoch [4/50] Batch 1/12               Loss D: 0.5797, loss G: 0.8563\n",
      "Epoch [4/50] Batch 2/12               Loss D: 0.5974, loss G: 0.8367\n",
      "Epoch [4/50] Batch 3/12               Loss D: 0.6021, loss G: 0.8355\n",
      "Epoch [4/50] Batch 4/12               Loss D: 0.6052, loss G: 0.8336\n",
      "Epoch [4/50] Batch 5/12               Loss D: 0.5755, loss G: 0.8410\n",
      "Epoch [4/50] Batch 6/12               Loss D: 0.6037, loss G: 0.8346\n",
      "Epoch [4/50] Batch 7/12               Loss D: 0.5856, loss G: 0.8477\n",
      "Epoch [4/50] Batch 8/12               Loss D: 0.5712, loss G: 0.8709\n",
      "Epoch [4/50] Batch 9/12               Loss D: 0.5926, loss G: 0.8780\n",
      "Epoch [4/50] Batch 10/12               Loss D: 0.5468, loss G: 0.9054\n",
      "Epoch [4/50] Batch 11/12               Loss D: 0.5648, loss G: 0.9278\n",
      "Epoch [5/50] Batch 0/12               Loss D: 0.5426, loss G: 0.9501\n",
      "Epoch [5/50] Batch 1/12               Loss D: 0.5286, loss G: 0.9858\n",
      "Epoch [5/50] Batch 2/12               Loss D: 0.5394, loss G: 0.9982\n",
      "Epoch [5/50] Batch 3/12               Loss D: 0.5137, loss G: 1.0269\n",
      "Epoch [5/50] Batch 4/12               Loss D: 0.5053, loss G: 1.0593\n",
      "Epoch [5/50] Batch 5/12               Loss D: 0.4925, loss G: 1.0771\n",
      "Epoch [5/50] Batch 6/12               Loss D: 0.4622, loss G: 1.1150\n",
      "Epoch [5/50] Batch 7/12               Loss D: 0.4430, loss G: 1.1509\n",
      "Epoch [5/50] Batch 8/12               Loss D: 0.4778, loss G: 1.1778\n",
      "Epoch [5/50] Batch 9/12               Loss D: 0.4629, loss G: 1.1974\n",
      "Epoch [5/50] Batch 10/12               Loss D: 0.4695, loss G: 1.2223\n",
      "Epoch [5/50] Batch 11/12               Loss D: 0.4458, loss G: 1.2617\n",
      "Epoch [6/50] Batch 0/12               Loss D: 0.4400, loss G: 1.2769\n",
      "Epoch [6/50] Batch 1/12               Loss D: 0.4218, loss G: 1.3056\n",
      "Epoch [6/50] Batch 2/12               Loss D: 0.4268, loss G: 1.3372\n",
      "Epoch [6/50] Batch 3/12               Loss D: 0.4333, loss G: 1.3328\n",
      "Epoch [6/50] Batch 4/12               Loss D: 0.4283, loss G: 1.3298\n",
      "Epoch [6/50] Batch 5/12               Loss D: 0.4315, loss G: 1.3289\n",
      "Epoch [6/50] Batch 6/12               Loss D: 0.4289, loss G: 1.3464\n",
      "Epoch [6/50] Batch 7/12               Loss D: 0.4211, loss G: 1.3506\n",
      "Epoch [6/50] Batch 8/12               Loss D: 0.4020, loss G: 1.3057\n",
      "Epoch [6/50] Batch 9/12               Loss D: 0.4226, loss G: 1.3024\n",
      "Epoch [6/50] Batch 10/12               Loss D: 0.4466, loss G: 1.3071\n",
      "Epoch [6/50] Batch 11/12               Loss D: 0.4227, loss G: 1.2788\n",
      "Epoch [7/50] Batch 0/12               Loss D: 0.4204, loss G: 1.2702\n",
      "Epoch [7/50] Batch 1/12               Loss D: 0.4530, loss G: 1.2388\n",
      "Epoch [7/50] Batch 2/12               Loss D: 0.4555, loss G: 1.2288\n",
      "Epoch [7/50] Batch 3/12               Loss D: 0.4546, loss G: 1.2085\n",
      "Epoch [7/50] Batch 4/12               Loss D: 0.4464, loss G: 1.2001\n",
      "Epoch [7/50] Batch 5/12               Loss D: 0.4505, loss G: 1.1607\n",
      "Epoch [7/50] Batch 6/12               Loss D: 0.4686, loss G: 1.1385\n",
      "Epoch [7/50] Batch 7/12               Loss D: 0.4530, loss G: 1.1519\n",
      "Epoch [7/50] Batch 8/12               Loss D: 0.4853, loss G: 1.0853\n",
      "Epoch [7/50] Batch 9/12               Loss D: 0.5050, loss G: 1.0664\n",
      "Epoch [7/50] Batch 10/12               Loss D: 0.5046, loss G: 1.0533\n",
      "Epoch [7/50] Batch 11/12               Loss D: 0.4969, loss G: 1.0504\n",
      "Epoch [8/50] Batch 0/12               Loss D: 0.5003, loss G: 1.0519\n",
      "Epoch [8/50] Batch 1/12               Loss D: 0.5015, loss G: 1.0230\n",
      "Epoch [8/50] Batch 2/12               Loss D: 0.4999, loss G: 1.0250\n",
      "Epoch [8/50] Batch 3/12               Loss D: 0.4966, loss G: 1.0013\n",
      "Epoch [8/50] Batch 4/12               Loss D: 0.4910, loss G: 1.0155\n",
      "Epoch [8/50] Batch 5/12               Loss D: 0.4949, loss G: 0.9859\n",
      "Epoch [8/50] Batch 6/12               Loss D: 0.5173, loss G: 0.9938\n",
      "Epoch [8/50] Batch 7/12               Loss D: 0.5163, loss G: 1.0051\n",
      "Epoch [8/50] Batch 8/12               Loss D: 0.5022, loss G: 0.9908\n",
      "Epoch [8/50] Batch 9/12               Loss D: 0.4872, loss G: 0.9889\n",
      "Epoch [8/50] Batch 10/12               Loss D: 0.4800, loss G: 0.9995\n",
      "Epoch [8/50] Batch 11/12               Loss D: 0.4793, loss G: 1.0260\n",
      "Epoch [9/50] Batch 0/12               Loss D: 0.4490, loss G: 1.0403\n",
      "Epoch [9/50] Batch 1/12               Loss D: 0.4494, loss G: 1.0527\n",
      "Epoch [9/50] Batch 2/12               Loss D: 0.4374, loss G: 1.0757\n",
      "Epoch [9/50] Batch 3/12               Loss D: 0.4504, loss G: 1.0844\n",
      "Epoch [9/50] Batch 4/12               Loss D: 0.4737, loss G: 1.1072\n",
      "Epoch [9/50] Batch 5/12               Loss D: 0.4460, loss G: 1.1207\n",
      "Epoch [9/50] Batch 6/12               Loss D: 0.4410, loss G: 1.1305\n",
      "Epoch [9/50] Batch 7/12               Loss D: 0.4291, loss G: 1.1506\n",
      "Epoch [9/50] Batch 8/12               Loss D: 0.4285, loss G: 1.1591\n",
      "Epoch [9/50] Batch 9/12               Loss D: 0.4128, loss G: 1.1808\n",
      "Epoch [9/50] Batch 10/12               Loss D: 0.4199, loss G: 1.2007\n",
      "Epoch [9/50] Batch 11/12               Loss D: 0.4199, loss G: 1.2028\n",
      "Epoch [10/50] Batch 0/12               Loss D: 0.4135, loss G: 1.2118\n",
      "Epoch [10/50] Batch 1/12               Loss D: 0.4202, loss G: 1.2200\n",
      "Epoch [10/50] Batch 2/12               Loss D: 0.3916, loss G: 1.2424\n",
      "Epoch [10/50] Batch 3/12               Loss D: 0.3930, loss G: 1.2406\n",
      "Epoch [10/50] Batch 4/12               Loss D: 0.3934, loss G: 1.2475\n",
      "Epoch [10/50] Batch 5/12               Loss D: 0.3864, loss G: 1.2715\n",
      "Epoch [10/50] Batch 6/12               Loss D: 0.3908, loss G: 1.2792\n",
      "Epoch [10/50] Batch 7/12               Loss D: 0.3698, loss G: 1.2936\n",
      "Epoch [10/50] Batch 8/12               Loss D: 0.3815, loss G: 1.3173\n",
      "Epoch [10/50] Batch 9/12               Loss D: 0.3522, loss G: 1.3566\n",
      "Epoch [10/50] Batch 10/12               Loss D: 0.3652, loss G: 1.3541\n",
      "Epoch [10/50] Batch 11/12               Loss D: 0.3451, loss G: 1.3871\n",
      "Epoch [11/50] Batch 0/12               Loss D: 0.3367, loss G: 1.4291\n",
      "Epoch [11/50] Batch 1/12               Loss D: 0.3209, loss G: 1.4837\n",
      "Epoch [11/50] Batch 2/12               Loss D: 0.3170, loss G: 1.5151\n",
      "Epoch [11/50] Batch 3/12               Loss D: 0.3356, loss G: 1.5420\n",
      "Epoch [11/50] Batch 4/12               Loss D: 0.3435, loss G: 1.5728\n",
      "Epoch [11/50] Batch 5/12               Loss D: 0.3016, loss G: 1.6372\n",
      "Epoch [11/50] Batch 6/12               Loss D: 0.3009, loss G: 1.6754\n",
      "Epoch [11/50] Batch 7/12               Loss D: 0.3101, loss G: 1.6747\n",
      "Epoch [11/50] Batch 8/12               Loss D: 0.3033, loss G: 1.7269\n",
      "Epoch [11/50] Batch 9/12               Loss D: 0.2793, loss G: 1.7505\n",
      "Epoch [11/50] Batch 10/12               Loss D: 0.2894, loss G: 1.7275\n",
      "Epoch [11/50] Batch 11/12               Loss D: 0.3042, loss G: 1.7573\n",
      "Epoch [12/50] Batch 0/12               Loss D: 0.3180, loss G: 1.7650\n",
      "Epoch [12/50] Batch 1/12               Loss D: 0.2796, loss G: 1.8084\n",
      "Epoch [12/50] Batch 2/12               Loss D: 0.2818, loss G: 1.8234\n",
      "Epoch [12/50] Batch 3/12               Loss D: 0.3049, loss G: 1.8202\n",
      "Epoch [12/50] Batch 4/12               Loss D: 0.2891, loss G: 1.7942\n",
      "Epoch [12/50] Batch 5/12               Loss D: 0.3142, loss G: 1.8019\n",
      "Epoch [12/50] Batch 6/12               Loss D: 0.3015, loss G: 1.7995\n",
      "Epoch [12/50] Batch 7/12               Loss D: 0.3030, loss G: 1.8124\n",
      "Epoch [12/50] Batch 8/12               Loss D: 0.2951, loss G: 1.8409\n",
      "Epoch [12/50] Batch 9/12               Loss D: 0.2626, loss G: 1.9058\n",
      "Epoch [12/50] Batch 10/12               Loss D: 0.2794, loss G: 1.9088\n",
      "Epoch [12/50] Batch 11/12               Loss D: 0.2854, loss G: 1.8889\n",
      "Epoch [13/50] Batch 0/12               Loss D: 0.3050, loss G: 1.8791\n",
      "Epoch [13/50] Batch 1/12               Loss D: 0.2905, loss G: 1.8815\n",
      "Epoch [13/50] Batch 2/12               Loss D: 0.2775, loss G: 1.9419\n",
      "Epoch [13/50] Batch 3/12               Loss D: 0.2649, loss G: 1.9646\n",
      "Epoch [13/50] Batch 4/12               Loss D: 0.2815, loss G: 1.9276\n",
      "Epoch [13/50] Batch 5/12               Loss D: 0.3014, loss G: 1.9531\n",
      "Epoch [13/50] Batch 6/12               Loss D: 0.2944, loss G: 1.9729\n",
      "Epoch [13/50] Batch 7/12               Loss D: 0.2920, loss G: 1.9256\n",
      "Epoch [13/50] Batch 8/12               Loss D: 0.3005, loss G: 1.9870\n",
      "Epoch [13/50] Batch 9/12               Loss D: 0.3345, loss G: 1.9854\n",
      "Epoch [13/50] Batch 10/12               Loss D: 0.2879, loss G: 1.9644\n",
      "Epoch [13/50] Batch 11/12               Loss D: 0.3516, loss G: 1.8669\n",
      "Epoch [14/50] Batch 0/12               Loss D: 0.3207, loss G: 1.9193\n",
      "Epoch [14/50] Batch 1/12               Loss D: 0.3622, loss G: 1.8501\n",
      "Epoch [14/50] Batch 2/12               Loss D: 0.3371, loss G: 1.8656\n",
      "Epoch [14/50] Batch 3/12               Loss D: 0.3111, loss G: 1.8955\n",
      "Epoch [14/50] Batch 4/12               Loss D: 0.3662, loss G: 1.8201\n",
      "Epoch [14/50] Batch 5/12               Loss D: 0.3214, loss G: 1.8477\n",
      "Epoch [14/50] Batch 6/12               Loss D: 0.3512, loss G: 1.8552\n",
      "Epoch [14/50] Batch 7/12               Loss D: 0.3903, loss G: 1.8369\n",
      "Epoch [14/50] Batch 8/12               Loss D: 0.3537, loss G: 1.8407\n",
      "Epoch [14/50] Batch 9/12               Loss D: 0.3515, loss G: 1.9105\n",
      "Epoch [14/50] Batch 10/12               Loss D: 0.3706, loss G: 1.9446\n",
      "Epoch [14/50] Batch 11/12               Loss D: 0.3762, loss G: 2.0438\n",
      "Epoch [15/50] Batch 0/12               Loss D: 0.3522, loss G: 2.0320\n",
      "Epoch [15/50] Batch 1/12               Loss D: 0.3369, loss G: 2.1120\n",
      "Epoch [15/50] Batch 2/12               Loss D: 0.3358, loss G: 2.1543\n",
      "Epoch [15/50] Batch 3/12               Loss D: 0.2941, loss G: 2.2030\n",
      "Epoch [15/50] Batch 4/12               Loss D: 0.3323, loss G: 2.2063\n",
      "Epoch [15/50] Batch 5/12               Loss D: 0.3159, loss G: 2.3549\n",
      "Epoch [15/50] Batch 6/12               Loss D: 0.3198, loss G: 2.2840\n",
      "Epoch [15/50] Batch 7/12               Loss D: 0.2907, loss G: 2.3875\n",
      "Epoch [15/50] Batch 8/12               Loss D: 0.3303, loss G: 2.4100\n",
      "Epoch [15/50] Batch 9/12               Loss D: 0.3259, loss G: 2.4252\n",
      "Epoch [15/50] Batch 10/12               Loss D: 0.2952, loss G: 2.5110\n",
      "Epoch [15/50] Batch 11/12               Loss D: 0.3403, loss G: 2.4543\n",
      "Epoch [16/50] Batch 0/12               Loss D: 0.3379, loss G: 2.4421\n",
      "Epoch [16/50] Batch 1/12               Loss D: 0.3298, loss G: 2.4197\n",
      "Epoch [16/50] Batch 2/12               Loss D: 0.3592, loss G: 2.3678\n",
      "Epoch [16/50] Batch 3/12               Loss D: 0.3380, loss G: 2.3479\n",
      "Epoch [16/50] Batch 4/12               Loss D: 0.3278, loss G: 2.4370\n",
      "Epoch [16/50] Batch 5/12               Loss D: 0.3270, loss G: 2.4223\n",
      "Epoch [16/50] Batch 6/12               Loss D: 0.3145, loss G: 2.5150\n",
      "Epoch [16/50] Batch 7/12               Loss D: 0.3139, loss G: 2.4686\n",
      "Epoch [16/50] Batch 8/12               Loss D: 0.3399, loss G: 2.6563\n",
      "Epoch [16/50] Batch 9/12               Loss D: 0.3359, loss G: 2.5673\n",
      "Epoch [16/50] Batch 10/12               Loss D: 0.3631, loss G: 2.6858\n",
      "Epoch [16/50] Batch 11/12               Loss D: 0.3339, loss G: 2.7003\n",
      "Epoch [17/50] Batch 0/12               Loss D: 0.3411, loss G: 2.7352\n",
      "Epoch [17/50] Batch 1/12               Loss D: 0.3302, loss G: 2.8039\n",
      "Epoch [17/50] Batch 2/12               Loss D: 0.3451, loss G: 2.7598\n",
      "Epoch [17/50] Batch 3/12               Loss D: 0.3016, loss G: 2.7797\n",
      "Epoch [17/50] Batch 4/12               Loss D: 0.3319, loss G: 2.8026\n",
      "Epoch [17/50] Batch 5/12               Loss D: 0.3222, loss G: 2.8700\n",
      "Epoch [17/50] Batch 6/12               Loss D: 0.3481, loss G: 2.7330\n",
      "Epoch [17/50] Batch 7/12               Loss D: 0.3070, loss G: 2.8947\n",
      "Epoch [17/50] Batch 8/12               Loss D: 0.3466, loss G: 2.8614\n",
      "Epoch [17/50] Batch 9/12               Loss D: 0.3255, loss G: 2.7987\n",
      "Epoch [17/50] Batch 10/12               Loss D: 0.3604, loss G: 2.8080\n",
      "Epoch [17/50] Batch 11/12               Loss D: 0.4012, loss G: 2.7950\n",
      "Epoch [18/50] Batch 0/12               Loss D: 0.3541, loss G: 2.6905\n",
      "Epoch [18/50] Batch 1/12               Loss D: 0.3140, loss G: 2.7747\n",
      "Epoch [18/50] Batch 2/12               Loss D: 0.3383, loss G: 2.9099\n",
      "Epoch [18/50] Batch 3/12               Loss D: 0.3536, loss G: 2.8043\n",
      "Epoch [18/50] Batch 4/12               Loss D: 0.3196, loss G: 2.9286\n",
      "Epoch [18/50] Batch 5/12               Loss D: 0.3287, loss G: 2.8521\n",
      "Epoch [18/50] Batch 6/12               Loss D: 0.3419, loss G: 3.0205\n",
      "Epoch [18/50] Batch 7/12               Loss D: 0.3760, loss G: 3.0092\n",
      "Epoch [18/50] Batch 8/12               Loss D: 0.3642, loss G: 3.0117\n",
      "Epoch [18/50] Batch 9/12               Loss D: 0.3612, loss G: 2.9753\n",
      "Epoch [18/50] Batch 10/12               Loss D: 0.3512, loss G: 3.0134\n",
      "Epoch [18/50] Batch 11/12               Loss D: 0.3726, loss G: 3.0179\n",
      "Epoch [19/50] Batch 0/12               Loss D: 0.3575, loss G: 2.9600\n",
      "Epoch [19/50] Batch 1/12               Loss D: 0.3658, loss G: 2.9086\n",
      "Epoch [19/50] Batch 2/12               Loss D: 0.4146, loss G: 2.8916\n",
      "Epoch [19/50] Batch 3/12               Loss D: 0.3612, loss G: 2.7538\n",
      "Epoch [19/50] Batch 4/12               Loss D: 0.3307, loss G: 2.7937\n",
      "Epoch [19/50] Batch 5/12               Loss D: 0.3167, loss G: 2.8269\n",
      "Epoch [19/50] Batch 6/12               Loss D: 0.3369, loss G: 2.7957\n",
      "Epoch [19/50] Batch 7/12               Loss D: 0.3779, loss G: 2.6867\n",
      "Epoch [19/50] Batch 8/12               Loss D: 0.3542, loss G: 2.8279\n",
      "Epoch [19/50] Batch 9/12               Loss D: 0.4095, loss G: 2.7881\n",
      "Epoch [19/50] Batch 10/12               Loss D: 0.3505, loss G: 2.8981\n",
      "Epoch [19/50] Batch 11/12               Loss D: 0.3720, loss G: 2.8143\n",
      "Epoch [20/50] Batch 0/12               Loss D: 0.3173, loss G: 3.0275\n",
      "Epoch [20/50] Batch 1/12               Loss D: 0.3719, loss G: 2.8986\n",
      "Epoch [20/50] Batch 2/12               Loss D: 0.3058, loss G: 2.9369\n",
      "Epoch [20/50] Batch 3/12               Loss D: 0.4113, loss G: 2.9792\n",
      "Epoch [20/50] Batch 4/12               Loss D: 0.3193, loss G: 3.0966\n",
      "Epoch [20/50] Batch 5/12               Loss D: 0.3537, loss G: 2.9883\n",
      "Epoch [20/50] Batch 6/12               Loss D: 0.4003, loss G: 2.9431\n",
      "Epoch [20/50] Batch 7/12               Loss D: 0.3343, loss G: 2.9631\n",
      "Epoch [20/50] Batch 8/12               Loss D: 0.3334, loss G: 2.9726\n",
      "Epoch [20/50] Batch 9/12               Loss D: 0.3919, loss G: 3.0122\n",
      "Epoch [20/50] Batch 10/12               Loss D: 0.4126, loss G: 3.0952\n",
      "Epoch [20/50] Batch 11/12               Loss D: 0.3031, loss G: 3.0269\n",
      "Epoch [21/50] Batch 0/12               Loss D: 0.3673, loss G: 3.1523\n",
      "Epoch [21/50] Batch 1/12               Loss D: 0.3099, loss G: 3.0141\n",
      "Epoch [21/50] Batch 2/12               Loss D: 0.3854, loss G: 3.0691\n",
      "Epoch [21/50] Batch 3/12               Loss D: 0.3605, loss G: 2.9698\n",
      "Epoch [21/50] Batch 4/12               Loss D: 0.3087, loss G: 2.9926\n",
      "Epoch [21/50] Batch 5/12               Loss D: 0.2616, loss G: 2.9558\n",
      "Epoch [21/50] Batch 6/12               Loss D: 0.3438, loss G: 2.9483\n",
      "Epoch [21/50] Batch 7/12               Loss D: 0.3349, loss G: 2.8652\n",
      "Epoch [21/50] Batch 8/12               Loss D: 0.3654, loss G: 2.7913\n",
      "Epoch [21/50] Batch 9/12               Loss D: 0.3573, loss G: 2.8444\n",
      "Epoch [21/50] Batch 10/12               Loss D: 0.3602, loss G: 2.7916\n",
      "Epoch [21/50] Batch 11/12               Loss D: 0.4219, loss G: 2.8862\n",
      "Epoch [22/50] Batch 0/12               Loss D: 0.2926, loss G: 2.9103\n",
      "Epoch [22/50] Batch 1/12               Loss D: 0.3345, loss G: 2.9103\n",
      "Epoch [22/50] Batch 2/12               Loss D: 0.3877, loss G: 2.8934\n",
      "Epoch [22/50] Batch 3/12               Loss D: 0.2991, loss G: 2.8628\n",
      "Epoch [22/50] Batch 4/12               Loss D: 0.3105, loss G: 2.9727\n",
      "Epoch [22/50] Batch 5/12               Loss D: 0.3061, loss G: 2.8108\n",
      "Epoch [22/50] Batch 6/12               Loss D: 0.3495, loss G: 2.9814\n",
      "Epoch [22/50] Batch 7/12               Loss D: 0.3410, loss G: 2.9713\n",
      "Epoch [22/50] Batch 8/12               Loss D: 0.3834, loss G: 3.1486\n",
      "Epoch [22/50] Batch 9/12               Loss D: 0.3116, loss G: 2.9589\n",
      "Epoch [22/50] Batch 10/12               Loss D: 0.3035, loss G: 2.9435\n",
      "Epoch [22/50] Batch 11/12               Loss D: 0.3972, loss G: 2.9529\n",
      "Epoch [23/50] Batch 0/12               Loss D: 0.3146, loss G: 3.0151\n",
      "Epoch [23/50] Batch 1/12               Loss D: 0.3455, loss G: 2.9561\n",
      "Epoch [23/50] Batch 2/12               Loss D: 0.3154, loss G: 2.9109\n",
      "Epoch [23/50] Batch 3/12               Loss D: 0.2759, loss G: 2.9844\n",
      "Epoch [23/50] Batch 4/12               Loss D: 0.3315, loss G: 3.0419\n",
      "Epoch [23/50] Batch 5/12               Loss D: 0.2811, loss G: 2.9816\n",
      "Epoch [23/50] Batch 6/12               Loss D: 0.2809, loss G: 2.8848\n",
      "Epoch [23/50] Batch 7/12               Loss D: 0.2966, loss G: 2.8700\n",
      "Epoch [23/50] Batch 8/12               Loss D: 0.3317, loss G: 3.0015\n",
      "Epoch [23/50] Batch 9/12               Loss D: 0.3483, loss G: 2.8824\n",
      "Epoch [23/50] Batch 10/12               Loss D: 0.3630, loss G: 2.8634\n",
      "Epoch [23/50] Batch 11/12               Loss D: 0.2945, loss G: 2.8571\n",
      "Epoch [24/50] Batch 0/12               Loss D: 0.3071, loss G: 2.6775\n",
      "Epoch [24/50] Batch 1/12               Loss D: 0.2773, loss G: 2.9010\n",
      "Epoch [24/50] Batch 2/12               Loss D: 0.3260, loss G: 2.7813\n",
      "Epoch [24/50] Batch 3/12               Loss D: 0.2593, loss G: 2.8561\n",
      "Epoch [24/50] Batch 4/12               Loss D: 0.3249, loss G: 2.8663\n",
      "Epoch [24/50] Batch 5/12               Loss D: 0.2880, loss G: 2.7448\n",
      "Epoch [24/50] Batch 6/12               Loss D: 0.2967, loss G: 2.8482\n",
      "Epoch [24/50] Batch 7/12               Loss D: 0.2743, loss G: 2.7173\n",
      "Epoch [24/50] Batch 8/12               Loss D: 0.3578, loss G: 2.7249\n",
      "Epoch [24/50] Batch 9/12               Loss D: 0.3125, loss G: 2.6616\n",
      "Epoch [24/50] Batch 10/12               Loss D: 0.3031, loss G: 2.6365\n",
      "Epoch [24/50] Batch 11/12               Loss D: 0.3454, loss G: 2.6928\n",
      "Epoch [25/50] Batch 0/12               Loss D: 0.2530, loss G: 2.7639\n",
      "Epoch [25/50] Batch 1/12               Loss D: 0.3262, loss G: 2.8274\n",
      "Epoch [25/50] Batch 2/12               Loss D: 0.2919, loss G: 2.7066\n",
      "Epoch [25/50] Batch 3/12               Loss D: 0.2946, loss G: 2.6641\n",
      "Epoch [25/50] Batch 4/12               Loss D: 0.2982, loss G: 2.7274\n",
      "Epoch [25/50] Batch 5/12               Loss D: 0.2859, loss G: 2.7235\n",
      "Epoch [25/50] Batch 6/12               Loss D: 0.2847, loss G: 2.7454\n",
      "Epoch [25/50] Batch 7/12               Loss D: 0.2738, loss G: 2.8041\n",
      "Epoch [25/50] Batch 8/12               Loss D: 0.2622, loss G: 2.7795\n",
      "Epoch [25/50] Batch 9/12               Loss D: 0.3046, loss G: 2.7726\n",
      "Epoch [25/50] Batch 10/12               Loss D: 0.3054, loss G: 2.7426\n",
      "Epoch [25/50] Batch 11/12               Loss D: 0.3231, loss G: 2.6517\n",
      "Epoch [26/50] Batch 0/12               Loss D: 0.2657, loss G: 2.6752\n",
      "Epoch [26/50] Batch 1/12               Loss D: 0.2378, loss G: 2.6452\n",
      "Epoch [26/50] Batch 2/12               Loss D: 0.2848, loss G: 2.7679\n",
      "Epoch [26/50] Batch 3/12               Loss D: 0.2887, loss G: 2.7354\n",
      "Epoch [26/50] Batch 4/12               Loss D: 0.2438, loss G: 2.7635\n",
      "Epoch [26/50] Batch 5/12               Loss D: 0.2913, loss G: 2.7603\n",
      "Epoch [26/50] Batch 6/12               Loss D: 0.3420, loss G: 2.6095\n",
      "Epoch [26/50] Batch 7/12               Loss D: 0.2491, loss G: 2.7029\n",
      "Epoch [26/50] Batch 8/12               Loss D: 0.2808, loss G: 2.6775\n",
      "Epoch [26/50] Batch 9/12               Loss D: 0.2988, loss G: 2.7547\n",
      "Epoch [26/50] Batch 10/12               Loss D: 0.2720, loss G: 2.6092\n",
      "Epoch [26/50] Batch 11/12               Loss D: 0.2513, loss G: 2.8301\n",
      "Epoch [27/50] Batch 0/12               Loss D: 0.2595, loss G: 2.6695\n",
      "Epoch [27/50] Batch 1/12               Loss D: 0.2410, loss G: 2.6250\n",
      "Epoch [27/50] Batch 2/12               Loss D: 0.2556, loss G: 2.6938\n",
      "Epoch [27/50] Batch 3/12               Loss D: 0.2789, loss G: 2.7446\n",
      "Epoch [27/50] Batch 4/12               Loss D: 0.2591, loss G: 2.6233\n",
      "Epoch [27/50] Batch 5/12               Loss D: 0.2926, loss G: 2.5382\n",
      "Epoch [27/50] Batch 6/12               Loss D: 0.2433, loss G: 2.6218\n",
      "Epoch [27/50] Batch 7/12               Loss D: 0.2267, loss G: 2.5777\n",
      "Epoch [27/50] Batch 8/12               Loss D: 0.2654, loss G: 2.5213\n",
      "Epoch [27/50] Batch 9/12               Loss D: 0.3606, loss G: 2.5232\n",
      "Epoch [27/50] Batch 10/12               Loss D: 0.2758, loss G: 2.4919\n",
      "Epoch [27/50] Batch 11/12               Loss D: 0.2098, loss G: 2.5700\n",
      "Epoch [28/50] Batch 0/12               Loss D: 0.2495, loss G: 2.5839\n",
      "Epoch [28/50] Batch 1/12               Loss D: 0.2588, loss G: 2.5143\n",
      "Epoch [28/50] Batch 2/12               Loss D: 0.2762, loss G: 2.3859\n",
      "Epoch [28/50] Batch 3/12               Loss D: 0.2271, loss G: 2.5967\n",
      "Epoch [28/50] Batch 4/12               Loss D: 0.2621, loss G: 2.5393\n",
      "Epoch [28/50] Batch 5/12               Loss D: 0.2758, loss G: 2.3565\n",
      "Epoch [28/50] Batch 6/12               Loss D: 0.3041, loss G: 2.3513\n",
      "Epoch [28/50] Batch 7/12               Loss D: 0.2513, loss G: 2.3175\n",
      "Epoch [28/50] Batch 8/12               Loss D: 0.2654, loss G: 2.3953\n",
      "Epoch [28/50] Batch 9/12               Loss D: 0.1937, loss G: 2.4226\n",
      "Epoch [28/50] Batch 10/12               Loss D: 0.2851, loss G: 2.4213\n",
      "Epoch [28/50] Batch 11/12               Loss D: 0.2518, loss G: 2.2099\n",
      "Epoch [29/50] Batch 0/12               Loss D: 0.2526, loss G: 2.4229\n",
      "Epoch [29/50] Batch 1/12               Loss D: 0.2415, loss G: 2.4472\n",
      "Epoch [29/50] Batch 2/12               Loss D: 0.2448, loss G: 2.2964\n",
      "Epoch [29/50] Batch 3/12               Loss D: 0.2667, loss G: 2.4160\n",
      "Epoch [29/50] Batch 4/12               Loss D: 0.2299, loss G: 2.4226\n",
      "Epoch [29/50] Batch 5/12               Loss D: 0.2265, loss G: 2.3622\n",
      "Epoch [29/50] Batch 6/12               Loss D: 0.2525, loss G: 2.3535\n",
      "Epoch [29/50] Batch 7/12               Loss D: 0.2321, loss G: 2.4671\n",
      "Epoch [29/50] Batch 8/12               Loss D: 0.2420, loss G: 2.2924\n",
      "Epoch [29/50] Batch 9/12               Loss D: 0.2631, loss G: 2.3815\n",
      "Epoch [29/50] Batch 10/12               Loss D: 0.2501, loss G: 2.2466\n",
      "Epoch [29/50] Batch 11/12               Loss D: 0.2605, loss G: 2.2839\n",
      "Epoch [30/50] Batch 0/12               Loss D: 0.2437, loss G: 2.2347\n",
      "Epoch [30/50] Batch 1/12               Loss D: 0.2638, loss G: 2.2921\n",
      "Epoch [30/50] Batch 2/12               Loss D: 0.2799, loss G: 2.2573\n",
      "Epoch [30/50] Batch 3/12               Loss D: 0.1954, loss G: 2.3068\n",
      "Epoch [30/50] Batch 4/12               Loss D: 0.2503, loss G: 2.2264\n",
      "Epoch [30/50] Batch 5/12               Loss D: 0.2597, loss G: 2.1211\n",
      "Epoch [30/50] Batch 6/12               Loss D: 0.2496, loss G: 2.1025\n",
      "Epoch [30/50] Batch 7/12               Loss D: 0.2425, loss G: 2.1382\n",
      "Epoch [30/50] Batch 8/12               Loss D: 0.2557, loss G: 2.1656\n",
      "Epoch [30/50] Batch 9/12               Loss D: 0.2332, loss G: 2.2989\n",
      "Epoch [30/50] Batch 10/12               Loss D: 0.2665, loss G: 1.9814\n",
      "Epoch [30/50] Batch 11/12               Loss D: 0.2447, loss G: 2.1378\n",
      "Epoch [31/50] Batch 0/12               Loss D: 0.2460, loss G: 2.1312\n",
      "Epoch [31/50] Batch 1/12               Loss D: 0.2417, loss G: 2.1145\n",
      "Epoch [31/50] Batch 2/12               Loss D: 0.2643, loss G: 2.2947\n",
      "Epoch [31/50] Batch 3/12               Loss D: 0.2472, loss G: 2.1444\n",
      "Epoch [31/50] Batch 4/12               Loss D: 0.2320, loss G: 2.1247\n",
      "Epoch [31/50] Batch 5/12               Loss D: 0.2501, loss G: 2.1816\n",
      "Epoch [31/50] Batch 6/12               Loss D: 0.2554, loss G: 2.0801\n",
      "Epoch [31/50] Batch 7/12               Loss D: 0.2614, loss G: 2.0024\n",
      "Epoch [31/50] Batch 8/12               Loss D: 0.2909, loss G: 1.9414\n",
      "Epoch [31/50] Batch 9/12               Loss D: 0.2344, loss G: 1.9957\n",
      "Epoch [31/50] Batch 10/12               Loss D: 0.2383, loss G: 2.0311\n",
      "Epoch [31/50] Batch 11/12               Loss D: 0.2122, loss G: 2.1315\n",
      "Epoch [32/50] Batch 0/12               Loss D: 0.2609, loss G: 1.9955\n",
      "Epoch [32/50] Batch 1/12               Loss D: 0.2548, loss G: 1.9536\n",
      "Epoch [32/50] Batch 2/12               Loss D: 0.2233, loss G: 1.9373\n",
      "Epoch [32/50] Batch 3/12               Loss D: 0.2346, loss G: 1.9833\n",
      "Epoch [32/50] Batch 4/12               Loss D: 0.2506, loss G: 1.9652\n",
      "Epoch [32/50] Batch 5/12               Loss D: 0.2663, loss G: 2.0068\n",
      "Epoch [32/50] Batch 6/12               Loss D: 0.2135, loss G: 2.0818\n",
      "Epoch [32/50] Batch 7/12               Loss D: 0.2592, loss G: 1.9547\n",
      "Epoch [32/50] Batch 8/12               Loss D: 0.2602, loss G: 1.9963\n",
      "Epoch [32/50] Batch 9/12               Loss D: 0.2479, loss G: 1.9130\n",
      "Epoch [32/50] Batch 10/12               Loss D: 0.2277, loss G: 2.0866\n",
      "Epoch [32/50] Batch 11/12               Loss D: 0.2693, loss G: 1.9784\n",
      "Epoch [33/50] Batch 0/12               Loss D: 0.2506, loss G: 1.9769\n",
      "Epoch [33/50] Batch 1/12               Loss D: 0.2410, loss G: 1.9296\n",
      "Epoch [33/50] Batch 2/12               Loss D: 0.2594, loss G: 1.9025\n",
      "Epoch [33/50] Batch 3/12               Loss D: 0.2397, loss G: 1.9015\n",
      "Epoch [33/50] Batch 4/12               Loss D: 0.2835, loss G: 1.9321\n",
      "Epoch [33/50] Batch 5/12               Loss D: 0.2304, loss G: 1.9886\n",
      "Epoch [33/50] Batch 6/12               Loss D: 0.2413, loss G: 1.8874\n",
      "Epoch [33/50] Batch 7/12               Loss D: 0.2343, loss G: 1.8996\n",
      "Epoch [33/50] Batch 8/12               Loss D: 0.2337, loss G: 1.9690\n",
      "Epoch [33/50] Batch 9/12               Loss D: 0.2470, loss G: 1.9180\n",
      "Epoch [33/50] Batch 10/12               Loss D: 0.2428, loss G: 1.9109\n",
      "Epoch [33/50] Batch 11/12               Loss D: 0.2588, loss G: 1.7777\n",
      "Epoch [34/50] Batch 0/12               Loss D: 0.2446, loss G: 1.8604\n",
      "Epoch [34/50] Batch 1/12               Loss D: 0.2509, loss G: 1.8038\n",
      "Epoch [34/50] Batch 2/12               Loss D: 0.2382, loss G: 1.8628\n",
      "Epoch [34/50] Batch 3/12               Loss D: 0.2710, loss G: 1.7914\n",
      "Epoch [34/50] Batch 4/12               Loss D: 0.2603, loss G: 1.8550\n",
      "Epoch [34/50] Batch 5/12               Loss D: 0.2491, loss G: 1.8520\n",
      "Epoch [34/50] Batch 6/12               Loss D: 0.2443, loss G: 1.8123\n",
      "Epoch [34/50] Batch 7/12               Loss D: 0.2278, loss G: 1.7902\n",
      "Epoch [34/50] Batch 8/12               Loss D: 0.2405, loss G: 1.7702\n",
      "Epoch [34/50] Batch 9/12               Loss D: 0.2615, loss G: 1.8159\n",
      "Epoch [34/50] Batch 10/12               Loss D: 0.2537, loss G: 1.7889\n",
      "Epoch [34/50] Batch 11/12               Loss D: 0.2570, loss G: 1.7877\n",
      "Epoch [35/50] Batch 0/12               Loss D: 0.2543, loss G: 1.7147\n",
      "Epoch [35/50] Batch 1/12               Loss D: 0.2405, loss G: 1.7650\n",
      "Epoch [35/50] Batch 2/12               Loss D: 0.2537, loss G: 1.6446\n",
      "Epoch [35/50] Batch 3/12               Loss D: 0.2582, loss G: 1.7325\n",
      "Epoch [35/50] Batch 4/12               Loss D: 0.2486, loss G: 1.6847\n",
      "Epoch [35/50] Batch 5/12               Loss D: 0.2743, loss G: 1.6654\n",
      "Epoch [35/50] Batch 6/12               Loss D: 0.2280, loss G: 1.7988\n",
      "Epoch [35/50] Batch 7/12               Loss D: 0.2513, loss G: 1.7198\n",
      "Epoch [35/50] Batch 8/12               Loss D: 0.2719, loss G: 1.6778\n",
      "Epoch [35/50] Batch 9/12               Loss D: 0.2468, loss G: 1.7055\n",
      "Epoch [35/50] Batch 10/12               Loss D: 0.2791, loss G: 1.6451\n",
      "Epoch [35/50] Batch 11/12               Loss D: 0.2596, loss G: 1.7789\n",
      "Epoch [36/50] Batch 0/12               Loss D: 0.2442, loss G: 1.6284\n",
      "Epoch [36/50] Batch 1/12               Loss D: 0.2249, loss G: 1.8173\n",
      "Epoch [36/50] Batch 2/12               Loss D: 0.2580, loss G: 1.7523\n",
      "Epoch [36/50] Batch 3/12               Loss D: 0.2557, loss G: 1.7266\n",
      "Epoch [36/50] Batch 4/12               Loss D: 0.2509, loss G: 1.7490\n",
      "Epoch [36/50] Batch 5/12               Loss D: 0.2710, loss G: 1.6477\n",
      "Epoch [36/50] Batch 6/12               Loss D: 0.2464, loss G: 1.6443\n",
      "Epoch [36/50] Batch 7/12               Loss D: 0.2639, loss G: 1.6338\n",
      "Epoch [36/50] Batch 8/12               Loss D: 0.2581, loss G: 1.5792\n",
      "Epoch [36/50] Batch 9/12               Loss D: 0.2581, loss G: 1.6483\n",
      "Epoch [36/50] Batch 10/12               Loss D: 0.2446, loss G: 1.6490\n",
      "Epoch [36/50] Batch 11/12               Loss D: 0.2757, loss G: 1.6435\n",
      "Epoch [37/50] Batch 0/12               Loss D: 0.2627, loss G: 1.6773\n",
      "Epoch [37/50] Batch 1/12               Loss D: 0.2232, loss G: 1.6130\n",
      "Epoch [37/50] Batch 2/12               Loss D: 0.2533, loss G: 1.6235\n",
      "Epoch [37/50] Batch 3/12               Loss D: 0.2586, loss G: 1.5873\n",
      "Epoch [37/50] Batch 4/12               Loss D: 0.2855, loss G: 1.5783\n",
      "Epoch [37/50] Batch 5/12               Loss D: 0.2764, loss G: 1.5096\n",
      "Epoch [37/50] Batch 6/12               Loss D: 0.2552, loss G: 1.5660\n",
      "Epoch [37/50] Batch 7/12               Loss D: 0.2661, loss G: 1.5544\n",
      "Epoch [37/50] Batch 8/12               Loss D: 0.2665, loss G: 1.5154\n",
      "Epoch [37/50] Batch 9/12               Loss D: 0.3025, loss G: 1.4799\n",
      "Epoch [37/50] Batch 10/12               Loss D: 0.2600, loss G: 1.5056\n",
      "Epoch [37/50] Batch 11/12               Loss D: 0.3122, loss G: 1.5412\n",
      "Epoch [38/50] Batch 0/12               Loss D: 0.2861, loss G: 1.4890\n",
      "Epoch [38/50] Batch 1/12               Loss D: 0.2581, loss G: 1.5339\n",
      "Epoch [38/50] Batch 2/12               Loss D: 0.2716, loss G: 1.4698\n",
      "Epoch [38/50] Batch 3/12               Loss D: 0.2562, loss G: 1.5014\n",
      "Epoch [38/50] Batch 4/12               Loss D: 0.2842, loss G: 1.5085\n",
      "Epoch [38/50] Batch 5/12               Loss D: 0.2875, loss G: 1.4488\n",
      "Epoch [38/50] Batch 6/12               Loss D: 0.2954, loss G: 1.3435\n",
      "Epoch [38/50] Batch 7/12               Loss D: 0.2668, loss G: 1.4762\n",
      "Epoch [38/50] Batch 8/12               Loss D: 0.2733, loss G: 1.3606\n",
      "Epoch [38/50] Batch 9/12               Loss D: 0.3031, loss G: 1.3970\n",
      "Epoch [38/50] Batch 10/12               Loss D: 0.3029, loss G: 1.4008\n",
      "Epoch [38/50] Batch 11/12               Loss D: 0.3086, loss G: 1.4400\n",
      "Epoch [39/50] Batch 0/12               Loss D: 0.2990, loss G: 1.3392\n",
      "Epoch [39/50] Batch 1/12               Loss D: 0.2907, loss G: 1.3615\n",
      "Epoch [39/50] Batch 2/12               Loss D: 0.2949, loss G: 1.3431\n",
      "Epoch [39/50] Batch 3/12               Loss D: 0.2904, loss G: 1.3271\n",
      "Epoch [39/50] Batch 4/12               Loss D: 0.2979, loss G: 1.2883\n",
      "Epoch [39/50] Batch 5/12               Loss D: 0.3025, loss G: 1.2930\n",
      "Epoch [39/50] Batch 6/12               Loss D: 0.3035, loss G: 1.3092\n",
      "Epoch [39/50] Batch 7/12               Loss D: 0.3247, loss G: 1.2856\n",
      "Epoch [39/50] Batch 8/12               Loss D: 0.2997, loss G: 1.2847\n",
      "Epoch [39/50] Batch 9/12               Loss D: 0.3130, loss G: 1.3167\n",
      "Epoch [39/50] Batch 10/12               Loss D: 0.3147, loss G: 1.2144\n",
      "Epoch [39/50] Batch 11/12               Loss D: 0.3518, loss G: 1.2532\n",
      "Epoch [40/50] Batch 0/12               Loss D: 0.3279, loss G: 1.2327\n",
      "Epoch [40/50] Batch 1/12               Loss D: 0.2872, loss G: 1.2626\n",
      "Epoch [40/50] Batch 2/12               Loss D: 0.3257, loss G: 1.2187\n",
      "Epoch [40/50] Batch 3/12               Loss D: 0.3274, loss G: 1.2076\n",
      "Epoch [40/50] Batch 4/12               Loss D: 0.3343, loss G: 1.1944\n",
      "Epoch [40/50] Batch 5/12               Loss D: 0.3362, loss G: 1.2052\n",
      "Epoch [40/50] Batch 6/12               Loss D: 0.3415, loss G: 1.2171\n",
      "Epoch [40/50] Batch 7/12               Loss D: 0.3315, loss G: 1.2117\n",
      "Epoch [40/50] Batch 8/12               Loss D: 0.3221, loss G: 1.2022\n",
      "Epoch [40/50] Batch 9/12               Loss D: 0.3146, loss G: 1.1814\n",
      "Epoch [40/50] Batch 10/12               Loss D: 0.3126, loss G: 1.1938\n",
      "Epoch [40/50] Batch 11/12               Loss D: 0.3316, loss G: 1.1367\n",
      "Epoch [41/50] Batch 0/12               Loss D: 0.3531, loss G: 1.1287\n",
      "Epoch [41/50] Batch 1/12               Loss D: 0.3346, loss G: 1.1204\n",
      "Epoch [41/50] Batch 2/12               Loss D: 0.3576, loss G: 1.1530\n",
      "Epoch [41/50] Batch 3/12               Loss D: 0.3193, loss G: 1.1269\n",
      "Epoch [41/50] Batch 4/12               Loss D: 0.3243, loss G: 1.1632\n",
      "Epoch [41/50] Batch 5/12               Loss D: 0.3329, loss G: 1.1235\n",
      "Epoch [41/50] Batch 6/12               Loss D: 0.3457, loss G: 1.1762\n",
      "Epoch [41/50] Batch 7/12               Loss D: 0.3090, loss G: 1.1704\n",
      "Epoch [41/50] Batch 8/12               Loss D: 0.3180, loss G: 1.1486\n",
      "Epoch [41/50] Batch 9/12               Loss D: 0.3791, loss G: 1.1070\n",
      "Epoch [41/50] Batch 10/12               Loss D: 0.3459, loss G: 1.1358\n",
      "Epoch [41/50] Batch 11/12               Loss D: 0.3413, loss G: 1.0856\n",
      "Epoch [42/50] Batch 0/12               Loss D: 0.3612, loss G: 1.0515\n",
      "Epoch [42/50] Batch 1/12               Loss D: 0.3316, loss G: 1.1086\n",
      "Epoch [42/50] Batch 2/12               Loss D: 0.3628, loss G: 1.0437\n",
      "Epoch [42/50] Batch 3/12               Loss D: 0.3306, loss G: 1.1014\n",
      "Epoch [42/50] Batch 4/12               Loss D: 0.3401, loss G: 1.1215\n",
      "Epoch [42/50] Batch 5/12               Loss D: 0.3335, loss G: 1.1149\n",
      "Epoch [42/50] Batch 6/12               Loss D: 0.3545, loss G: 1.0576\n",
      "Epoch [42/50] Batch 7/12               Loss D: 0.3577, loss G: 1.0963\n",
      "Epoch [42/50] Batch 8/12               Loss D: 0.3583, loss G: 1.0743\n",
      "Epoch [42/50] Batch 9/12               Loss D: 0.3371, loss G: 1.0874\n",
      "Epoch [42/50] Batch 10/12               Loss D: 0.3593, loss G: 1.0974\n",
      "Epoch [42/50] Batch 11/12               Loss D: 0.3275, loss G: 1.1075\n",
      "Epoch [43/50] Batch 0/12               Loss D: 0.3684, loss G: 1.0079\n",
      "Epoch [43/50] Batch 1/12               Loss D: 0.3256, loss G: 1.0741\n",
      "Epoch [43/50] Batch 2/12               Loss D: 0.3758, loss G: 1.0894\n",
      "Epoch [43/50] Batch 3/12               Loss D: 0.3439, loss G: 1.0332\n",
      "Epoch [43/50] Batch 4/12               Loss D: 0.3463, loss G: 1.0494\n",
      "Epoch [43/50] Batch 5/12               Loss D: 0.3237, loss G: 1.1084\n",
      "Epoch [43/50] Batch 6/12               Loss D: 0.3329, loss G: 1.1121\n",
      "Epoch [43/50] Batch 7/12               Loss D: 0.3423, loss G: 1.0872\n",
      "Epoch [43/50] Batch 8/12               Loss D: 0.3356, loss G: 1.0846\n",
      "Epoch [43/50] Batch 9/12               Loss D: 0.3430, loss G: 1.0934\n",
      "Epoch [43/50] Batch 10/12               Loss D: 0.3440, loss G: 1.0764\n",
      "Epoch [43/50] Batch 11/12               Loss D: 0.3281, loss G: 1.0990\n",
      "Epoch [44/50] Batch 0/12               Loss D: 0.3511, loss G: 1.0732\n",
      "Epoch [44/50] Batch 1/12               Loss D: 0.3339, loss G: 1.0753\n",
      "Epoch [44/50] Batch 2/12               Loss D: 0.3457, loss G: 1.0736\n",
      "Epoch [44/50] Batch 3/12               Loss D: 0.3229, loss G: 1.0891\n",
      "Epoch [44/50] Batch 4/12               Loss D: 0.3140, loss G: 1.0676\n",
      "Epoch [44/50] Batch 5/12               Loss D: 0.3400, loss G: 1.1036\n",
      "Epoch [44/50] Batch 6/12               Loss D: 0.3227, loss G: 1.0325\n",
      "Epoch [44/50] Batch 7/12               Loss D: 0.3488, loss G: 1.0190\n",
      "Epoch [44/50] Batch 8/12               Loss D: 0.3572, loss G: 1.0652\n",
      "Epoch [44/50] Batch 9/12               Loss D: 0.3219, loss G: 1.0765\n",
      "Epoch [44/50] Batch 10/12               Loss D: 0.3256, loss G: 1.0470\n",
      "Epoch [44/50] Batch 11/12               Loss D: 0.3320, loss G: 1.0601\n",
      "Epoch [45/50] Batch 0/12               Loss D: 0.3260, loss G: 1.0982\n",
      "Epoch [45/50] Batch 1/12               Loss D: 0.3232, loss G: 1.0796\n",
      "Epoch [45/50] Batch 2/12               Loss D: 0.3355, loss G: 1.0369\n",
      "Epoch [45/50] Batch 3/12               Loss D: 0.3087, loss G: 1.0514\n",
      "Epoch [45/50] Batch 4/12               Loss D: 0.3234, loss G: 1.0580\n",
      "Epoch [45/50] Batch 5/12               Loss D: 0.3058, loss G: 1.0497\n",
      "Epoch [45/50] Batch 6/12               Loss D: 0.3535, loss G: 1.0534\n",
      "Epoch [45/50] Batch 7/12               Loss D: 0.3096, loss G: 1.0887\n",
      "Epoch [45/50] Batch 8/12               Loss D: 0.3504, loss G: 1.0546\n",
      "Epoch [45/50] Batch 9/12               Loss D: 0.3199, loss G: 1.0156\n",
      "Epoch [45/50] Batch 10/12               Loss D: 0.3151, loss G: 1.0483\n",
      "Epoch [45/50] Batch 11/12               Loss D: 0.3261, loss G: 1.0453\n",
      "Epoch [46/50] Batch 0/12               Loss D: 0.3223, loss G: 1.0312\n",
      "Epoch [46/50] Batch 1/12               Loss D: 0.3258, loss G: 1.0344\n",
      "Epoch [46/50] Batch 2/12               Loss D: 0.3298, loss G: 1.0424\n",
      "Epoch [46/50] Batch 3/12               Loss D: 0.3100, loss G: 1.0399\n",
      "Epoch [46/50] Batch 4/12               Loss D: 0.3159, loss G: 1.0297\n",
      "Epoch [46/50] Batch 5/12               Loss D: 0.3064, loss G: 1.0318\n",
      "Epoch [46/50] Batch 6/12               Loss D: 0.3216, loss G: 1.0449\n",
      "Epoch [46/50] Batch 7/12               Loss D: 0.2945, loss G: 1.0372\n",
      "Epoch [46/50] Batch 8/12               Loss D: 0.3282, loss G: 1.0482\n",
      "Epoch [46/50] Batch 9/12               Loss D: 0.3079, loss G: 1.0474\n",
      "Epoch [46/50] Batch 10/12               Loss D: 0.3223, loss G: 1.0381\n",
      "Epoch [46/50] Batch 11/12               Loss D: 0.3191, loss G: 1.0131\n",
      "Epoch [47/50] Batch 0/12               Loss D: 0.3055, loss G: 1.0505\n",
      "Epoch [47/50] Batch 1/12               Loss D: 0.2854, loss G: 1.0435\n",
      "Epoch [47/50] Batch 2/12               Loss D: 0.3025, loss G: 1.0566\n",
      "Epoch [47/50] Batch 3/12               Loss D: 0.3013, loss G: 1.0228\n",
      "Epoch [47/50] Batch 4/12               Loss D: 0.3149, loss G: 1.0271\n",
      "Epoch [47/50] Batch 5/12               Loss D: 0.2929, loss G: 1.0279\n",
      "Epoch [47/50] Batch 6/12               Loss D: 0.2998, loss G: 1.0512\n",
      "Epoch [47/50] Batch 7/12               Loss D: 0.3063, loss G: 1.0300\n",
      "Epoch [47/50] Batch 8/12               Loss D: 0.3109, loss G: 1.0099\n",
      "Epoch [47/50] Batch 9/12               Loss D: 0.3190, loss G: 1.0232\n",
      "Epoch [47/50] Batch 10/12               Loss D: 0.3266, loss G: 1.0201\n",
      "Epoch [47/50] Batch 11/12               Loss D: 0.3187, loss G: 1.0356\n",
      "Epoch [48/50] Batch 0/12               Loss D: 0.2901, loss G: 1.0416\n",
      "Epoch [48/50] Batch 1/12               Loss D: 0.3159, loss G: 1.0225\n",
      "Epoch [48/50] Batch 2/12               Loss D: 0.3083, loss G: 1.0258\n",
      "Epoch [48/50] Batch 3/12               Loss D: 0.3020, loss G: 1.0101\n",
      "Epoch [48/50] Batch 4/12               Loss D: 0.2931, loss G: 1.0229\n",
      "Epoch [48/50] Batch 5/12               Loss D: 0.2910, loss G: 1.0164\n",
      "Epoch [48/50] Batch 6/12               Loss D: 0.3194, loss G: 1.0106\n",
      "Epoch [48/50] Batch 7/12               Loss D: 0.2995, loss G: 1.0053\n",
      "Epoch [48/50] Batch 8/12               Loss D: 0.3004, loss G: 0.9921\n",
      "Epoch [48/50] Batch 9/12               Loss D: 0.3044, loss G: 0.9918\n",
      "Epoch [48/50] Batch 10/12               Loss D: 0.3112, loss G: 0.9958\n",
      "Epoch [48/50] Batch 11/12               Loss D: 0.3142, loss G: 0.9774\n",
      "Epoch [49/50] Batch 0/12               Loss D: 0.2954, loss G: 0.9895\n",
      "Epoch [49/50] Batch 1/12               Loss D: 0.3013, loss G: 0.9972\n",
      "Epoch [49/50] Batch 2/12               Loss D: 0.2984, loss G: 1.0005\n",
      "Epoch [49/50] Batch 3/12               Loss D: 0.3142, loss G: 0.9811\n",
      "Epoch [49/50] Batch 4/12               Loss D: 0.2992, loss G: 0.9926\n",
      "Epoch [49/50] Batch 5/12               Loss D: 0.2970, loss G: 0.9868\n",
      "Epoch [49/50] Batch 6/12               Loss D: 0.2992, loss G: 0.9781\n",
      "Epoch [49/50] Batch 7/12               Loss D: 0.2938, loss G: 0.9826\n",
      "Epoch [49/50] Batch 8/12               Loss D: 0.3027, loss G: 0.9724\n",
      "Epoch [49/50] Batch 9/12               Loss D: 0.3091, loss G: 0.9827\n",
      "Epoch [49/50] Batch 10/12               Loss D: 0.3166, loss G: 0.9597\n",
      "Epoch [49/50] Batch 11/12               Loss D: 0.3240, loss G: 0.9694\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.datasets as datasets\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.tensorboard import SummaryWriter  # to print to tensorboard\n",
    "\n",
    "MAX_LENGTH = 768\n",
    "\n",
    "\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, in_features):\n",
    "        super().__init__()\n",
    "        self.disc = nn.Sequential(\n",
    "            nn.Linear(in_features, 128),\n",
    "            nn.LeakyReLU(0.01),\n",
    "            nn.Linear(128, 1),\n",
    "            nn.Sigmoid(),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.disc(x)\n",
    "\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self, z_dim, emb_dim):\n",
    "        super().__init__()\n",
    "        self.gen = nn.Sequential(\n",
    "            nn.Linear(z_dim, 256),\n",
    "            nn.LeakyReLU(0.01),\n",
    "            nn.Linear(256, emb_dim),\n",
    "            nn.Tanh(),  # Assuming you want to normalize the outputs\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.gen(x)\n",
    "\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, embeddings, labels):\n",
    "        self.embeddings = embeddings\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.embeddings)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.embeddings[idx], self.labels[idx]\n",
    "\n",
    "\n",
    "\n",
    "# Hyperparameters etc.\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "lr = 3e-4\n",
    "z_dim = 64\n",
    "embed_dim = MAX_LENGTH  # 784\n",
    "batch_size = 32\n",
    "num_epochs = 50\n",
    "\n",
    "disc = Discriminator(embed_dim).to(device)\n",
    "gen = Generator(z_dim, embed_dim).to(device)\n",
    "fixed_noise = torch.randn((batch_size, z_dim)).to(device)\n",
    "transforms = transforms.Compose(\n",
    "    [\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.5,), (0.5,)),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Assuming you have your embeddings and labels as follows:\n",
    "embeddings = torch.randn(768, 768)  # Placeholder for actual sentence embeddings\n",
    "labels = torch.zeros(768, 1)  # Dummy labels, not used in training but required to create the dataset\n",
    "\n",
    "# Instantiate the custom dataset\n",
    "dataset = CustomDataset(embeddings, labels)\n",
    "\n",
    "# Now, create the DataLoader using the dataset\n",
    "batch_size = 64  # Or any other batch size you wish to use\n",
    "loader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# Optimizers\n",
    "opt_disc = optim.Adam(disc.parameters(), lr=lr)\n",
    "opt_gen = optim.Adam(gen.parameters(), lr=lr)\n",
    "\n",
    "# Loss function\n",
    "criterion = nn.BCELoss()\n",
    "\n",
    "# TensorBoard writers\n",
    "writer_fake = SummaryWriter(f\"logs/fake\")\n",
    "writer_real = SummaryWriter(f\"logs/real\")\n",
    "step = 0\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    for batch_idx, (real, _) in enumerate(loader):\n",
    "        # Adjust for embeddings size\n",
    "        real = real.to(device)\n",
    "        batch_size = real.shape[0]\n",
    "\n",
    "        ### Train Discriminator: max log(D(x)) + log(1 - D(G(z)))\n",
    "        noise = torch.randn(batch_size, z_dim).to(device)\n",
    "        fake = gen(noise)\n",
    "        disc_real = disc(real).view(-1)\n",
    "        lossD_real = criterion(disc_real, torch.ones_like(disc_real))\n",
    "        disc_fake = disc(fake).view(-1)\n",
    "        lossD_fake = criterion(disc_fake, torch.zeros_like(disc_fake))\n",
    "        lossD = (lossD_real + lossD_fake) / 2\n",
    "        disc.zero_grad()\n",
    "        lossD.backward(retain_graph=True)\n",
    "        opt_disc.step()\n",
    "\n",
    "        ### Train Generator: min log(1 - D(G(z))) <-> max log(D(G(z))\n",
    "        output = disc(fake).view(-1)\n",
    "        lossG = criterion(output, torch.ones_like(output))\n",
    "        gen.zero_grad()\n",
    "        lossG.backward()\n",
    "        opt_gen.step()\n",
    "\n",
    "       \n",
    "        print(f\"Epoch [{epoch}/{num_epochs}] Batch {batch_idx}/{len(loader)} \\\n",
    "              Loss D: {lossD:.4f}, loss G: {lossG:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4591df6-3976-4c17-972d-634a71e48eb1",
   "metadata": {},
   "source": [
    "## Tokenizing Ancient Greek Texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ba24be9-67c2-4d0f-b08a-0c07d02724df",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "# Initialize the tokenizer and model\n",
    "aristoberto_tokenizer = AutoTokenizer.from_pretrained(\"Jacobo/aristoBERTo\")\n",
    "aristoberto_model = AutoModel.from_pretrained(\"Jacobo/aristoBERTo\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fa769728-bd9f-430f-917e-4984e2f3bc0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_text = \"\"\" οἳ μὲν γὰρ Δρακάνῳ σ᾽, οἳ δ᾽ Ἰκάρῳ ἠνεμοέσσῃ\n",
    "φάσ᾽, οἳ δ᾽ ἐν Νάξῳ, δῖον γένος, εἰραφιῶτα,\n",
    "οἳ δέ σ᾽ ἐπ᾽ Ἀλφειῷ ποταμῷ βαθυδινήεντι\n",
    "κυσαμένην Σεμέλην τεκέειν Διὶ τερπικεραύνῳ:\n",
    "5ἄλλοι δ᾽ ἐν Θήβῃσιν, ἄναξ, σε λέγουσι γενέσθαι,\n",
    "ψευδόμενοι: σὲ δ᾽ ἔτικτε πατὴρ ἀνδρῶν τε θεῶν τε\n",
    "πολλὸν ἀπ᾽ ἀνθρώπων, κρύπτων λευκώλενον Ἥρην.\n",
    "ἔστι δέ τις Νύση, ὕπατον ὄρος, ἀνθέον ὕλῃ,\n",
    "τηλοῦ Φοινίκης, σχεδὸν Αἰγύπτοιο ῥοάων,\n",
    "10... καί οἱ ἀναστήσουσιν ἀγάλματα πόλλ᾽ ἐνὶ νηοῖς.\n",
    "ὣς δὲ τὰ μὲν τρία, σοὶ πάντως τριετηρίσιν αἰεὶ\n",
    "ἄνθρωποι ῥέξουσι τεληέσσας ἑκατόμβας\"\"\"\n",
    "\n",
    "inputs = aristoberto_tokenizer(sample_text, return_tensors=\"pt\", padding=True, truncation=True, max_length = MAX_LENGTH)\n",
    "\n",
    "# Generate embeddings\n",
    "with torch.no_grad():\n",
    "    outputs = aristoberto_model(**inputs)\n",
    "\n",
    "# outputs.last_hidden_state will contain the token-level embeddings\n",
    "# For sentence-level embeddings, you can average the token embeddings\n",
    "sentence_embedding = outputs.last_hidden_state.mean(dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6c7008c6-3639-4dec-88c2-526a0636b60f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 768])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence_embedding.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5289e47b-f12c-4209-bb84-9d27e930382c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.1584e-01, -4.3876e-01,  7.6951e-02,  2.2028e-01, -7.1418e-01,\n",
       "          3.3325e-01,  1.9584e-01, -1.3508e-02,  1.1990e-01, -4.0512e-01,\n",
       "         -4.2947e-01, -3.4709e-01, -4.0955e-02,  4.4730e-02, -8.5780e-02,\n",
       "          2.2162e-01,  2.5557e-01, -6.5016e-01,  2.3086e-02,  3.0195e-01,\n",
       "          4.6583e-01, -4.7642e-01, -1.0109e-01,  1.0951e-01, -2.7022e-01,\n",
       "          3.3001e-01,  2.7636e-01, -1.5663e-01,  5.1859e-01, -2.0124e-01,\n",
       "          5.7697e-01,  1.5916e-01,  4.5630e-01, -7.6024e-02,  2.4529e-01,\n",
       "          4.8486e-01, -2.8936e-01,  3.5514e-01, -1.5313e-01,  2.5443e-01,\n",
       "         -1.4930e-01, -3.8183e-01,  8.2846e-02,  2.7116e-01,  1.7931e-01,\n",
       "          5.4954e-02, -1.2382e-01, -6.5090e-02, -3.3930e-01,  4.2462e-01,\n",
       "          8.6823e-02, -2.3818e-01,  3.4314e-01,  1.3457e-02, -9.2304e-02,\n",
       "          4.3891e-03, -1.6361e-01, -2.4262e-01,  3.0555e-01, -2.3122e-02,\n",
       "          1.2961e-02, -8.5956e-02,  3.4778e-01,  8.8530e-02,  4.4103e-02,\n",
       "         -2.1675e-01,  1.0899e-01, -2.2948e-02,  2.2065e-01,  3.7016e-02,\n",
       "         -7.6018e-03,  4.1202e-01, -9.3199e-03, -8.7688e-03, -9.1750e-03,\n",
       "          3.0694e-02,  2.5432e-01, -3.2224e-01, -5.1190e-01,  1.5572e-02,\n",
       "          3.0204e-01,  4.1583e-01, -3.8694e-01, -8.0263e-02,  1.9239e-02,\n",
       "          1.4683e-01,  3.8406e-01, -1.1117e-01,  2.1034e-01, -1.1074e-01,\n",
       "          5.8680e-01, -2.8805e-01, -1.0239e-01, -6.5436e-02,  3.7142e-02,\n",
       "          1.2502e-01, -8.5935e-01,  2.4403e-01, -1.7759e-01, -4.2981e-01,\n",
       "          1.7103e-01, -4.2033e-01,  5.1906e-01, -4.1951e-02,  8.8129e-01,\n",
       "         -6.7244e-01,  5.0435e-01,  4.2018e-01,  3.1236e-01, -4.6013e-01,\n",
       "         -6.1850e-02, -4.2478e-01,  9.9250e-01, -4.6255e-01,  3.4705e-01,\n",
       "          1.5826e-02,  6.0662e-01,  3.2921e-01, -5.6387e-01,  4.1942e-01,\n",
       "         -5.0039e-02, -1.6111e-01, -2.3616e-01,  3.2996e-01, -1.3857e-01,\n",
       "         -4.0860e-01, -1.5478e-01,  2.7637e-01, -7.6997e-01, -3.9270e-01,\n",
       "         -4.0475e-01, -5.6747e-01,  2.7112e-01, -2.6757e-01,  1.7712e-01,\n",
       "         -1.2399e-01, -3.3021e-02,  3.0162e-01,  2.5040e-01,  2.2265e-01,\n",
       "         -5.1580e-01,  5.7450e-01, -6.5564e-01,  1.3377e-01,  2.2599e-01,\n",
       "          1.5281e-01,  2.6436e-02, -1.5880e-01, -3.0071e-01,  7.6993e-01,\n",
       "         -4.0526e-01,  2.5065e-01, -1.4918e-02, -2.0591e-02, -9.9679e-03,\n",
       "          2.2546e-01, -2.3387e-01,  2.9744e-01, -4.9706e-01, -1.2936e+01,\n",
       "         -3.1593e-01, -4.1585e-05,  6.2540e-03, -2.9142e-01,  5.8256e-02,\n",
       "          3.0088e-01, -6.2957e-01, -4.2148e-01, -1.6146e-01, -6.6636e-01,\n",
       "         -9.4809e-02,  4.4767e-01, -4.7451e-01,  2.3314e-01,  2.1867e-01,\n",
       "         -2.8548e-01,  1.0451e+00, -5.1965e-01, -2.0834e-01,  6.8695e-01,\n",
       "          6.3661e-01, -6.1237e-01,  2.1960e-01, -2.1655e-01, -2.7282e-02,\n",
       "         -1.6971e-01,  1.7884e-02, -1.7899e-01,  6.8862e-01,  3.7020e-01,\n",
       "          1.6807e-01, -1.1089e-01, -7.1335e-01,  1.4290e+00,  1.0851e-01,\n",
       "         -1.6414e-01,  1.4188e-02, -2.0843e-01, -6.7663e-02,  1.5035e-01,\n",
       "          6.9755e-02, -1.7552e-01, -7.6595e-01,  3.2656e-01,  3.0763e-01,\n",
       "         -3.6412e-01, -3.2170e-01, -5.3241e-02,  1.1585e-01, -1.1245e-01,\n",
       "          5.2926e-02,  4.8443e-02, -1.6731e-01, -3.6602e-01, -1.4461e-01,\n",
       "          3.4124e-01,  2.0526e-01,  8.0548e-02,  4.1930e-01,  3.9833e-01,\n",
       "         -8.4741e-01, -3.9663e-01, -3.1829e-01,  1.2834e-01,  1.5786e-01,\n",
       "         -5.2231e-01,  3.3620e-01,  5.8395e-03, -2.1152e-01,  5.9187e-01,\n",
       "          1.0444e-01,  2.8979e-01, -5.6673e-01,  1.5857e-01,  5.5354e-01,\n",
       "          5.6804e-01, -6.0943e-01,  5.4004e-02,  6.1898e-01, -1.5308e-01,\n",
       "         -2.0953e-01,  5.0338e-01, -7.7254e-01, -6.6176e-01,  5.3831e-01,\n",
       "          3.7285e-01,  2.9517e-01, -1.2788e-01,  1.5187e-01,  1.9741e-01,\n",
       "          4.9143e-02,  2.6394e-01, -3.4939e-01, -5.1865e-02,  1.7701e-01,\n",
       "         -2.1374e-01, -2.4322e-02,  2.9360e-01,  1.3442e-01,  3.9279e-01,\n",
       "          2.1563e-01,  3.3383e-01,  7.4497e-01, -1.1664e-01, -2.6344e-01,\n",
       "         -1.0045e-01, -3.4187e-01,  8.2321e-02, -5.0118e-01, -1.5583e-01,\n",
       "         -4.3101e-01,  2.6092e-01, -3.5153e-02,  3.1724e-01,  6.4066e-01,\n",
       "         -5.2979e-02, -3.8738e-01, -6.0883e-02,  3.5193e-01, -2.3835e-01,\n",
       "         -1.5178e-01,  7.3011e-02, -4.5733e-01,  1.0143e-02,  1.4086e-01,\n",
       "         -5.7280e-02,  1.4317e-01, -6.1298e-01, -7.1691e-02,  1.3361e-01,\n",
       "          2.0886e-01, -1.2931e-01, -1.4225e-01, -3.8916e-01, -2.9183e-01,\n",
       "         -1.3975e-01, -2.0664e-01, -3.8408e-01, -6.7798e-01, -1.8107e-01,\n",
       "         -4.5214e-01, -6.4962e-02,  3.6218e-01, -4.2965e-01,  5.6385e-02,\n",
       "         -2.0762e-01, -6.3237e-01, -6.6917e-01,  8.6850e-01, -1.0761e-01,\n",
       "         -2.2064e-01,  2.4686e-01, -1.0342e-01, -1.4211e-01,  9.2078e-02,\n",
       "         -4.3183e-01, -2.4833e-01, -7.8091e-02,  2.7994e-01, -4.9792e-01,\n",
       "          5.5050e-02, -2.2555e-01, -2.6266e-01, -3.6017e-01, -4.0609e-01,\n",
       "         -2.9018e-01,  5.1543e-02,  4.9507e-03,  4.0979e-01, -1.4402e-01,\n",
       "         -4.7325e-01, -4.0096e-01, -1.4242e-01, -1.2198e-01, -5.1178e-01,\n",
       "         -6.1905e-01,  4.7028e-01,  3.0923e-01,  3.5426e-01,  3.1850e-01,\n",
       "         -3.0402e-01,  5.3728e-02, -2.0021e-01,  2.5308e-01, -3.6918e-01,\n",
       "         -6.1027e-01,  3.2993e-01, -1.1618e+00,  8.8900e-02,  5.7226e-01,\n",
       "         -2.9255e-02,  3.6753e-02, -2.3220e-01, -3.4754e-01,  3.1953e-01,\n",
       "          3.5256e-01,  2.6358e-01, -5.1269e-01, -3.3045e-02, -3.2508e-01,\n",
       "          1.1523e-01,  2.4546e-01, -6.2662e-01, -9.0700e-02,  2.7330e-01,\n",
       "          3.6779e-01, -2.1692e-01, -1.9779e-01,  4.5310e-01,  3.7266e-01,\n",
       "          1.0315e-01,  4.5774e-01, -1.3523e-02,  8.9746e-02,  4.5546e-01,\n",
       "          9.6954e-02, -3.8115e-01, -6.2432e-01, -6.4560e-01,  2.9781e-01,\n",
       "         -1.5385e-01,  5.4780e-01,  9.1750e-02, -1.7807e-01, -6.1667e-02,\n",
       "          8.2941e-01,  4.6154e-01,  9.4384e-02, -4.0700e-01, -6.1705e-01,\n",
       "         -1.4601e-01, -2.9062e-01, -6.0382e-01,  3.3736e-01,  7.4268e-01,\n",
       "          2.8996e-01, -4.6311e-01, -2.9632e-01,  2.8511e-01, -5.6240e-01,\n",
       "         -6.3173e-01,  2.1490e-01, -9.7864e-02,  6.7577e-01, -2.9528e-01,\n",
       "          1.1628e-01,  8.1474e-01,  3.3038e-02,  5.0275e-02, -1.3386e-01,\n",
       "          4.0868e-01, -6.4511e-01, -3.1668e-01,  3.0830e-01,  6.9127e-01,\n",
       "         -3.7529e-01,  1.2547e-01,  1.9111e-01,  2.3063e-01, -3.4764e-01,\n",
       "         -4.1659e-01, -3.5010e-01, -1.8017e-01, -1.6976e-02,  7.7540e-01,\n",
       "         -3.1661e-02,  6.3084e-01,  7.5423e-01,  2.5480e-01,  4.6932e-01,\n",
       "          4.0665e-01,  1.8530e-01,  4.1364e-02, -5.2884e-02,  1.6594e-01,\n",
       "         -1.8996e-01,  6.4211e-01, -7.0756e-01, -1.2934e-01, -6.9503e-02,\n",
       "         -1.4587e-01,  3.0325e-01,  2.0499e-01,  1.7703e-01, -1.4207e-01,\n",
       "         -6.8982e-01,  2.5539e-01,  2.8956e-01,  7.1309e-02,  3.0528e-01,\n",
       "         -3.5829e-02,  3.6401e-01,  9.2273e-01,  2.0173e-01,  7.8804e-01,\n",
       "          5.5653e-01, -2.3248e-01, -2.4243e-01, -5.3717e-02, -1.4418e+00,\n",
       "          7.3847e-01, -2.6189e-02, -4.2162e-01, -2.8655e-01,  8.0732e-01,\n",
       "          1.9147e-01,  1.5735e-01, -4.2216e-01, -2.0199e-01, -3.2903e-02,\n",
       "         -4.0769e-01, -3.7834e-01, -4.5914e-01,  3.0411e-01,  4.8595e-01,\n",
       "         -1.9343e-01, -3.0761e-01,  4.6671e-02, -3.2332e-01, -2.7683e-01,\n",
       "          1.1835e-02, -3.0653e-01, -8.5134e-02, -8.2021e-02, -6.1112e-02,\n",
       "         -4.9495e-01, -1.8596e-01,  2.6764e-02, -3.4990e-01,  9.9689e-02,\n",
       "          3.7077e-01,  2.3255e-01, -1.7435e-01, -9.2536e-02, -4.0074e-02,\n",
       "         -4.1200e-01, -3.1705e-01,  9.2175e-02,  3.3511e-01, -4.6215e-01,\n",
       "         -2.6831e-01, -9.5432e-02, -2.7277e-01, -5.0895e-01,  6.9898e-01,\n",
       "          6.4971e-01,  1.0143e+00,  5.1726e-01,  6.7369e-02, -3.8752e-01,\n",
       "         -3.9490e-01, -2.3649e-01, -2.4099e-01,  5.1270e-01,  1.5776e-01,\n",
       "          4.8538e-01,  3.8865e-01,  1.6148e-01, -1.1558e-01,  1.5274e-01,\n",
       "         -4.2597e-01, -3.8357e-01,  1.9350e-01, -4.8454e-01, -1.6737e-01,\n",
       "         -3.3527e-03,  5.3712e-02,  5.5047e-01,  1.2942e-01,  1.8501e-01,\n",
       "          4.1144e-01,  6.4646e-02,  1.6556e-01,  7.3738e-01,  2.4422e-01,\n",
       "         -1.3103e-02,  2.6056e-02, -4.3891e-01,  5.1394e-01,  7.7700e-03,\n",
       "         -1.1984e-01, -6.9904e-02,  3.8614e-01,  3.3308e-01, -5.1474e-02,\n",
       "         -2.7849e-02,  3.5198e-01,  3.9690e-02,  2.8324e-02, -6.1277e-01,\n",
       "          3.0609e-02,  1.3883e-01,  6.0484e-02, -2.0539e-01, -2.9968e-01,\n",
       "          5.6958e-02,  4.9966e-01,  2.4857e-01, -1.4983e+00, -6.0031e-02,\n",
       "          3.4710e-01,  4.2790e-01, -3.6331e-01, -1.1548e-01, -1.5294e-01,\n",
       "          1.0512e-01, -7.2574e-02,  1.4700e-01, -3.5053e-01,  2.4629e-01,\n",
       "          4.3623e-01,  6.1110e-01, -1.5800e-01, -1.0211e-01,  2.9726e-01,\n",
       "          4.0447e-03,  2.9960e-01,  4.6479e-01, -3.1933e-01, -4.2637e-01,\n",
       "          4.7289e-01, -5.0610e-01, -3.6666e-01, -2.6804e-02, -1.5145e-01,\n",
       "         -2.1766e-01, -5.2833e-02,  1.3199e-01,  6.9647e-02,  3.3285e-01,\n",
       "         -4.5671e-01, -8.0634e-01, -2.7336e-01, -6.7966e-02, -2.7704e-01,\n",
       "          3.4565e-01, -9.3812e-01,  3.4688e-01,  5.5106e-02,  3.1877e-01,\n",
       "         -1.1564e-01,  1.3983e-01, -5.6780e-01, -9.6469e-02,  3.3968e-01,\n",
       "          6.9364e-02, -2.8306e-01,  2.0313e-01,  7.5956e-01,  1.6435e-01,\n",
       "          2.9929e-01, -5.8537e-02,  2.7221e-01,  9.7131e-01, -2.8572e-01,\n",
       "          5.7567e-01,  2.8044e-01,  2.0862e-02, -1.4520e-02, -3.0248e-01,\n",
       "          1.1156e-01,  3.1099e-01,  5.5571e-01,  3.2181e-01,  4.9525e-01,\n",
       "          5.9372e-03,  4.2247e-02, -6.4482e-02, -2.2847e-01,  3.7182e-01,\n",
       "         -2.3949e-01, -3.9590e-02, -1.2879e-01,  1.2791e-01,  2.2348e-01,\n",
       "         -4.1955e-01,  2.4633e-01,  1.6208e-01, -1.2993e-01,  3.6214e-01,\n",
       "         -4.9390e-01,  1.7487e-01, -2.9239e-01,  1.2557e-01,  6.8066e-01,\n",
       "         -1.2616e-01,  4.4303e-01, -4.7694e-02,  1.3869e-01, -3.0635e-01,\n",
       "          5.6300e-01, -4.4337e-02,  5.7766e-02,  4.6555e-01,  2.8881e-01,\n",
       "          1.2299e+00,  4.5377e-01,  7.3589e-01,  4.4983e-02, -7.9878e-02,\n",
       "          5.3777e-02,  9.0426e-02,  1.2116e-01,  3.1691e-01, -3.2307e-01,\n",
       "         -3.8125e-01,  6.5228e-02,  1.8058e-01, -3.2641e-02,  2.5298e-01,\n",
       "         -3.5801e-01, -4.6998e-01,  7.2204e-01,  3.5401e-01,  9.0392e-02,\n",
       "          8.7330e-02, -3.0844e-01,  6.3049e-01,  1.2592e-01, -1.5738e-01,\n",
       "         -6.4292e-02,  3.6408e-01, -1.0505e-01,  5.1820e-01,  1.8411e-01,\n",
       "          2.3428e-01, -7.2458e-02, -2.8993e-01, -4.3100e-01,  5.7175e-01,\n",
       "          1.4881e-01, -2.1735e-01,  5.9980e-01, -5.7452e-01, -1.4447e-01,\n",
       "          5.3349e-01,  7.1141e-01, -2.9471e-01, -5.9746e-02, -5.3533e-01,\n",
       "         -3.5083e-01,  6.2551e-03,  3.5661e-01, -4.9305e-01, -3.4245e-01,\n",
       "         -1.0199e-01,  7.5127e-02,  5.2953e-01, -8.0445e-01, -3.4510e-01,\n",
       "         -4.6074e-03, -3.1347e-01, -4.7173e-01,  9.2312e-01, -7.2767e-01,\n",
       "          1.2427e-01,  1.8337e-01, -1.0984e-01,  1.4044e-01,  8.7221e-02,\n",
       "          3.9422e-01, -1.5537e-01,  9.8949e-03,  7.0803e-02,  2.7765e-01,\n",
       "         -1.7080e-01, -5.8525e-01, -4.3646e-01, -4.2071e-02, -5.0507e-01,\n",
       "          4.0704e-01, -2.4268e-01,  4.5605e-01, -1.2686e-01, -3.4000e-01,\n",
       "         -4.7791e-01, -1.5816e-01, -9.6192e-03, -3.2563e-02, -4.4732e-02,\n",
       "         -8.0465e-01,  3.7461e-01, -4.7697e-01,  2.6267e-02, -1.1183e-03,\n",
       "          1.5033e-01, -1.8458e-01, -5.0353e-01,  9.2344e-02, -1.7179e-01,\n",
       "          1.4152e-01,  4.7554e-02,  1.6844e-01,  1.4498e-02,  5.1739e-02,\n",
       "         -1.3626e-01,  7.7271e-02,  4.5335e-01,  6.2554e-01, -2.2301e-01,\n",
       "         -7.0691e-02,  3.0207e-01, -1.4265e-01, -4.0672e-01, -4.4315e-02,\n",
       "         -1.6107e-01,  1.4463e-01,  5.0508e-01]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence_embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eca3d371-d6f9-4728-a0f4-b2a9c099986c",
   "metadata": {},
   "source": [
    "# Using Ancient-Greek BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a1570f60-714f-4b0a-bee9-672e402f0364",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13 ['[CLS]', 'του', 'βιου', 'του', 'καθ', '΄', 'εαυτους', 'πολλα', 'γινε', '##σθαι', 'συγχ', '##ωρου', '##ν', '[MASK]', '[SEP]']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 15, 768])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokeniser = AutoTokenizer.from_pretrained(\"pranaydeeps/Ancient-Greek-BERT\")\n",
    "model = AutoModel.from_pretrained(\"pranaydeeps/Ancient-Greek-BERT\")\n",
    "\n",
    "input_ids = tokeniser.encode('τοῦ βίου τοῦ καθ ΄ εαυτοὺς πολλὰ γίνεσθαι συγχωροῦν [MASK]')\n",
    "tokens = tokeniser.convert_ids_to_tokens(input_ids)\n",
    "idx = tokens.index(\"[MASK]\")\n",
    "print(idx, tokens)\n",
    "outputs = model(torch.tensor([input_ids]))[0]\n",
    "outputs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "531c8bf2-b0e8-49b8-ac82-b304d58c1af2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5219f8ee-fa0a-4962-ac6c-1c0cc14eafd3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
