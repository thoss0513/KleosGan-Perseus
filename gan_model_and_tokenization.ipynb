{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b26b6e1b-a96d-4b8f-b69c-97caad5edb75",
   "metadata": {},
   "source": [
    "# Training a Simple GAN Model for Sentence Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "9f13714e-a0d7-40f6-adce-bbdc125592ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.datasets as datasets\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.tensorboard import SummaryWriter  # to print to tensorboard\n",
    "import pandas as pd\n",
    "\n",
    "MAX_LENGTH = 100\n",
    "MAX_SEQUENCE_LENGTH = 100\n",
    "\n",
    "\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, in_features):\n",
    "        super().__init__()\n",
    "        self.disc = nn.Sequential(\n",
    "            nn.Linear(in_features, 128),\n",
    "            nn.LeakyReLU(0.01),\n",
    "            nn.Linear(128, 1),\n",
    "            nn.Sigmoid(),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.disc(x)\n",
    "\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self, z_dim, emb_dim):\n",
    "        super().__init__()\n",
    "        self.gen = nn.Sequential(\n",
    "            nn.Linear(z_dim, 256),\n",
    "            nn.LeakyReLU(0.01),\n",
    "            nn.Linear(256, emb_dim),\n",
    "            nn.Tanh(),  # Assuming you want to normalize the outputs\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.gen(x)\n",
    "\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, embeddings):\n",
    "        self.embeddings = embeddings\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.embeddings)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.embeddings[idx]\n",
    "\n",
    "\n",
    "\n",
    "# Hyperparameters etc.\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "lr = 3e-4\n",
    "z_dim = 64\n",
    "embed_dim = MAX_LENGTH  # 784\n",
    "batch_size = 32\n",
    "num_epochs = 50\n",
    "\n",
    "disc = Discriminator(embed_dim).to(device)\n",
    "gen = Generator(z_dim, embed_dim).to(device)\n",
    "fixed_noise = torch.randn((batch_size, z_dim)).to(device)\n",
    "transforms = transforms.Compose(\n",
    "    [\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.5,), (0.5,)),\n",
    "    ]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f5696eb-9815-4a42-a1cd-85ed58a42387",
   "metadata": {},
   "source": [
    "# Preprocessing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "04f8a920-9f2d-4cc9-9213-d9974e3bb544",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, TFAutoModel\n",
    "import tensorflow as tf\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "le = LabelEncoder()\n",
    "\n",
    "model_names = [\"Jacobo/aristoBERTo\", \"pranaydeeps/Ancient-Greek-BERT\"]\n",
    "\n",
    "max_length = MAX_SEQUENCE_LENGTH\n",
    "model_name = model_names[1]\n",
    "\n",
    "def get_cls_token(sentence):\n",
    "    \n",
    "    # Initialize tokenizer and model\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"pranaydeeps/Ancient-Greek-BERT\")\n",
    "    model = TFAutoModel.from_pretrained(\"pranaydeeps/Ancient-Greek-BERT\")\n",
    "\n",
    "    model.trainable = True\n",
    "\n",
    "    # Tokenize the input sentence and prepare input tensors\n",
    "    inputs = tokenizer(sentence, \n",
    "                       max_length=max_length,\n",
    "                       truncation=True,\n",
    "                       padding='max_length',\n",
    "                       return_tensors=\"tf\")  # Ensure to use \"tf\" for TensorFlow models\n",
    "\n",
    "    bert_inputs = {'input_ids': inputs.input_ids,\n",
    "                   'token_type_ids': inputs.token_type_ids,\n",
    "                   'attention_mask': inputs.attention_mask}\n",
    "\n",
    "    # Pass the inputs directly to the model\n",
    "    outputs = model(bert_inputs)\n",
    "\n",
    "    cls_token = outputs[0][:, 0, :]\n",
    "\n",
    "    hidden = cls_token\n",
    "\n",
    "    #Apply Dense Layer to bring down CLS token to max length tokens\n",
    "    hidden = tf.keras.layers.Dense(max_length, activation='relu', name='hidden_layer')(cls_token)\n",
    "\n",
    "    return hidden\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c1cc60b7-a136-4ea6-9575-1c1f96f65ab4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>author</th>\n",
       "      <th>title</th>\n",
       "      <th>line_idx</th>\n",
       "      <th>line_txt</th>\n",
       "      <th>encoded_author</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Homer</td>\n",
       "      <td>Iliad</td>\n",
       "      <td>1</td>\n",
       "      <td>μῆνιν ἄειδε θεὰ Πηληϊάδεω Ἀχιλῆος</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Homer</td>\n",
       "      <td>Iliad</td>\n",
       "      <td>2</td>\n",
       "      <td>οὐλομένην, ἣ μυρίʼ Ἀχαιοῖς ἄλγεʼ ἔθηκε,</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0 author  title  line_idx  \\\n",
       "0           0  Homer  Iliad         1   \n",
       "1           1  Homer  Iliad         2   \n",
       "\n",
       "                                  line_txt  encoded_author  \n",
       "0        μῆνιν ἄειδε θεὰ Πηληϊάδεω Ἀχιλῆος               0  \n",
       "1  οὐλομένην, ἣ μυρίʼ Ἀχαιοῖς ἄλγεʼ ἔθηκε,               0  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Limit number of rows for experimentation\n",
    "num_rows = 2\n",
    "\n",
    "df = pd.read_csv('ancient_greek.csv')[:num_rows]\n",
    "df['encoded_author'] = le.fit_transform(df['author'])\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d1e94ad2-cfa2-4718-8886-0ff9ab8eb581",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFBertModel: ['embeddings.position_ids']\n",
      "- This IS expected if you are initializing TFBertModel from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFBertModel from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the weights of TFBertModel were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n",
      "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFBertModel: ['embeddings.position_ids']\n",
      "- This IS expected if you are initializing TFBertModel from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFBertModel from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the weights of TFBertModel were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>author</th>\n",
       "      <th>title</th>\n",
       "      <th>line_idx</th>\n",
       "      <th>line_txt</th>\n",
       "      <th>encoded_author</th>\n",
       "      <th>agc_embeddings</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Homer</td>\n",
       "      <td>Iliad</td>\n",
       "      <td>1</td>\n",
       "      <td>μῆνιν ἄειδε θεὰ Πηληϊάδεω Ἀχιλῆος</td>\n",
       "      <td>0</td>\n",
       "      <td>((tf.Tensor(0.521089, shape=(), dtype=float32)...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Homer</td>\n",
       "      <td>Iliad</td>\n",
       "      <td>2</td>\n",
       "      <td>οὐλομένην, ἣ μυρίʼ Ἀχαιοῖς ἄλγεʼ ἔθηκε,</td>\n",
       "      <td>0</td>\n",
       "      <td>((tf.Tensor(0.0, shape=(), dtype=float32), tf....</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0 author  title  line_idx  \\\n",
       "0           0  Homer  Iliad         1   \n",
       "1           1  Homer  Iliad         2   \n",
       "\n",
       "                                  line_txt  encoded_author  \\\n",
       "0        μῆνιν ἄειδε θεὰ Πηληϊάδεω Ἀχιλῆος               0   \n",
       "1  οὐλομένην, ἣ μυρίʼ Ἀχαιοῖς ἄλγεʼ ἔθηκε,               0   \n",
       "\n",
       "                                      agc_embeddings  \n",
       "0  ((tf.Tensor(0.521089, shape=(), dtype=float32)...  \n",
       "1  ((tf.Tensor(0.0, shape=(), dtype=float32), tf....  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['agc_embeddings'] = df['line_txt'].apply(get_cls_token)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "bab501a5-f34e-46e9-ba16-b1dfa185bfa0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFBertModel: ['embeddings.position_ids']\n",
      "- This IS expected if you are initializing TFBertModel from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFBertModel from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the weights of TFBertModel were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n",
      "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFBertModel: ['embeddings.position_ids']\n",
      "- This IS expected if you are initializing TFBertModel from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFBertModel from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the weights of TFBertModel were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 100])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the text lines\n",
    "texts = df['line_txt'].tolist()\n",
    "\n",
    "#Wrapper function to turn texts to lists of tensors\n",
    "def texts_to_embeddings(texts):\n",
    "\n",
    "    return [get_cls_token(_) for _ in texts]\n",
    "\n",
    "embeddings = texts_to_embeddings(texts)\n",
    "\n",
    "#Turn EagerTensors list to Normal Tensors list\n",
    "embeddings_pytorch = [torch.tensor(e.numpy()) for e in embeddings]\n",
    "\n",
    "# Convert list of tensors to a single tensor\n",
    "embeddings_tensor = torch.stack(embeddings_pytorch).squeeze(1)  # Adjust dimensions as needed\n",
    "\n",
    "embeddings_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "794028ae-f73a-4a32-ac2f-a684d6aa1979",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Will change this to include the actual dataset when it's done tokenizing\n",
    "embeddings = torch.randn(768, 768)  # Placeholder for actual sentence embeddings\n",
    "\n",
    "# Instantiate the custom dataset\n",
    "dataset = CustomDataset(embeddings, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "9ffebf5c-56d3-4a3b-85d9-2f8d7609f250",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Initialize Dataset\n",
    "embeddings = embeddings_tensor \n",
    "\n",
    "# Instantiate the custom dataset\n",
    "dataset = CustomDataset(embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7316bd16-7e44-41eb-8334-b087d75a97bf",
   "metadata": {},
   "source": [
    "# Training the Actual GAN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4371697f-e139-4706-946e-8b79ad144e89",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "556d7e98-9e3a-4695-8933-6f6c3c40910d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "b23b0d57-0fad-44ba-ae37-797cbf65d23a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [0/10] Batch 0/1               Loss D: 0.6856, loss G: 0.6371\n",
      "Epoch [1/10] Batch 0/1               Loss D: 0.6744, loss G: 0.6371\n",
      "Epoch [2/10] Batch 0/1               Loss D: 0.6662, loss G: 0.6320\n",
      "Epoch [3/10] Batch 0/1               Loss D: 0.6587, loss G: 0.6270\n",
      "Epoch [4/10] Batch 0/1               Loss D: 0.7147, loss G: 0.6209\n",
      "Epoch [5/10] Batch 0/1               Loss D: 0.6481, loss G: 0.6122\n",
      "Epoch [6/10] Batch 0/1               Loss D: 0.7076, loss G: 0.6111\n",
      "Epoch [7/10] Batch 0/1               Loss D: 0.7027, loss G: 0.6075\n",
      "Epoch [8/10] Batch 0/1               Loss D: 0.6337, loss G: 0.6010\n",
      "Epoch [9/10] Batch 0/1               Loss D: 0.6937, loss G: 0.5985\n"
     ]
    }
   ],
   "source": [
    "# Now, create the DataLoader using the dataset\n",
    "batch_size = 64  # Or any other batch size you wish to use\n",
    "loader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# Optimizers\n",
    "opt_disc = optim.Adam(disc.parameters(), lr=lr)\n",
    "opt_gen = optim.Adam(gen.parameters(), lr=lr)\n",
    "\n",
    "# Loss function\n",
    "criterion = nn.BCELoss()\n",
    "\n",
    "# TensorBoard writers\n",
    "writer_fake = SummaryWriter(f\"logs/fake\")\n",
    "writer_real = SummaryWriter(f\"logs/real\")\n",
    "step = 0\n",
    "\n",
    "# Assuming the generator (gen), discriminator (disc), and their optimizers (opt_gen, opt_disc) are defined\n",
    "# Also assuming a loss function (criterion) is defined\n",
    "# z_dim is the dimensionality of the latent space (noise vector)\n",
    "\n",
    "num_epochs = 10  # Number of epochs to train for\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    for batch_idx, real_embeddings in enumerate(loader):\n",
    "        batch_size = real_embeddings[0].size(0)\n",
    "        real_embeddings = real_embeddings[0].to(device)\n",
    "\n",
    "        # Train Discriminator\n",
    "        # Generate fake embeddings\n",
    "        noise = torch.randn(batch_size, z_dim, device=device)\n",
    "        fake_embeddings = gen(noise)\n",
    "\n",
    "        # Get discriminator predictions on real and fake data\n",
    "        disc_real = disc(real_embeddings).view(-1)\n",
    "        disc_fake = disc(fake_embeddings.detach()).view(-1)\n",
    "\n",
    "        # Calculate loss on real and fake\n",
    "        lossD_real = criterion(disc_real, torch.ones_like(disc_real))\n",
    "        lossD_fake = criterion(disc_fake, torch.zeros_like(disc_fake))\n",
    "        lossD = (lossD_real + lossD_fake) / 2\n",
    "\n",
    "        # Update discriminator\n",
    "        opt_disc.zero_grad()\n",
    "        lossD.backward()\n",
    "        opt_disc.step()\n",
    "\n",
    "        # Train Generator\n",
    "        # Generate fake embeddings\n",
    "        output = disc(fake_embeddings).view(-1)\n",
    "        lossG = criterion(output, torch.ones_like(output))\n",
    "\n",
    "        # Update generator\n",
    "        opt_gen.zero_grad()\n",
    "        lossG.backward()\n",
    "        opt_gen.step()\n",
    "\n",
    "        # Optional: Print out loss values or save models/checkpoints here\n",
    "\n",
    "       \n",
    "        print(f\"Epoch [{epoch}/{num_epochs}] Batch {batch_idx}/{len(loader)} \\\n",
    "              Loss D: {lossD:.4f}, loss G: {lossG:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ee8895e-1c24-4198-91bf-c24e69b5e8f4",
   "metadata": {},
   "source": [
    "# Generating a Fake CLS Token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "0d463e18-db86-480f-abdb-1aaac6fb3e01",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 100])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Generating a single sample\n",
    "noise = torch.randn(1, z_dim)\n",
    "\n",
    "with torch.no_grad():  # We don't need to track gradients for generation\n",
    "    fake_data = gen(noise)  # For generating a single sample\n",
    "    \n",
    "fake_data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4591df6-3976-4c17-972d-634a71e48eb1",
   "metadata": {},
   "source": [
    "## Tokenizing Ancient Greek Texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ba24be9-67c2-4d0f-b08a-0c07d02724df",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "# Initialize the tokenizer and model\n",
    "aristoberto_tokenizer = AutoTokenizer.from_pretrained(\"Jacobo/aristoBERTo\")\n",
    "aristoberto_model = AutoModel.from_pretrained(\"Jacobo/aristoBERTo\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fa769728-bd9f-430f-917e-4984e2f3bc0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_text = \"\"\" οἳ μὲν γὰρ Δρακάνῳ σ᾽, οἳ δ᾽ Ἰκάρῳ ἠνεμοέσσῃ\n",
    "φάσ᾽, οἳ δ᾽ ἐν Νάξῳ, δῖον γένος, εἰραφιῶτα,\n",
    "οἳ δέ σ᾽ ἐπ᾽ Ἀλφειῷ ποταμῷ βαθυδινήεντι\n",
    "κυσαμένην Σεμέλην τεκέειν Διὶ τερπικεραύνῳ:\n",
    "5ἄλλοι δ᾽ ἐν Θήβῃσιν, ἄναξ, σε λέγουσι γενέσθαι,\n",
    "ψευδόμενοι: σὲ δ᾽ ἔτικτε πατὴρ ἀνδρῶν τε θεῶν τε\n",
    "πολλὸν ἀπ᾽ ἀνθρώπων, κρύπτων λευκώλενον Ἥρην.\n",
    "ἔστι δέ τις Νύση, ὕπατον ὄρος, ἀνθέον ὕλῃ,\n",
    "τηλοῦ Φοινίκης, σχεδὸν Αἰγύπτοιο ῥοάων,\n",
    "10... καί οἱ ἀναστήσουσιν ἀγάλματα πόλλ᾽ ἐνὶ νηοῖς.\n",
    "ὣς δὲ τὰ μὲν τρία, σοὶ πάντως τριετηρίσιν αἰεὶ\n",
    "ἄνθρωποι ῥέξουσι τεληέσσας ἑκατόμβας\"\"\"\n",
    "\n",
    "inputs = aristoberto_tokenizer(sample_text, return_tensors=\"pt\", padding=True, truncation=True, max_length = MAX_LENGTH)\n",
    "\n",
    "# Generate embeddings\n",
    "with torch.no_grad():\n",
    "    outputs = aristoberto_model(**inputs)\n",
    "\n",
    "# outputs.last_hidden_state will contain the token-level embeddings\n",
    "# For sentence-level embeddings, you can average the token embeddings\n",
    "sentence_embedding = outputs.last_hidden_state.mean(dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6c7008c6-3639-4dec-88c2-526a0636b60f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 768])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence_embedding.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5289e47b-f12c-4209-bb84-9d27e930382c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.1584e-01, -4.3876e-01,  7.6951e-02,  2.2028e-01, -7.1418e-01,\n",
       "          3.3325e-01,  1.9584e-01, -1.3508e-02,  1.1990e-01, -4.0512e-01,\n",
       "         -4.2947e-01, -3.4709e-01, -4.0955e-02,  4.4730e-02, -8.5780e-02,\n",
       "          2.2162e-01,  2.5557e-01, -6.5016e-01,  2.3086e-02,  3.0195e-01,\n",
       "          4.6583e-01, -4.7642e-01, -1.0109e-01,  1.0951e-01, -2.7022e-01,\n",
       "          3.3001e-01,  2.7636e-01, -1.5663e-01,  5.1859e-01, -2.0124e-01,\n",
       "          5.7697e-01,  1.5916e-01,  4.5630e-01, -7.6024e-02,  2.4529e-01,\n",
       "          4.8486e-01, -2.8936e-01,  3.5514e-01, -1.5313e-01,  2.5443e-01,\n",
       "         -1.4930e-01, -3.8183e-01,  8.2846e-02,  2.7116e-01,  1.7931e-01,\n",
       "          5.4954e-02, -1.2382e-01, -6.5090e-02, -3.3930e-01,  4.2462e-01,\n",
       "          8.6823e-02, -2.3818e-01,  3.4314e-01,  1.3457e-02, -9.2304e-02,\n",
       "          4.3891e-03, -1.6361e-01, -2.4262e-01,  3.0555e-01, -2.3122e-02,\n",
       "          1.2961e-02, -8.5956e-02,  3.4778e-01,  8.8530e-02,  4.4103e-02,\n",
       "         -2.1675e-01,  1.0899e-01, -2.2948e-02,  2.2065e-01,  3.7016e-02,\n",
       "         -7.6018e-03,  4.1202e-01, -9.3199e-03, -8.7688e-03, -9.1750e-03,\n",
       "          3.0694e-02,  2.5432e-01, -3.2224e-01, -5.1190e-01,  1.5572e-02,\n",
       "          3.0204e-01,  4.1583e-01, -3.8694e-01, -8.0263e-02,  1.9239e-02,\n",
       "          1.4683e-01,  3.8406e-01, -1.1117e-01,  2.1034e-01, -1.1074e-01,\n",
       "          5.8680e-01, -2.8805e-01, -1.0239e-01, -6.5436e-02,  3.7142e-02,\n",
       "          1.2502e-01, -8.5935e-01,  2.4403e-01, -1.7759e-01, -4.2981e-01,\n",
       "          1.7103e-01, -4.2033e-01,  5.1906e-01, -4.1951e-02,  8.8129e-01,\n",
       "         -6.7244e-01,  5.0435e-01,  4.2018e-01,  3.1236e-01, -4.6013e-01,\n",
       "         -6.1850e-02, -4.2478e-01,  9.9250e-01, -4.6255e-01,  3.4705e-01,\n",
       "          1.5826e-02,  6.0662e-01,  3.2921e-01, -5.6387e-01,  4.1942e-01,\n",
       "         -5.0039e-02, -1.6111e-01, -2.3616e-01,  3.2996e-01, -1.3857e-01,\n",
       "         -4.0860e-01, -1.5478e-01,  2.7637e-01, -7.6997e-01, -3.9270e-01,\n",
       "         -4.0475e-01, -5.6747e-01,  2.7112e-01, -2.6757e-01,  1.7712e-01,\n",
       "         -1.2399e-01, -3.3021e-02,  3.0162e-01,  2.5040e-01,  2.2265e-01,\n",
       "         -5.1580e-01,  5.7450e-01, -6.5564e-01,  1.3377e-01,  2.2599e-01,\n",
       "          1.5281e-01,  2.6436e-02, -1.5880e-01, -3.0071e-01,  7.6993e-01,\n",
       "         -4.0526e-01,  2.5065e-01, -1.4918e-02, -2.0591e-02, -9.9679e-03,\n",
       "          2.2546e-01, -2.3387e-01,  2.9744e-01, -4.9706e-01, -1.2936e+01,\n",
       "         -3.1593e-01, -4.1585e-05,  6.2540e-03, -2.9142e-01,  5.8256e-02,\n",
       "          3.0088e-01, -6.2957e-01, -4.2148e-01, -1.6146e-01, -6.6636e-01,\n",
       "         -9.4809e-02,  4.4767e-01, -4.7451e-01,  2.3314e-01,  2.1867e-01,\n",
       "         -2.8548e-01,  1.0451e+00, -5.1965e-01, -2.0834e-01,  6.8695e-01,\n",
       "          6.3661e-01, -6.1237e-01,  2.1960e-01, -2.1655e-01, -2.7282e-02,\n",
       "         -1.6971e-01,  1.7884e-02, -1.7899e-01,  6.8862e-01,  3.7020e-01,\n",
       "          1.6807e-01, -1.1089e-01, -7.1335e-01,  1.4290e+00,  1.0851e-01,\n",
       "         -1.6414e-01,  1.4188e-02, -2.0843e-01, -6.7663e-02,  1.5035e-01,\n",
       "          6.9755e-02, -1.7552e-01, -7.6595e-01,  3.2656e-01,  3.0763e-01,\n",
       "         -3.6412e-01, -3.2170e-01, -5.3241e-02,  1.1585e-01, -1.1245e-01,\n",
       "          5.2926e-02,  4.8443e-02, -1.6731e-01, -3.6602e-01, -1.4461e-01,\n",
       "          3.4124e-01,  2.0526e-01,  8.0548e-02,  4.1930e-01,  3.9833e-01,\n",
       "         -8.4741e-01, -3.9663e-01, -3.1829e-01,  1.2834e-01,  1.5786e-01,\n",
       "         -5.2231e-01,  3.3620e-01,  5.8395e-03, -2.1152e-01,  5.9187e-01,\n",
       "          1.0444e-01,  2.8979e-01, -5.6673e-01,  1.5857e-01,  5.5354e-01,\n",
       "          5.6804e-01, -6.0943e-01,  5.4004e-02,  6.1898e-01, -1.5308e-01,\n",
       "         -2.0953e-01,  5.0338e-01, -7.7254e-01, -6.6176e-01,  5.3831e-01,\n",
       "          3.7285e-01,  2.9517e-01, -1.2788e-01,  1.5187e-01,  1.9741e-01,\n",
       "          4.9143e-02,  2.6394e-01, -3.4939e-01, -5.1865e-02,  1.7701e-01,\n",
       "         -2.1374e-01, -2.4322e-02,  2.9360e-01,  1.3442e-01,  3.9279e-01,\n",
       "          2.1563e-01,  3.3383e-01,  7.4497e-01, -1.1664e-01, -2.6344e-01,\n",
       "         -1.0045e-01, -3.4187e-01,  8.2321e-02, -5.0118e-01, -1.5583e-01,\n",
       "         -4.3101e-01,  2.6092e-01, -3.5153e-02,  3.1724e-01,  6.4066e-01,\n",
       "         -5.2979e-02, -3.8738e-01, -6.0883e-02,  3.5193e-01, -2.3835e-01,\n",
       "         -1.5178e-01,  7.3011e-02, -4.5733e-01,  1.0143e-02,  1.4086e-01,\n",
       "         -5.7280e-02,  1.4317e-01, -6.1298e-01, -7.1691e-02,  1.3361e-01,\n",
       "          2.0886e-01, -1.2931e-01, -1.4225e-01, -3.8916e-01, -2.9183e-01,\n",
       "         -1.3975e-01, -2.0664e-01, -3.8408e-01, -6.7798e-01, -1.8107e-01,\n",
       "         -4.5214e-01, -6.4962e-02,  3.6218e-01, -4.2965e-01,  5.6385e-02,\n",
       "         -2.0762e-01, -6.3237e-01, -6.6917e-01,  8.6850e-01, -1.0761e-01,\n",
       "         -2.2064e-01,  2.4686e-01, -1.0342e-01, -1.4211e-01,  9.2078e-02,\n",
       "         -4.3183e-01, -2.4833e-01, -7.8091e-02,  2.7994e-01, -4.9792e-01,\n",
       "          5.5050e-02, -2.2555e-01, -2.6266e-01, -3.6017e-01, -4.0609e-01,\n",
       "         -2.9018e-01,  5.1543e-02,  4.9507e-03,  4.0979e-01, -1.4402e-01,\n",
       "         -4.7325e-01, -4.0096e-01, -1.4242e-01, -1.2198e-01, -5.1178e-01,\n",
       "         -6.1905e-01,  4.7028e-01,  3.0923e-01,  3.5426e-01,  3.1850e-01,\n",
       "         -3.0402e-01,  5.3728e-02, -2.0021e-01,  2.5308e-01, -3.6918e-01,\n",
       "         -6.1027e-01,  3.2993e-01, -1.1618e+00,  8.8900e-02,  5.7226e-01,\n",
       "         -2.9255e-02,  3.6753e-02, -2.3220e-01, -3.4754e-01,  3.1953e-01,\n",
       "          3.5256e-01,  2.6358e-01, -5.1269e-01, -3.3045e-02, -3.2508e-01,\n",
       "          1.1523e-01,  2.4546e-01, -6.2662e-01, -9.0700e-02,  2.7330e-01,\n",
       "          3.6779e-01, -2.1692e-01, -1.9779e-01,  4.5310e-01,  3.7266e-01,\n",
       "          1.0315e-01,  4.5774e-01, -1.3523e-02,  8.9746e-02,  4.5546e-01,\n",
       "          9.6954e-02, -3.8115e-01, -6.2432e-01, -6.4560e-01,  2.9781e-01,\n",
       "         -1.5385e-01,  5.4780e-01,  9.1750e-02, -1.7807e-01, -6.1667e-02,\n",
       "          8.2941e-01,  4.6154e-01,  9.4384e-02, -4.0700e-01, -6.1705e-01,\n",
       "         -1.4601e-01, -2.9062e-01, -6.0382e-01,  3.3736e-01,  7.4268e-01,\n",
       "          2.8996e-01, -4.6311e-01, -2.9632e-01,  2.8511e-01, -5.6240e-01,\n",
       "         -6.3173e-01,  2.1490e-01, -9.7864e-02,  6.7577e-01, -2.9528e-01,\n",
       "          1.1628e-01,  8.1474e-01,  3.3038e-02,  5.0275e-02, -1.3386e-01,\n",
       "          4.0868e-01, -6.4511e-01, -3.1668e-01,  3.0830e-01,  6.9127e-01,\n",
       "         -3.7529e-01,  1.2547e-01,  1.9111e-01,  2.3063e-01, -3.4764e-01,\n",
       "         -4.1659e-01, -3.5010e-01, -1.8017e-01, -1.6976e-02,  7.7540e-01,\n",
       "         -3.1661e-02,  6.3084e-01,  7.5423e-01,  2.5480e-01,  4.6932e-01,\n",
       "          4.0665e-01,  1.8530e-01,  4.1364e-02, -5.2884e-02,  1.6594e-01,\n",
       "         -1.8996e-01,  6.4211e-01, -7.0756e-01, -1.2934e-01, -6.9503e-02,\n",
       "         -1.4587e-01,  3.0325e-01,  2.0499e-01,  1.7703e-01, -1.4207e-01,\n",
       "         -6.8982e-01,  2.5539e-01,  2.8956e-01,  7.1309e-02,  3.0528e-01,\n",
       "         -3.5829e-02,  3.6401e-01,  9.2273e-01,  2.0173e-01,  7.8804e-01,\n",
       "          5.5653e-01, -2.3248e-01, -2.4243e-01, -5.3717e-02, -1.4418e+00,\n",
       "          7.3847e-01, -2.6189e-02, -4.2162e-01, -2.8655e-01,  8.0732e-01,\n",
       "          1.9147e-01,  1.5735e-01, -4.2216e-01, -2.0199e-01, -3.2903e-02,\n",
       "         -4.0769e-01, -3.7834e-01, -4.5914e-01,  3.0411e-01,  4.8595e-01,\n",
       "         -1.9343e-01, -3.0761e-01,  4.6671e-02, -3.2332e-01, -2.7683e-01,\n",
       "          1.1835e-02, -3.0653e-01, -8.5134e-02, -8.2021e-02, -6.1112e-02,\n",
       "         -4.9495e-01, -1.8596e-01,  2.6764e-02, -3.4990e-01,  9.9689e-02,\n",
       "          3.7077e-01,  2.3255e-01, -1.7435e-01, -9.2536e-02, -4.0074e-02,\n",
       "         -4.1200e-01, -3.1705e-01,  9.2175e-02,  3.3511e-01, -4.6215e-01,\n",
       "         -2.6831e-01, -9.5432e-02, -2.7277e-01, -5.0895e-01,  6.9898e-01,\n",
       "          6.4971e-01,  1.0143e+00,  5.1726e-01,  6.7369e-02, -3.8752e-01,\n",
       "         -3.9490e-01, -2.3649e-01, -2.4099e-01,  5.1270e-01,  1.5776e-01,\n",
       "          4.8538e-01,  3.8865e-01,  1.6148e-01, -1.1558e-01,  1.5274e-01,\n",
       "         -4.2597e-01, -3.8357e-01,  1.9350e-01, -4.8454e-01, -1.6737e-01,\n",
       "         -3.3527e-03,  5.3712e-02,  5.5047e-01,  1.2942e-01,  1.8501e-01,\n",
       "          4.1144e-01,  6.4646e-02,  1.6556e-01,  7.3738e-01,  2.4422e-01,\n",
       "         -1.3103e-02,  2.6056e-02, -4.3891e-01,  5.1394e-01,  7.7700e-03,\n",
       "         -1.1984e-01, -6.9904e-02,  3.8614e-01,  3.3308e-01, -5.1474e-02,\n",
       "         -2.7849e-02,  3.5198e-01,  3.9690e-02,  2.8324e-02, -6.1277e-01,\n",
       "          3.0609e-02,  1.3883e-01,  6.0484e-02, -2.0539e-01, -2.9968e-01,\n",
       "          5.6958e-02,  4.9966e-01,  2.4857e-01, -1.4983e+00, -6.0031e-02,\n",
       "          3.4710e-01,  4.2790e-01, -3.6331e-01, -1.1548e-01, -1.5294e-01,\n",
       "          1.0512e-01, -7.2574e-02,  1.4700e-01, -3.5053e-01,  2.4629e-01,\n",
       "          4.3623e-01,  6.1110e-01, -1.5800e-01, -1.0211e-01,  2.9726e-01,\n",
       "          4.0447e-03,  2.9960e-01,  4.6479e-01, -3.1933e-01, -4.2637e-01,\n",
       "          4.7289e-01, -5.0610e-01, -3.6666e-01, -2.6804e-02, -1.5145e-01,\n",
       "         -2.1766e-01, -5.2833e-02,  1.3199e-01,  6.9647e-02,  3.3285e-01,\n",
       "         -4.5671e-01, -8.0634e-01, -2.7336e-01, -6.7966e-02, -2.7704e-01,\n",
       "          3.4565e-01, -9.3812e-01,  3.4688e-01,  5.5106e-02,  3.1877e-01,\n",
       "         -1.1564e-01,  1.3983e-01, -5.6780e-01, -9.6469e-02,  3.3968e-01,\n",
       "          6.9364e-02, -2.8306e-01,  2.0313e-01,  7.5956e-01,  1.6435e-01,\n",
       "          2.9929e-01, -5.8537e-02,  2.7221e-01,  9.7131e-01, -2.8572e-01,\n",
       "          5.7567e-01,  2.8044e-01,  2.0862e-02, -1.4520e-02, -3.0248e-01,\n",
       "          1.1156e-01,  3.1099e-01,  5.5571e-01,  3.2181e-01,  4.9525e-01,\n",
       "          5.9372e-03,  4.2247e-02, -6.4482e-02, -2.2847e-01,  3.7182e-01,\n",
       "         -2.3949e-01, -3.9590e-02, -1.2879e-01,  1.2791e-01,  2.2348e-01,\n",
       "         -4.1955e-01,  2.4633e-01,  1.6208e-01, -1.2993e-01,  3.6214e-01,\n",
       "         -4.9390e-01,  1.7487e-01, -2.9239e-01,  1.2557e-01,  6.8066e-01,\n",
       "         -1.2616e-01,  4.4303e-01, -4.7694e-02,  1.3869e-01, -3.0635e-01,\n",
       "          5.6300e-01, -4.4337e-02,  5.7766e-02,  4.6555e-01,  2.8881e-01,\n",
       "          1.2299e+00,  4.5377e-01,  7.3589e-01,  4.4983e-02, -7.9878e-02,\n",
       "          5.3777e-02,  9.0426e-02,  1.2116e-01,  3.1691e-01, -3.2307e-01,\n",
       "         -3.8125e-01,  6.5228e-02,  1.8058e-01, -3.2641e-02,  2.5298e-01,\n",
       "         -3.5801e-01, -4.6998e-01,  7.2204e-01,  3.5401e-01,  9.0392e-02,\n",
       "          8.7330e-02, -3.0844e-01,  6.3049e-01,  1.2592e-01, -1.5738e-01,\n",
       "         -6.4292e-02,  3.6408e-01, -1.0505e-01,  5.1820e-01,  1.8411e-01,\n",
       "          2.3428e-01, -7.2458e-02, -2.8993e-01, -4.3100e-01,  5.7175e-01,\n",
       "          1.4881e-01, -2.1735e-01,  5.9980e-01, -5.7452e-01, -1.4447e-01,\n",
       "          5.3349e-01,  7.1141e-01, -2.9471e-01, -5.9746e-02, -5.3533e-01,\n",
       "         -3.5083e-01,  6.2551e-03,  3.5661e-01, -4.9305e-01, -3.4245e-01,\n",
       "         -1.0199e-01,  7.5127e-02,  5.2953e-01, -8.0445e-01, -3.4510e-01,\n",
       "         -4.6074e-03, -3.1347e-01, -4.7173e-01,  9.2312e-01, -7.2767e-01,\n",
       "          1.2427e-01,  1.8337e-01, -1.0984e-01,  1.4044e-01,  8.7221e-02,\n",
       "          3.9422e-01, -1.5537e-01,  9.8949e-03,  7.0803e-02,  2.7765e-01,\n",
       "         -1.7080e-01, -5.8525e-01, -4.3646e-01, -4.2071e-02, -5.0507e-01,\n",
       "          4.0704e-01, -2.4268e-01,  4.5605e-01, -1.2686e-01, -3.4000e-01,\n",
       "         -4.7791e-01, -1.5816e-01, -9.6192e-03, -3.2563e-02, -4.4732e-02,\n",
       "         -8.0465e-01,  3.7461e-01, -4.7697e-01,  2.6267e-02, -1.1183e-03,\n",
       "          1.5033e-01, -1.8458e-01, -5.0353e-01,  9.2344e-02, -1.7179e-01,\n",
       "          1.4152e-01,  4.7554e-02,  1.6844e-01,  1.4498e-02,  5.1739e-02,\n",
       "         -1.3626e-01,  7.7271e-02,  4.5335e-01,  6.2554e-01, -2.2301e-01,\n",
       "         -7.0691e-02,  3.0207e-01, -1.4265e-01, -4.0672e-01, -4.4315e-02,\n",
       "         -1.6107e-01,  1.4463e-01,  5.0508e-01]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence_embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eca3d371-d6f9-4728-a0f4-b2a9c099986c",
   "metadata": {},
   "source": [
    "# Using Ancient-Greek BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a1570f60-714f-4b0a-bee9-672e402f0364",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13 ['[CLS]', 'του', 'βιου', 'του', 'καθ', '΄', 'εαυτους', 'πολλα', 'γινε', '##σθαι', 'συγχ', '##ωρου', '##ν', '[MASK]', '[SEP]']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 15, 768])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokeniser = AutoTokenizer.from_pretrained(\"pranaydeeps/Ancient-Greek-BERT\")\n",
    "model = AutoModel.from_pretrained(\"pranaydeeps/Ancient-Greek-BERT\")\n",
    "\n",
    "input_ids = tokeniser.encode('τοῦ βίου τοῦ καθ ΄ εαυτοὺς πολλὰ γίνεσθαι συγχωροῦν [MASK]')\n",
    "tokens = tokeniser.convert_ids_to_tokens(input_ids)\n",
    "idx = tokens.index(\"[MASK]\")\n",
    "print(idx, tokens)\n",
    "outputs = model(torch.tensor([input_ids]))[0]\n",
    "outputs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "531c8bf2-b0e8-49b8-ac82-b304d58c1af2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5219f8ee-fa0a-4962-ac6c-1c0cc14eafd3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
