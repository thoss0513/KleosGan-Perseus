{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b26b6e1b-a96d-4b8f-b69c-97caad5edb75",
   "metadata": {},
   "source": [
    "# Training a Simple GAN Model for Sentence Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9f13714e-a0d7-40f6-adce-bbdc125592ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-14 18:02:05.441631: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.datasets as datasets\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.tensorboard import SummaryWriter  # to print to tensorboard\n",
    "import pandas as pd\n",
    "import ast\n",
    "import numpy as np\n",
    "from numpy import expand_dims\n",
    "from numpy import zeros\n",
    "from numpy import ones\n",
    "from numpy import asarray\n",
    "from numpy.random import randn\n",
    "from numpy.random import randint\n",
    "from keras.optimizers import Adam\n",
    "from keras.models import Model\n",
    "from keras.layers import Input\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Reshape\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import Conv2D\n",
    "from keras.layers import Conv1D\n",
    "from keras.layers import Conv2DTranspose\n",
    "from keras.layers import LeakyReLU\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import Lambda\n",
    "from keras.layers import Activation\n",
    "from matplotlib import pyplot as plt\n",
    "from keras import backend\n",
    "from keras.models import Sequential\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from sklearn.metrics import classification_report\n",
    "z\n",
    "np.random.seed(42)\n",
    "MAX_LENGTH = 768\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f5696eb-9815-4a42-a1cd-85ed58a42387",
   "metadata": {},
   "source": [
    "# Preprocessing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c1cc60b7-a136-4ea6-9575-1c1f96f65ab4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>author_labels</th>\n",
       "      <th>cls_tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>[0.278237104415893, -0.33750003576278603, 0.84...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>[-0.13946822285652102, 0.093057677149772, 0.73...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>[-1.493581295013427, 0.748962104320526, 1.0086...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>[-0.9141901135444641, 0.6804959774017331, 0.90...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>[0.040547348558902005, 0.08085644245147701, 1....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32592</th>\n",
       "      <td>32592</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.036508537828922, -0.9830706119537351, 0.194...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32593</th>\n",
       "      <td>32593</td>\n",
       "      <td>0</td>\n",
       "      <td>[-0.49513417482376104, -0.42453348636627203, 0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32594</th>\n",
       "      <td>32594</td>\n",
       "      <td>0</td>\n",
       "      <td>[-0.6720252633094781, -0.37544131278991705, 2....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32595</th>\n",
       "      <td>32595</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.153745874762535, -0.533583104610443, -0.371...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32596</th>\n",
       "      <td>32596</td>\n",
       "      <td>0</td>\n",
       "      <td>[-0.7284083962440491, -0.38280051946640004, -0...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>32597 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Unnamed: 0  author_labels  \\\n",
       "0               0              1   \n",
       "1               1              1   \n",
       "2               2              1   \n",
       "3               3              1   \n",
       "4               4              1   \n",
       "...           ...            ...   \n",
       "32592       32592              0   \n",
       "32593       32593              0   \n",
       "32594       32594              0   \n",
       "32595       32595              0   \n",
       "32596       32596              0   \n",
       "\n",
       "                                              cls_tokens  \n",
       "0      [0.278237104415893, -0.33750003576278603, 0.84...  \n",
       "1      [-0.13946822285652102, 0.093057677149772, 0.73...  \n",
       "2      [-1.493581295013427, 0.748962104320526, 1.0086...  \n",
       "3      [-0.9141901135444641, 0.6804959774017331, 0.90...  \n",
       "4      [0.040547348558902005, 0.08085644245147701, 1....  \n",
       "...                                                  ...  \n",
       "32592  [0.036508537828922, -0.9830706119537351, 0.194...  \n",
       "32593  [-0.49513417482376104, -0.42453348636627203, 0...  \n",
       "32594  [-0.6720252633094781, -0.37544131278991705, 2....  \n",
       "32595  [0.153745874762535, -0.533583104610443, -0.371...  \n",
       "32596  [-0.7284083962440491, -0.38280051946640004, -0...  \n",
       "\n",
       "[32597 rows x 3 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Limit number of rows for experimentation\n",
    "df = pd.read_csv('../data/author_csv.csv')\n",
    "num_rows = len(df)\n",
    "df = df[:num_rows]\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6fbd792d-6676-4c01-8841-dbbff92f6ff6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "author_labels\n",
       "1    27903\n",
       "0     2352\n",
       "2     2342\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['author_labels'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bab501a5-f34e-46e9-ba16-b1dfa185bfa0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.2782, -0.3375,  0.8478,  ..., -0.7637, -0.5338,  0.3607],\n",
       "        [-0.1395,  0.0931,  0.7390,  ...,  0.8308, -0.3973,  1.1339],\n",
       "        [-1.4936,  0.7490,  1.0087,  ...,  1.5033, -1.2829, -0.5658],\n",
       "        ...,\n",
       "        [-0.6720, -0.3754,  2.1211,  ...,  1.0377, -0.6048, -0.7254],\n",
       "        [ 0.1537, -0.5336, -0.3715,  ...,  1.6371,  0.6499, -0.5228],\n",
       "        [-0.7284, -0.3828, -0.2024,  ...,  1.4613,  0.1921,  1.5803]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the text lines\n",
    "n_classes = df['author_labels'].nunique()\n",
    "\n",
    "embeddings = df['cls_tokens']\n",
    "\n",
    "#Turn EagerTensors list to Normal Tensors list\n",
    "embeddings_pytorch = [torch.tensor(np.array(ast.literal_eval(e)), dtype=torch.float32) for e in embeddings]\n",
    "\n",
    "# Convert list of tensors to a single tensor\n",
    "embeddings_tensor = torch.stack(embeddings_pytorch).squeeze(1)  # Adjust dimensions as needed\n",
    "\n",
    "embeddings_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6bd18ba9-ad03-441b-ae16-6c57b6299035",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32597, 768])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings_tensor.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9ffebf5c-56d3-4a3b-85d9-2f8d7609f250",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Initialize Dataset\n",
    "embeddings = embeddings_tensor \n",
    "\n",
    "# Instantiate the custom dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a466ebf-7433-4603-ab37-84204a728102",
   "metadata": {},
   "source": [
    "# Training the SGAN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "77893c09-9853-4b6d-9f70-a404daeba950",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8150, 768])\n",
      "(8150,)\n"
     ]
    }
   ],
   "source": [
    "X = embeddings_tensor\n",
    "y = df['author_labels'].to_numpy()\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=6)\n",
    "print(X_test.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a7e334f-ae89-444d-a951-7fc68392ec8b",
   "metadata": {},
   "source": [
    "The next code block contains all the helper functions and the training script. The main functions to look at are the \n",
    "define_discriminator, define_generator, and define_gan functions. They contain the model architecture for the discriminator and generator functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ab96ffce-c405-4993-afac-b5322c7f30d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 3e-4\n",
    "n_samples = 2000\n",
    "n_epochs = 2\n",
    "loss_var_threshold = 0.01\n",
    "latent_dim=500\n",
    "\n",
    "# define the standalone supervised and unsupervised discriminator models\n",
    "def define_discriminator(in_shape=(MAX_LENGTH,), n_classes=df['author_labels'].nunique()):\n",
    "    \n",
    "  # Embedding input\n",
    "  in_image = Input(shape=in_shape)\n",
    "\n",
    "  # downsample\n",
    "  fe = Dense(units=100, activation='sigmoid')(in_image)\n",
    "  fe = LeakyReLU(negative_slope=0.2)(fe)\n",
    "\n",
    "  # downsample\n",
    "  fe = Dense(units=100, activation='sigmoid')(in_image)\n",
    "  fe = LeakyReLU(negative_slope=0.2)(fe)\n",
    "    \n",
    "  # dropout\n",
    "  fe = Dropout(0.2)(fe)\n",
    "\n",
    "  # output layer nodes\n",
    "  fe = Dense(n_classes)(fe)\n",
    "    \n",
    "  # supervised output\n",
    "  c_out_layer = Activation('softmax')(fe)\n",
    "    \n",
    "  # define and compile supervised discriminator model\n",
    "  c_model = Model(in_image, c_out_layer, name = \"supervised_discrimminator\")\n",
    "  c_model.compile(loss='sparse_categorical_crossentropy', optimizer=Adam(learning_rate=lr, beta_1=0.5), metrics=['accuracy'])\n",
    "    \n",
    "  # unsupervised output\n",
    "  d_out_layer = Dense(units = n_classes + 1, activation = 'softmax')(fe)\n",
    "    \n",
    "  # define and compile unsupervised discriminator model\n",
    "  d_model = Model(in_image, d_out_layer, name = \"discriminator-fake\")\n",
    "  d_model.compile(loss='sparse_categorical_crossentropy', optimizer=Adam(learning_rate=lr, beta_1=0.5), metrics = ['accuracy'])\n",
    "  d_model.trainable = True\n",
    "  c_model.trainable = True\n",
    "    \n",
    "  return d_model, c_model\n",
    "\n",
    "# define the standalone generator model\n",
    "def define_generator(latent_dim, n_outputs=MAX_LENGTH):\n",
    "\tmodel = Sequential(name = \"Generator\")\n",
    "\tmodel.add(Dense(200, activation='sigmoid', kernel_initializer='he_uniform', input_dim=latent_dim))\n",
    "\tmodel.add(Dense(100, activation='sigmoid'))\n",
    "\tmodel.add(Dense(n_outputs, activation='tanh'))\n",
    "\treturn model\n",
    "\n",
    "# define the combined generator and discriminator model, for updating the generator\n",
    "def define_gan(generator, discriminator):\n",
    "    # Ensure the discriminator's parameters are not trainable in the combined model\n",
    "    discriminator.trainable = False\n",
    "    \n",
    "    # Create a new input layer for the GAN (noise sample)\n",
    "    gan_input = Input(shape=(latent_dim,))\n",
    "    \n",
    "    # Output of the generator\n",
    "    generator_output = generator(gan_input)\n",
    "    \n",
    "    # Output of the discriminator (takes generator's output as input)\n",
    "    gan_output = discriminator(generator_output)\n",
    "    \n",
    "    # Define the GAN model\n",
    "    gan = Model(gan_input, gan_output, name = 'gan-model')\n",
    "    \n",
    "    # Compile the GAN model\n",
    "    optimizer = Adam(learning_rate=lr)\n",
    "    gan.compile(optimizer=optimizer, loss='sparse_categorical_crossentropy')\n",
    "    \n",
    "    return gan\n",
    "\n",
    "\n",
    "# load the embeddings and classifications\n",
    "def load_real_samples(X,y):\n",
    "\tprint(X.shape, y.shape)\n",
    "\treturn [X, y]\n",
    "\n",
    "# select a supervised subset of the dataset, ensures classes are balanced\n",
    "def select_supervised_samples(dataset, n_samples=n_samples, n_classes=df['author_labels'].nunique()):\n",
    "    X, y = dataset\n",
    "    n_per_class = int(n_samples / n_classes)\n",
    "    X_samples = []\n",
    "    y_samples = []\n",
    "\n",
    "    for class_index in range(n_classes):\n",
    "        \n",
    "        # Find the indices of all samples belonging to the current class\n",
    "        class_indices = np.where(y == class_index)[0]\n",
    "        \n",
    "        # Randomly choose n_per_class indices for this class\n",
    "        selected_indices = np.random.choice(class_indices, n_per_class, replace=False)\n",
    "        # Append the selected samples to the lists\n",
    "        X_samples.append(X[selected_indices])\n",
    "        y_samples.append(y[selected_indices])\n",
    "    \n",
    "    # Concatenate all selected samples\n",
    "    X_samples = np.concatenate(X_samples, axis=0)\n",
    "    y_samples = np.concatenate(y_samples, axis=0)\n",
    "    return X_samples, y_samples\n",
    "\n",
    "    \n",
    "def generate_real_samples(dataset, n_samples = n_samples):\n",
    "    features, labels = dataset\n",
    "    # Generate random indices\n",
    "    indices = np.random.choice(features.shape[0], n_samples, replace=False)\n",
    "    # Select a random subset of features and labels using the indices\n",
    "    X = features[indices]\n",
    "    labels = labels[indices]\n",
    "    return X, labels\n",
    "\n",
    "def generate_real_samples_multiclass(dataset, n_samples = n_samples):\n",
    "    features, labels = dataset\n",
    "    # Generate random indices\n",
    "    indices = np.random.choice(features.shape[0], n_samples, replace=False)\n",
    "    # Select a random subset of features and labels using the indices\n",
    "    X = features[indices]\n",
    "    labels = labels[indices]\n",
    "    # Generate class labels (assuming you want all ones for real samples)\n",
    "    y = np.ones((n_samples, 1))\n",
    "    return [X, labels], y\n",
    "\n",
    "\n",
    "# generate points in latent space as input for the generator\n",
    "def generate_latent_points(latent_dim, n_samples):\n",
    "\t# generate points in the latent space\n",
    "\tz_input = randn(latent_dim * n_samples)\n",
    "\t# reshape into a batch of inputs for the network\n",
    "\tz_input = z_input.reshape(n_samples, latent_dim)\n",
    "\treturn z_input\n",
    "\n",
    "# use the generator to generate n fake examples, with class labels\n",
    "def generate_fake_samples(generator, latent_dim, n_samples):\n",
    "\t# generate points in latent space\n",
    "\tz_input = generate_latent_points(latent_dim, n_samples)\n",
    "\t# predict outputs\n",
    "\timages = generator.predict(z_input)\n",
    "\t# create class labels\n",
    "\ty = ones((n_samples, 1)) * n_classes\n",
    "\treturn images, y\n",
    "\n",
    "# generate samples and save as a plot and save the model\n",
    "def summarize_performance(step, g_model, c_model, latent_dim, dataset, acc_list, n_samples=100):\n",
    "    \n",
    "\t# prepare fake examples\n",
    "\tX, _ = generate_fake_samples(g_model, latent_dim, n_samples)\n",
    "    \n",
    "\t# scale from [-1,1] to [0,1]\n",
    "\tX = (X + 1) / 2.0\n",
    "    \n",
    "\t# evaluate the classifier model\n",
    "\tX, y = dataset\n",
    "\t_, acc = c_model.evaluate(X, y, verbose=0)\n",
    "\tacc_list.append(acc)\n",
    "\tprint('Classifier Accuracy: %.3f%%' % (acc * 100))\n",
    "\n",
    "\n",
    "# train the generator and discriminator\n",
    "def train(g_model, d_model, c_model, gan_model, dataset, latent_dim, acc_list, n_epochs=n_epochs, n_batch=100):\n",
    "    print(f'Latent dimensions: {latent_dim}\\n-------------')\n",
    "    # Initialize lists to track losses\n",
    "    c_losses, d_losses, g_losses = [], [], []\n",
    "    loss_variance_threshold = loss_var_threshold  # Set a threshold for the variance\n",
    "    min_epochs = 5  # Set a minimum number of epochs to prevent stopping too early\n",
    "    \n",
    "    # select supervised dataset\n",
    "    X_sup, y_sup = X_train, y_train\n",
    "    print(X_sup.shape, y_sup.shape)\n",
    "    \n",
    "    # calculate the number of batches per training epoch\n",
    "    bat_per_epo = int(dataset[0].shape[0] / n_batch)\n",
    "    \n",
    "    # calculate the number of training iterations\n",
    "    n_steps = bat_per_epo * n_epochs\n",
    "    \n",
    "    # calculate the size of half a batch of samples\n",
    "    half_batch = int(n_batch / 2)\n",
    "    print('n_epochs=%d, n_batch=%d, 1/2=%d, b/e=%d, steps=%d' % (n_epochs, n_batch, half_batch, bat_per_epo, n_steps))\n",
    "    \n",
    "    # manually enumerate epochs\n",
    "    for i in range(n_steps):\n",
    "        \n",
    "        # update supervised discriminator (c)\n",
    "        Xsup_real, ysup_real = generate_real_samples([X_sup, y_sup], n_batch)\n",
    "        c_loss, c_acc = c_model.train_on_batch(Xsup_real, ysup_real)\n",
    "        \n",
    "        # update discriminator (d) on real samples\n",
    "        X_real, y_real = generate_real_samples(dataset, n_batch)\n",
    "        d_loss1, d_acc1 = d_model.train_on_batch(X_real, y_real)  # Capture loss and accuracy\n",
    "        \n",
    "        # update unsupervised discriminator (d) on fake samples\n",
    "        X_fake, y_fake = generate_fake_samples(g_model, latent_dim, n_batch)\n",
    "        d_loss2, d_acc2 = d_model.train_on_batch(X_fake, y_fake)  # Capture loss and accuracy\n",
    "\n",
    "        \n",
    "        # update generator (g)\n",
    "        X_gan, y_gan = generate_latent_points(latent_dim, n_batch), ones((n_batch, 1))\n",
    "        g_loss = gan_model.train_on_batch(X_gan, y_gan)\n",
    "        \n",
    "        # summarize loss on this batch\n",
    "        print('>%d, c(loss and accuracy)[%.3f,%.0f], d(loss on real and fake)[%.3f,%.3f], d(acc on real and fake)[%.3f,%.3f], g(loss)[%.3f]' % (i+1, c_loss, c_acc*100, d_loss1, d_loss2, d_acc1 * 100, d_acc2 * 100, g_loss))\n",
    "        \n",
    "        # evaluate the model performance every so often\n",
    "        if (i+1) % (100) == 0:\n",
    "            summarize_performance(i, g_model, c_model, latent_dim, dataset, acc_list)\n",
    "    \n",
    "        # At the end of each epoch (or defined interval) in training loop:\n",
    "        c_losses.append(c_loss)\n",
    "        d_losses.append((d_loss1 + d_loss2) / 2)\n",
    "        g_losses.append(g_loss)\n",
    "        \n",
    "        # Calculate the variance of the last N losses to check for stability\n",
    "        if len(c_losses) > min_epochs * (i + 1):  # Ensure we have enough data to make a meaningful decision\n",
    "            recent_c_var = np.var(c_losses[-min_epochs:])\n",
    "            recent_d_var = np.var(d_losses[-min_epochs:])\n",
    "            recent_g_var = np.var(g_losses[-min_epochs:])\n",
    "            \n",
    "            # Check if all losses have stabilized\n",
    "            if recent_c_var < loss_variance_threshold and recent_d_var < loss_variance_threshold and recent_g_var < loss_variance_threshold:\n",
    "                print(f\"Stopping training at epoch {i+1} due to stabilized losses.\")\n",
    "                break\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bc0360e4-9037-4078-8f13-180764bf15c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def def_models(latent_dim):\n",
    "    d_model, c_model = define_discriminator()\n",
    "    g_model = define_generator(latent_dim)\n",
    "    gan_model = define_gan(g_model, d_model)\n",
    "    d_model.trainable = True\n",
    "    c_model.trainable = True\n",
    "    gan_model.trainable = True\n",
    "    g_model.trainable = True\n",
    "    return d_model, c_model, g_model, gan_model\n",
    "\n",
    "def train_script(latent_dim, dataset):\n",
    "    d_model, c_model, g_model, gan_model = def_models(latent_dim)\n",
    "\n",
    "    train(g_model, d_model, c_model, gan_model, dataset, latent_dim, acc_list=[])\n",
    "    return c_model, d_model, g_model, gan_model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d68b26c9",
   "metadata": {},
   "source": [
    "### Generating Predictions and Evaluating Model Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "abda37c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([24447, 768]) (24447,)\n",
      "Latent dimensions: 500\n",
      "-------------\n",
      "torch.Size([24447, 768]) (24447,)\n",
      "n_epochs=2, n_batch=100, 1/2=50, b/e=244, steps=488\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/thossain64/anaconda3/lib/python3.11/site-packages/keras/src/layers/core/dense.py:88: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step\n",
      ">1, c(loss and accuracy)[1.737,10], d(loss on real and fake)[1.326,1.246], d(acc on real and fake)[45.000,28.500], g(loss)[0.935]\n",
      "WARNING:tensorflow:5 out of the last 5 calls to <function TensorFlowTrainer.make_train_function.<locals>.one_step_on_iterator at 0x1a938b420> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "WARNING:tensorflow:6 out of the last 6 calls to <function TensorFlowTrainer.make_train_function.<locals>.one_step_on_iterator at 0x1a96d5580> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      ">2, c(loss and accuracy)[1.719,14], d(loss on real and fake)[1.223,1.205], d(acc on real and fake)[40.000,32.000], g(loss)[0.876]\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      ">3, c(loss and accuracy)[1.737,12], d(loss on real and fake)[1.193,1.179], d(acc on real and fake)[37.800,31.833], g(loss)[0.833]\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      ">4, c(loss and accuracy)[1.726,12], d(loss on real and fake)[1.168,1.165], d(acc on real and fake)[36.857,32.375], g(loss)[0.790]\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
      ">5, c(loss and accuracy)[1.713,11], d(loss on real and fake)[1.157,1.154], d(acc on real and fake)[36.667,33.100], g(loss)[0.754]\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      ">6, c(loss and accuracy)[1.714,11], d(loss on real and fake)[1.148,1.150], d(acc on real and fake)[37.000,34.000], g(loss)[0.725]\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      ">7, c(loss and accuracy)[1.716,11], d(loss on real and fake)[1.140,1.143], d(acc on real and fake)[37.308,34.643], g(loss)[0.701]\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      ">8, c(loss and accuracy)[1.717,10], d(loss on real and fake)[1.133,1.139], d(acc on real and fake)[37.667,35.313], g(loss)[0.676]\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      ">9, c(loss and accuracy)[1.722,10], d(loss on real and fake)[1.130,1.136], d(acc on real and fake)[37.706,35.611], g(loss)[0.654]\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      ">10, c(loss and accuracy)[1.717,10], d(loss on real and fake)[1.123,1.130], d(acc on real and fake)[38.211,36.300], g(loss)[0.634]\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      ">11, c(loss and accuracy)[1.704,10], d(loss on real and fake)[1.123,1.131], d(acc on real and fake)[38.238,36.500], g(loss)[0.614]\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step \n",
      ">12, c(loss and accuracy)[1.703,10], d(loss on real and fake)[1.121,1.129], d(acc on real and fake)[38.478,36.875], g(loss)[0.597]\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      ">13, c(loss and accuracy)[1.701,10], d(loss on real and fake)[1.123,1.131], d(acc on real and fake)[38.560,37.077], g(loss)[0.581]\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      ">14, c(loss and accuracy)[1.701,10], d(loss on real and fake)[1.123,1.132], d(acc on real and fake)[38.556,37.179], g(loss)[0.567]\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      ">15, c(loss and accuracy)[1.699,10], d(loss on real and fake)[1.125,1.133], d(acc on real and fake)[38.655,37.367], g(loss)[0.553]\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      ">16, c(loss and accuracy)[1.691,10], d(loss on real and fake)[1.126,1.134], d(acc on real and fake)[38.774,37.563], g(loss)[0.541]\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      ">17, c(loss and accuracy)[1.687,11], d(loss on real and fake)[1.124,1.132], d(acc on real and fake)[38.970,37.824], g(loss)[0.530]\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      ">18, c(loss and accuracy)[1.694,10], d(loss on real and fake)[1.122,1.131], d(acc on real and fake)[39.343,38.250], g(loss)[0.519]\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      ">19, c(loss and accuracy)[1.695,10], d(loss on real and fake)[1.124,1.133], d(acc on real and fake)[39.405,38.368], g(loss)[0.509]\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      ">20, c(loss and accuracy)[1.705,10], d(loss on real and fake)[1.127,1.135], d(acc on real and fake)[39.436,38.450], g(loss)[0.500]\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      ">21, c(loss and accuracy)[1.703,10], d(loss on real and fake)[1.130,1.138], d(acc on real and fake)[39.439,38.500], g(loss)[0.492]\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      ">22, c(loss and accuracy)[1.696,10], d(loss on real and fake)[1.133,1.142], d(acc on real and fake)[39.419,38.523], g(loss)[0.484]\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      ">23, c(loss and accuracy)[1.692,10], d(loss on real and fake)[1.134,1.142], d(acc on real and fake)[39.556,38.696], g(loss)[0.477]\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
      ">24, c(loss and accuracy)[1.684,10], d(loss on real and fake)[1.135,1.142], d(acc on real and fake)[39.660,38.833], g(loss)[0.470]\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
      ">25, c(loss and accuracy)[1.683,11], d(loss on real and fake)[1.136,1.144], d(acc on real and fake)[39.755,38.960], g(loss)[0.464]\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      ">26, c(loss and accuracy)[1.688,10], d(loss on real and fake)[1.138,1.144], d(acc on real and fake)[39.843,39.077], g(loss)[0.459]\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
      ">27, c(loss and accuracy)[1.688,10], d(loss on real and fake)[1.138,1.145], d(acc on real and fake)[39.887,39.148], g(loss)[0.453]\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      ">28, c(loss and accuracy)[1.689,10], d(loss on real and fake)[1.139,1.145], d(acc on real and fake)[39.945,39.232], g(loss)[0.448]\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      ">29, c(loss and accuracy)[1.689,10], d(loss on real and fake)[1.139,1.144], d(acc on real and fake)[40.018,39.328], g(loss)[0.444]\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      ">30, c(loss and accuracy)[1.688,10], d(loss on real and fake)[1.137,1.143], d(acc on real and fake)[40.153,39.483], g(loss)[0.440]\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      ">31, c(loss and accuracy)[1.688,10], d(loss on real and fake)[1.137,1.143], d(acc on real and fake)[40.230,39.581], g(loss)[0.436]\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      ">32, c(loss and accuracy)[1.688,10], d(loss on real and fake)[1.138,1.143], d(acc on real and fake)[40.270,39.641], g(loss)[0.432]\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
      ">33, c(loss and accuracy)[1.688,10], d(loss on real and fake)[1.138,1.143], d(acc on real and fake)[40.308,39.697], g(loss)[0.428]\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      ">34, c(loss and accuracy)[1.690,10], d(loss on real and fake)[1.137,1.142], d(acc on real and fake)[40.328,39.735], g(loss)[0.425]\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      ">35, c(loss and accuracy)[1.690,10], d(loss on real and fake)[1.139,1.144], d(acc on real and fake)[40.304,39.729], g(loss)[0.422]\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      ">36, c(loss and accuracy)[1.691,10], d(loss on real and fake)[1.139,1.144], d(acc on real and fake)[40.296,39.736], g(loss)[0.419]\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      ">37, c(loss and accuracy)[1.689,10], d(loss on real and fake)[1.139,1.143], d(acc on real and fake)[40.301,39.757], g(loss)[0.416]\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      ">38, c(loss and accuracy)[1.689,10], d(loss on real and fake)[1.139,1.143], d(acc on real and fake)[40.400,39.868], g(loss)[0.414]\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      ">39, c(loss and accuracy)[1.687,10], d(loss on real and fake)[1.139,1.143], d(acc on real and fake)[40.442,39.923], g(loss)[0.411]\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      ">40, c(loss and accuracy)[1.688,10], d(loss on real and fake)[1.139,1.143], d(acc on real and fake)[40.532,40.025], g(loss)[0.409]\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      ">41, c(loss and accuracy)[1.691,10], d(loss on real and fake)[1.140,1.144], d(acc on real and fake)[40.481,39.988], g(loss)[0.407]\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      ">42, c(loss and accuracy)[1.690,10], d(loss on real and fake)[1.140,1.144], d(acc on real and fake)[40.518,40.036], g(loss)[0.406]\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      ">43, c(loss and accuracy)[1.687,10], d(loss on real and fake)[1.141,1.144], d(acc on real and fake)[40.518,40.047], g(loss)[0.404]\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      ">44, c(loss and accuracy)[1.683,10], d(loss on real and fake)[1.141,1.145], d(acc on real and fake)[40.494,40.034], g(loss)[0.402]\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      ">45, c(loss and accuracy)[1.681,10], d(loss on real and fake)[1.141,1.144], d(acc on real and fake)[40.551,40.100], g(loss)[0.401]\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      ">46, c(loss and accuracy)[1.678,10], d(loss on real and fake)[1.141,1.144], d(acc on real and fake)[40.549,40.109], g(loss)[0.399]\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      ">47, c(loss and accuracy)[1.674,10], d(loss on real and fake)[1.141,1.144], d(acc on real and fake)[40.602,40.170], g(loss)[0.398]\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      ">48, c(loss and accuracy)[1.673,10], d(loss on real and fake)[1.142,1.145], d(acc on real and fake)[40.537,40.115], g(loss)[0.396]\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      ">49, c(loss and accuracy)[1.672,10], d(loss on real and fake)[1.142,1.145], d(acc on real and fake)[40.536,40.122], g(loss)[0.395]\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      ">50, c(loss and accuracy)[1.669,10], d(loss on real and fake)[1.142,1.145], d(acc on real and fake)[40.586,40.180], g(loss)[0.394]\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      ">51, c(loss and accuracy)[1.668,10], d(loss on real and fake)[1.142,1.145], d(acc on real and fake)[40.614,40.216], g(loss)[0.393]\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      ">52, c(loss and accuracy)[1.667,10], d(loss on real and fake)[1.142,1.145], d(acc on real and fake)[40.621,40.231], g(loss)[0.392]\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      ">53, c(loss and accuracy)[1.668,10], d(loss on real and fake)[1.141,1.144], d(acc on real and fake)[40.676,40.292], g(loss)[0.391]\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      ">54, c(loss and accuracy)[1.668,10], d(loss on real and fake)[1.141,1.144], d(acc on real and fake)[40.645,40.269], g(loss)[0.390]\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      ">55, c(loss and accuracy)[1.668,10], d(loss on real and fake)[1.141,1.144], d(acc on real and fake)[40.642,40.273], g(loss)[0.390]\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      ">56, c(loss and accuracy)[1.666,11], d(loss on real and fake)[1.141,1.144], d(acc on real and fake)[40.640,40.277], g(loss)[0.389]\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      ">57, c(loss and accuracy)[1.663,11], d(loss on real and fake)[1.141,1.143], d(acc on real and fake)[40.681,40.325], g(loss)[0.388]\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      ">58, c(loss and accuracy)[1.661,11], d(loss on real and fake)[1.140,1.143], d(acc on real and fake)[40.722,40.371], g(loss)[0.387]\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      ">59, c(loss and accuracy)[1.658,11], d(loss on real and fake)[1.140,1.142], d(acc on real and fake)[40.735,40.390], g(loss)[0.387]\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      ">60, c(loss and accuracy)[1.658,11], d(loss on real and fake)[1.140,1.142], d(acc on real and fake)[40.672,40.333], g(loss)[0.386]\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      ">61, c(loss and accuracy)[1.658,11], d(loss on real and fake)[1.139,1.142], d(acc on real and fake)[40.711,40.377], g(loss)[0.386]\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
      ">62, c(loss and accuracy)[1.657,11], d(loss on real and fake)[1.139,1.142], d(acc on real and fake)[40.675,40.347], g(loss)[0.385]\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      ">63, c(loss and accuracy)[1.658,11], d(loss on real and fake)[1.140,1.142], d(acc on real and fake)[40.664,40.341], g(loss)[0.385]\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      ">64, c(loss and accuracy)[1.658,11], d(loss on real and fake)[1.139,1.141], d(acc on real and fake)[40.677,40.359], g(loss)[0.384]\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      ">65, c(loss and accuracy)[1.657,11], d(loss on real and fake)[1.139,1.141], d(acc on real and fake)[40.674,40.362], g(loss)[0.384]\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      ">66, c(loss and accuracy)[1.655,11], d(loss on real and fake)[1.138,1.140], d(acc on real and fake)[40.695,40.386], g(loss)[0.384]\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      ">67, c(loss and accuracy)[1.654,11], d(loss on real and fake)[1.138,1.140], d(acc on real and fake)[40.714,40.410], g(loss)[0.383]\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      ">68, c(loss and accuracy)[1.654,11], d(loss on real and fake)[1.137,1.139], d(acc on real and fake)[40.696,40.397], g(loss)[0.383]\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      ">69, c(loss and accuracy)[1.653,11], d(loss on real and fake)[1.137,1.139], d(acc on real and fake)[40.672,40.377], g(loss)[0.383]\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      ">70, c(loss and accuracy)[1.654,11], d(loss on real and fake)[1.136,1.138], d(acc on real and fake)[40.712,40.421], g(loss)[0.382]\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      ">71, c(loss and accuracy)[1.653,11], d(loss on real and fake)[1.136,1.138], d(acc on real and fake)[40.702,40.415], g(loss)[0.382]\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      ">72, c(loss and accuracy)[1.654,11], d(loss on real and fake)[1.135,1.137], d(acc on real and fake)[40.762,40.479], g(loss)[0.382]\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
      ">73, c(loss and accuracy)[1.653,11], d(loss on real and fake)[1.135,1.137], d(acc on real and fake)[40.772,40.493], g(loss)[0.382]\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      ">74, c(loss and accuracy)[1.654,11], d(loss on real and fake)[1.134,1.136], d(acc on real and fake)[40.816,40.541], g(loss)[0.382]\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
      ">75, c(loss and accuracy)[1.655,11], d(loss on real and fake)[1.134,1.135], d(acc on real and fake)[40.832,40.560], g(loss)[0.382]\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      ">76, c(loss and accuracy)[1.653,11], d(loss on real and fake)[1.133,1.135], d(acc on real and fake)[40.854,40.586], g(loss)[0.381]\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      ">77, c(loss and accuracy)[1.653,11], d(loss on real and fake)[1.133,1.135], d(acc on real and fake)[40.882,40.617], g(loss)[0.381]\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      ">78, c(loss and accuracy)[1.654,11], d(loss on real and fake)[1.133,1.135], d(acc on real and fake)[40.871,40.609], g(loss)[0.381]\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
      ">79, c(loss and accuracy)[1.653,11], d(loss on real and fake)[1.132,1.134], d(acc on real and fake)[40.885,40.627], g(loss)[0.381]\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      ">80, c(loss and accuracy)[1.652,10], d(loss on real and fake)[1.132,1.134], d(acc on real and fake)[40.887,40.631], g(loss)[0.381]\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      ">81, c(loss and accuracy)[1.652,11], d(loss on real and fake)[1.132,1.134], d(acc on real and fake)[40.894,40.642], g(loss)[0.381]\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      ">82, c(loss and accuracy)[1.650,11], d(loss on real and fake)[1.132,1.134], d(acc on real and fake)[40.890,40.640], g(loss)[0.381]\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      ">83, c(loss and accuracy)[1.650,11], d(loss on real and fake)[1.131,1.133], d(acc on real and fake)[40.915,40.669], g(loss)[0.380]\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      ">84, c(loss and accuracy)[1.650,11], d(loss on real and fake)[1.131,1.133], d(acc on real and fake)[40.922,40.679], g(loss)[0.380]\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      ">85, c(loss and accuracy)[1.650,11], d(loss on real and fake)[1.131,1.133], d(acc on real and fake)[40.941,40.700], g(loss)[0.380]\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      ">86, c(loss and accuracy)[1.650,11], d(loss on real and fake)[1.131,1.133], d(acc on real and fake)[40.953,40.715], g(loss)[0.380]\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      ">87, c(loss and accuracy)[1.648,11], d(loss on real and fake)[1.131,1.132], d(acc on real and fake)[40.936,40.701], g(loss)[0.379]\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      ">88, c(loss and accuracy)[1.648,11], d(loss on real and fake)[1.130,1.132], d(acc on real and fake)[40.931,40.699], g(loss)[0.379]\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      ">89, c(loss and accuracy)[1.647,11], d(loss on real and fake)[1.130,1.132], d(acc on real and fake)[40.955,40.725], g(loss)[0.379]\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
      ">90, c(loss and accuracy)[1.646,11], d(loss on real and fake)[1.130,1.132], d(acc on real and fake)[40.944,40.717], g(loss)[0.379]\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      ">91, c(loss and accuracy)[1.646,11], d(loss on real and fake)[1.130,1.132], d(acc on real and fake)[40.945,40.720], g(loss)[0.379]\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      ">92, c(loss and accuracy)[1.646,11], d(loss on real and fake)[1.130,1.131], d(acc on real and fake)[40.967,40.745], g(loss)[0.378]\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      ">93, c(loss and accuracy)[1.644,11], d(loss on real and fake)[1.129,1.131], d(acc on real and fake)[40.984,40.763], g(loss)[0.378]\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      ">94, c(loss and accuracy)[1.644,11], d(loss on real and fake)[1.129,1.131], d(acc on real and fake)[41.000,40.782], g(loss)[0.378]\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      ">95, c(loss and accuracy)[1.644,11], d(loss on real and fake)[1.128,1.130], d(acc on real and fake)[41.037,40.821], g(loss)[0.378]\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      ">96, c(loss and accuracy)[1.644,11], d(loss on real and fake)[1.128,1.130], d(acc on real and fake)[41.052,40.839], g(loss)[0.378]\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      ">97, c(loss and accuracy)[1.644,11], d(loss on real and fake)[1.127,1.129], d(acc on real and fake)[41.083,40.871], g(loss)[0.378]\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      ">98, c(loss and accuracy)[1.645,11], d(loss on real and fake)[1.127,1.128], d(acc on real and fake)[41.118,40.908], g(loss)[0.377]\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
      ">99, c(loss and accuracy)[1.645,11], d(loss on real and fake)[1.126,1.128], d(acc on real and fake)[41.142,40.934], g(loss)[0.377]\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
      ">100, c(loss and accuracy)[1.645,11], d(loss on real and fake)[1.126,1.128], d(acc on real and fake)[41.136,40.930], g(loss)[0.377]\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
      "Classifier Accuracy: 7.379%\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      ">101, c(loss and accuracy)[1.578,7], d(loss on real and fake)[1.126,1.127], d(acc on real and fake)[41.164,40.960], g(loss)[0.377]\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
      ">102, c(loss and accuracy)[1.578,7], d(loss on real and fake)[1.125,1.127], d(acc on real and fake)[41.202,41.000], g(loss)[0.377]\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      ">103, c(loss and accuracy)[1.578,7], d(loss on real and fake)[1.125,1.126], d(acc on real and fake)[41.205,41.005], g(loss)[0.377]\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      ">104, c(loss and accuracy)[1.578,7], d(loss on real and fake)[1.125,1.126], d(acc on real and fake)[41.203,41.005], g(loss)[0.376]\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      ">105, c(loss and accuracy)[1.578,7], d(loss on real and fake)[1.125,1.126], d(acc on real and fake)[41.196,41.000], g(loss)[0.376]\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      ">106, c(loss and accuracy)[1.578,7], d(loss on real and fake)[1.124,1.126], d(acc on real and fake)[41.218,41.024], g(loss)[0.376]\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
      ">107, c(loss and accuracy)[1.578,7], d(loss on real and fake)[1.124,1.126], d(acc on real and fake)[41.225,41.033], g(loss)[0.376]\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      ">108, c(loss and accuracy)[1.578,8], d(loss on real and fake)[1.124,1.126], d(acc on real and fake)[41.233,41.042], g(loss)[0.375]\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
      ">109, c(loss and accuracy)[1.578,8], d(loss on real and fake)[1.124,1.126], d(acc on real and fake)[41.267,41.078], g(loss)[0.375]\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      ">110, c(loss and accuracy)[1.579,8], d(loss on real and fake)[1.124,1.125], d(acc on real and fake)[41.283,41.095], g(loss)[0.375]\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      ">111, c(loss and accuracy)[1.579,8], d(loss on real and fake)[1.124,1.125], d(acc on real and fake)[41.303,41.117], g(loss)[0.374]\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
      ">112, c(loss and accuracy)[1.579,8], d(loss on real and fake)[1.123,1.125], d(acc on real and fake)[41.323,41.138], g(loss)[0.374]\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
      ">113, c(loss and accuracy)[1.579,8], d(loss on real and fake)[1.124,1.125], d(acc on real and fake)[41.320,41.137], g(loss)[0.374]\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      ">114, c(loss and accuracy)[1.579,8], d(loss on real and fake)[1.124,1.126], d(acc on real and fake)[41.317,41.136], g(loss)[0.373]\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      ">115, c(loss and accuracy)[1.579,8], d(loss on real and fake)[1.124,1.126], d(acc on real and fake)[41.319,41.139], g(loss)[0.373]\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      ">116, c(loss and accuracy)[1.579,8], d(loss on real and fake)[1.125,1.126], d(acc on real and fake)[41.303,41.125], g(loss)[0.373]\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      ">117, c(loss and accuracy)[1.579,8], d(loss on real and fake)[1.125,1.127], d(acc on real and fake)[41.309,41.132], g(loss)[0.373]\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      ">118, c(loss and accuracy)[1.579,8], d(loss on real and fake)[1.125,1.127], d(acc on real and fake)[41.332,41.157], g(loss)[0.372]\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      ">119, c(loss and accuracy)[1.579,8], d(loss on real and fake)[1.125,1.127], d(acc on real and fake)[41.316,41.143], g(loss)[0.372]\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      ">120, c(loss and accuracy)[1.579,8], d(loss on real and fake)[1.125,1.127], d(acc on real and fake)[41.335,41.162], g(loss)[0.372]\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      ">121, c(loss and accuracy)[1.579,8], d(loss on real and fake)[1.125,1.127], d(acc on real and fake)[41.328,41.157], g(loss)[0.372]\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
      ">122, c(loss and accuracy)[1.579,8], d(loss on real and fake)[1.125,1.127], d(acc on real and fake)[41.354,41.184], g(loss)[0.371]\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      ">123, c(loss and accuracy)[1.579,8], d(loss on real and fake)[1.125,1.127], d(acc on real and fake)[41.371,41.203], g(loss)[0.371]\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      ">124, c(loss and accuracy)[1.579,8], d(loss on real and fake)[1.126,1.127], d(acc on real and fake)[41.368,41.202], g(loss)[0.371]\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      ">125, c(loss and accuracy)[1.579,8], d(loss on real and fake)[1.126,1.127], d(acc on real and fake)[41.365,41.200], g(loss)[0.371]\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      ">126, c(loss and accuracy)[1.579,8], d(loss on real and fake)[1.125,1.127], d(acc on real and fake)[41.390,41.226], g(loss)[0.370]\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      ">127, c(loss and accuracy)[1.579,8], d(loss on real and fake)[1.125,1.127], d(acc on real and fake)[41.383,41.220], g(loss)[0.370]\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      ">128, c(loss and accuracy)[1.579,8], d(loss on real and fake)[1.126,1.127], d(acc on real and fake)[41.376,41.215], g(loss)[0.370]\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      ">129, c(loss and accuracy)[1.579,8], d(loss on real and fake)[1.126,1.127], d(acc on real and fake)[41.385,41.225], g(loss)[0.369]\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      ">130, c(loss and accuracy)[1.579,8], d(loss on real and fake)[1.126,1.128], d(acc on real and fake)[41.371,41.212], g(loss)[0.369]\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      ">131, c(loss and accuracy)[1.578,8], d(loss on real and fake)[1.127,1.128], d(acc on real and fake)[41.360,41.202], g(loss)[0.369]\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      ">132, c(loss and accuracy)[1.578,8], d(loss on real and fake)[1.127,1.128], d(acc on real and fake)[41.361,41.205], g(loss)[0.369]\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      ">133, c(loss and accuracy)[1.578,8], d(loss on real and fake)[1.127,1.128], d(acc on real and fake)[41.385,41.229], g(loss)[0.368]\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      ">134, c(loss and accuracy)[1.578,8], d(loss on real and fake)[1.127,1.129], d(acc on real and fake)[41.386,41.231], g(loss)[0.368]\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
      ">135, c(loss and accuracy)[1.578,8], d(loss on real and fake)[1.127,1.129], d(acc on real and fake)[41.390,41.237], g(loss)[0.368]\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      ">136, c(loss and accuracy)[1.578,8], d(loss on real and fake)[1.128,1.129], d(acc on real and fake)[41.391,41.239], g(loss)[0.367]\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      ">137, c(loss and accuracy)[1.578,8], d(loss on real and fake)[1.128,1.130], d(acc on real and fake)[41.388,41.237], g(loss)[0.367]\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      ">138, c(loss and accuracy)[1.578,8], d(loss on real and fake)[1.128,1.130], d(acc on real and fake)[41.393,41.243], g(loss)[0.367]\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      ">139, c(loss and accuracy)[1.578,8], d(loss on real and fake)[1.129,1.130], d(acc on real and fake)[41.368,41.219], g(loss)[0.366]\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      ">140, c(loss and accuracy)[1.578,8], d(loss on real and fake)[1.129,1.131], d(acc on real and fake)[41.351,41.204], g(loss)[0.366]\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      ">141, c(loss and accuracy)[1.577,8], d(loss on real and fake)[1.129,1.131], d(acc on real and fake)[41.370,41.223], g(loss)[0.366]\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      ">142, c(loss and accuracy)[1.577,8], d(loss on real and fake)[1.130,1.131], d(acc on real and fake)[41.367,41.222], g(loss)[0.366]\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      ">143, c(loss and accuracy)[1.577,8], d(loss on real and fake)[1.130,1.132], d(acc on real and fake)[41.379,41.234], g(loss)[0.365]\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
      ">144, c(loss and accuracy)[1.577,8], d(loss on real and fake)[1.130,1.132], d(acc on real and fake)[41.408,41.264], g(loss)[0.365]\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
      ">145, c(loss and accuracy)[1.577,8], d(loss on real and fake)[1.130,1.132], d(acc on real and fake)[41.415,41.272], g(loss)[0.365]\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      ">146, c(loss and accuracy)[1.577,8], d(loss on real and fake)[1.131,1.133], d(acc on real and fake)[41.419,41.277], g(loss)[0.364]\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      ">147, c(loss and accuracy)[1.577,8], d(loss on real and fake)[1.131,1.133], d(acc on real and fake)[41.420,41.279], g(loss)[0.364]\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
      ">148, c(loss and accuracy)[1.577,8], d(loss on real and fake)[1.131,1.133], d(acc on real and fake)[41.424,41.284], g(loss)[0.364]\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      ">149, c(loss and accuracy)[1.577,8], d(loss on real and fake)[1.132,1.133], d(acc on real and fake)[41.434,41.295], g(loss)[0.364]\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      ">150, c(loss and accuracy)[1.577,8], d(loss on real and fake)[1.132,1.133], d(acc on real and fake)[41.458,41.320], g(loss)[0.363]\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      ">151, c(loss and accuracy)[1.576,8], d(loss on real and fake)[1.132,1.134], d(acc on real and fake)[41.458,41.321], g(loss)[0.363]\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
      ">152, c(loss and accuracy)[1.576,8], d(loss on real and fake)[1.133,1.134], d(acc on real and fake)[41.446,41.309], g(loss)[0.363]\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      ">153, c(loss and accuracy)[1.576,8], d(loss on real and fake)[1.133,1.135], d(acc on real and fake)[41.443,41.307], g(loss)[0.362]\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      ">154, c(loss and accuracy)[1.576,8], d(loss on real and fake)[1.133,1.135], d(acc on real and fake)[41.450,41.315], g(loss)[0.362]\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      ">155, c(loss and accuracy)[1.576,8], d(loss on real and fake)[1.134,1.136], d(acc on real and fake)[41.447,41.313], g(loss)[0.362]\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
      ">156, c(loss and accuracy)[1.576,8], d(loss on real and fake)[1.134,1.136], d(acc on real and fake)[41.447,41.314], g(loss)[0.362]\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      ">157, c(loss and accuracy)[1.576,8], d(loss on real and fake)[1.135,1.137], d(acc on real and fake)[41.438,41.306], g(loss)[0.361]\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      ">158, c(loss and accuracy)[1.575,8], d(loss on real and fake)[1.135,1.137], d(acc on real and fake)[41.451,41.320], g(loss)[0.361]\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      ">159, c(loss and accuracy)[1.575,8], d(loss on real and fake)[1.135,1.137], d(acc on real and fake)[41.473,41.343], g(loss)[0.361]\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      ">160, c(loss and accuracy)[1.575,8], d(loss on real and fake)[1.136,1.137], d(acc on real and fake)[41.464,41.334], g(loss)[0.361]\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      ">161, c(loss and accuracy)[1.575,9], d(loss on real and fake)[1.136,1.138], d(acc on real and fake)[41.483,41.354], g(loss)[0.361]\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      ">162, c(loss and accuracy)[1.575,9], d(loss on real and fake)[1.136,1.138], d(acc on real and fake)[41.495,41.367], g(loss)[0.360]\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      ">163, c(loss and accuracy)[1.575,9], d(loss on real and fake)[1.136,1.138], d(acc on real and fake)[41.511,41.383], g(loss)[0.360]\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      ">164, c(loss and accuracy)[1.575,9], d(loss on real and fake)[1.137,1.138], d(acc on real and fake)[41.502,41.375], g(loss)[0.360]\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      ">165, c(loss and accuracy)[1.574,9], d(loss on real and fake)[1.137,1.139], d(acc on real and fake)[41.508,41.382], g(loss)[0.360]\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      ">166, c(loss and accuracy)[1.574,9], d(loss on real and fake)[1.137,1.139], d(acc on real and fake)[41.517,41.392], g(loss)[0.359]\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      ">167, c(loss and accuracy)[1.574,9], d(loss on real and fake)[1.138,1.139], d(acc on real and fake)[41.523,41.398], g(loss)[0.359]\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      ">168, c(loss and accuracy)[1.574,9], d(loss on real and fake)[1.138,1.139], d(acc on real and fake)[41.537,41.414], g(loss)[0.359]\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step \n",
      ">169, c(loss and accuracy)[1.574,9], d(loss on real and fake)[1.138,1.140], d(acc on real and fake)[41.516,41.393], g(loss)[0.359]\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
      ">170, c(loss and accuracy)[1.574,9], d(loss on real and fake)[1.139,1.140], d(acc on real and fake)[41.519,41.397], g(loss)[0.358]\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      ">171, c(loss and accuracy)[1.574,9], d(loss on real and fake)[1.139,1.140], d(acc on real and fake)[41.545,41.424], g(loss)[0.358]\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
      ">172, c(loss and accuracy)[1.573,9], d(loss on real and fake)[1.139,1.141], d(acc on real and fake)[41.545,41.424], g(loss)[0.358]\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      ">173, c(loss and accuracy)[1.573,9], d(loss on real and fake)[1.139,1.141], d(acc on real and fake)[41.562,41.442], g(loss)[0.357]\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      ">174, c(loss and accuracy)[1.573,9], d(loss on real and fake)[1.139,1.141], d(acc on real and fake)[41.571,41.451], g(loss)[0.357]\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      ">175, c(loss and accuracy)[1.573,9], d(loss on real and fake)[1.140,1.142], d(acc on real and fake)[41.562,41.443], g(loss)[0.357]\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      ">176, c(loss and accuracy)[1.573,9], d(loss on real and fake)[1.141,1.142], d(acc on real and fake)[41.550,41.432], g(loss)[0.357]\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
      ">177, c(loss and accuracy)[1.573,9], d(loss on real and fake)[1.141,1.143], d(acc on real and fake)[41.533,41.415], g(loss)[0.356]\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      ">178, c(loss and accuracy)[1.573,9], d(loss on real and fake)[1.142,1.143], d(acc on real and fake)[41.538,41.421], g(loss)[0.356]\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      ">179, c(loss and accuracy)[1.573,9], d(loss on real and fake)[1.142,1.143], d(acc on real and fake)[41.552,41.436], g(loss)[0.356]\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      ">180, c(loss and accuracy)[1.572,9], d(loss on real and fake)[1.142,1.144], d(acc on real and fake)[41.563,41.447], g(loss)[0.356]\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      ">181, c(loss and accuracy)[1.572,9], d(loss on real and fake)[1.142,1.144], d(acc on real and fake)[41.576,41.461], g(loss)[0.355]\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      ">182, c(loss and accuracy)[1.572,9], d(loss on real and fake)[1.142,1.144], d(acc on real and fake)[41.595,41.481], g(loss)[0.355]\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      ">183, c(loss and accuracy)[1.572,9], d(loss on real and fake)[1.143,1.144], d(acc on real and fake)[41.600,41.486], g(loss)[0.355]\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      ">184, c(loss and accuracy)[1.571,9], d(loss on real and fake)[1.143,1.145], d(acc on real and fake)[41.602,41.489], g(loss)[0.355]\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      ">185, c(loss and accuracy)[1.571,9], d(loss on real and fake)[1.143,1.145], d(acc on real and fake)[41.618,41.505], g(loss)[0.354]\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      ">186, c(loss and accuracy)[1.571,9], d(loss on real and fake)[1.143,1.145], d(acc on real and fake)[41.623,41.511], g(loss)[0.354]\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      ">187, c(loss and accuracy)[1.571,9], d(loss on real and fake)[1.144,1.145], d(acc on real and fake)[41.630,41.519], g(loss)[0.354]\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      ">188, c(loss and accuracy)[1.571,9], d(loss on real and fake)[1.144,1.146], d(acc on real and fake)[41.640,41.529], g(loss)[0.354]\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      ">189, c(loss and accuracy)[1.571,9], d(loss on real and fake)[1.144,1.146], d(acc on real and fake)[41.666,41.556], g(loss)[0.353]\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      ">190, c(loss and accuracy)[1.570,9], d(loss on real and fake)[1.144,1.146], d(acc on real and fake)[41.681,41.571], g(loss)[0.353]\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      ">191, c(loss and accuracy)[1.570,9], d(loss on real and fake)[1.145,1.146], d(acc on real and fake)[41.682,41.573], g(loss)[0.353]\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      ">192, c(loss and accuracy)[1.570,9], d(loss on real and fake)[1.145,1.146], d(acc on real and fake)[41.695,41.586], g(loss)[0.353]\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      ">193, c(loss and accuracy)[1.570,9], d(loss on real and fake)[1.145,1.147], d(acc on real and fake)[41.688,41.580], g(loss)[0.353]\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      ">194, c(loss and accuracy)[1.570,9], d(loss on real and fake)[1.146,1.147], d(acc on real and fake)[41.687,41.580], g(loss)[0.352]\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      ">195, c(loss and accuracy)[1.569,9], d(loss on real and fake)[1.146,1.148], d(acc on real and fake)[41.697,41.590], g(loss)[0.352]\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      ">196, c(loss and accuracy)[1.569,9], d(loss on real and fake)[1.147,1.148], d(acc on real and fake)[41.691,41.584], g(loss)[0.352]\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      ">197, c(loss and accuracy)[1.569,9], d(loss on real and fake)[1.147,1.149], d(acc on real and fake)[41.690,41.584], g(loss)[0.352]\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      ">198, c(loss and accuracy)[1.569,9], d(loss on real and fake)[1.147,1.149], d(acc on real and fake)[41.696,41.591], g(loss)[0.351]\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      ">199, c(loss and accuracy)[1.568,9], d(loss on real and fake)[1.148,1.149], d(acc on real and fake)[41.693,41.588], g(loss)[0.351]\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      ">200, c(loss and accuracy)[1.568,9], d(loss on real and fake)[1.148,1.150], d(acc on real and fake)[41.687,41.583], g(loss)[0.351]\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      "Classifier Accuracy: 9.289%\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      ">201, c(loss and accuracy)[1.287,9], d(loss on real and fake)[1.149,1.150], d(acc on real and fake)[41.698,41.595], g(loss)[0.351]\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      ">202, c(loss and accuracy)[1.288,9], d(loss on real and fake)[1.149,1.151], d(acc on real and fake)[41.692,41.589], g(loss)[0.350]\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      ">203, c(loss and accuracy)[1.288,9], d(loss on real and fake)[1.150,1.151], d(acc on real and fake)[41.691,41.589], g(loss)[0.350]\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      ">204, c(loss and accuracy)[1.288,9], d(loss on real and fake)[1.150,1.152], d(acc on real and fake)[41.683,41.581], g(loss)[0.350]\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      ">205, c(loss and accuracy)[1.288,9], d(loss on real and fake)[1.151,1.152], d(acc on real and fake)[41.694,41.593], g(loss)[0.350]\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      ">206, c(loss and accuracy)[1.288,10], d(loss on real and fake)[1.151,1.153], d(acc on real and fake)[41.689,41.587], g(loss)[0.350]\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      ">207, c(loss and accuracy)[1.288,10], d(loss on real and fake)[1.152,1.153], d(acc on real and fake)[41.705,41.604], g(loss)[0.349]\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      ">208, c(loss and accuracy)[1.288,10], d(loss on real and fake)[1.152,1.153], d(acc on real and fake)[41.706,41.606], g(loss)[0.349]\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      ">209, c(loss and accuracy)[1.288,10], d(loss on real and fake)[1.153,1.154], d(acc on real and fake)[41.691,41.591], g(loss)[0.349]\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      ">210, c(loss and accuracy)[1.288,10], d(loss on real and fake)[1.153,1.155], d(acc on real and fake)[41.706,41.607], g(loss)[0.349]\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      ">211, c(loss and accuracy)[1.288,10], d(loss on real and fake)[1.153,1.155], d(acc on real and fake)[41.708,41.609], g(loss)[0.348]\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      ">212, c(loss and accuracy)[1.288,10], d(loss on real and fake)[1.154,1.155], d(acc on real and fake)[41.721,41.623], g(loss)[0.348]\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      ">213, c(loss and accuracy)[1.288,10], d(loss on real and fake)[1.154,1.155], d(acc on real and fake)[41.727,41.629], g(loss)[0.348]\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      ">214, c(loss and accuracy)[1.288,10], d(loss on real and fake)[1.154,1.155], d(acc on real and fake)[41.747,41.650], g(loss)[0.348]\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      ">215, c(loss and accuracy)[1.288,10], d(loss on real and fake)[1.154,1.156], d(acc on real and fake)[41.746,41.649], g(loss)[0.348]\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      ">216, c(loss and accuracy)[1.288,10], d(loss on real and fake)[1.154,1.156], d(acc on real and fake)[41.761,41.664], g(loss)[0.347]\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      ">217, c(loss and accuracy)[1.288,10], d(loss on real and fake)[1.154,1.156], d(acc on real and fake)[41.769,41.673], g(loss)[0.347]\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step \n",
      ">218, c(loss and accuracy)[1.288,10], d(loss on real and fake)[1.155,1.156], d(acc on real and fake)[41.779,41.683], g(loss)[0.347]\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      ">219, c(loss and accuracy)[1.289,10], d(loss on real and fake)[1.155,1.157], d(acc on real and fake)[41.789,41.694], g(loss)[0.346]\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
      ">220, c(loss and accuracy)[1.289,10], d(loss on real and fake)[1.155,1.157], d(acc on real and fake)[41.800,41.705], g(loss)[0.346]\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      ">221, c(loss and accuracy)[1.289,10], d(loss on real and fake)[1.156,1.157], d(acc on real and fake)[41.816,41.722], g(loss)[0.346]\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      ">222, c(loss and accuracy)[1.289,10], d(loss on real and fake)[1.156,1.157], d(acc on real and fake)[41.824,41.730], g(loss)[0.346]\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      ">223, c(loss and accuracy)[1.289,10], d(loss on real and fake)[1.156,1.158], d(acc on real and fake)[41.822,41.729], g(loss)[0.346]\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      ">224, c(loss and accuracy)[1.289,10], d(loss on real and fake)[1.157,1.158], d(acc on real and fake)[41.834,41.741], g(loss)[0.345]\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      ">225, c(loss and accuracy)[1.289,10], d(loss on real and fake)[1.157,1.159], d(acc on real and fake)[41.842,41.749], g(loss)[0.345]\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      ">226, c(loss and accuracy)[1.289,10], d(loss on real and fake)[1.158,1.159], d(acc on real and fake)[41.843,41.750], g(loss)[0.345]\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      ">227, c(loss and accuracy)[1.289,10], d(loss on real and fake)[1.158,1.159], d(acc on real and fake)[41.841,41.749], g(loss)[0.345]\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      ">228, c(loss and accuracy)[1.289,10], d(loss on real and fake)[1.158,1.160], d(acc on real and fake)[41.846,41.754], g(loss)[0.344]\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      ">229, c(loss and accuracy)[1.289,10], d(loss on real and fake)[1.159,1.160], d(acc on real and fake)[41.849,41.758], g(loss)[0.344]\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      ">230, c(loss and accuracy)[1.289,10], d(loss on real and fake)[1.159,1.160], d(acc on real and fake)[41.852,41.761], g(loss)[0.344]\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      ">231, c(loss and accuracy)[1.289,10], d(loss on real and fake)[1.159,1.161], d(acc on real and fake)[41.852,41.762], g(loss)[0.344]\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      ">232, c(loss and accuracy)[1.289,10], d(loss on real and fake)[1.160,1.161], d(acc on real and fake)[41.855,41.765], g(loss)[0.344]\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
      ">233, c(loss and accuracy)[1.289,11], d(loss on real and fake)[1.160,1.162], d(acc on real and fake)[41.854,41.764], g(loss)[0.343]\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      ">234, c(loss and accuracy)[1.289,11], d(loss on real and fake)[1.161,1.162], d(acc on real and fake)[41.857,41.767], g(loss)[0.343]\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
      ">235, c(loss and accuracy)[1.289,11], d(loss on real and fake)[1.161,1.162], d(acc on real and fake)[41.859,41.770], g(loss)[0.343]\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
      ">236, c(loss and accuracy)[1.289,11], d(loss on real and fake)[1.161,1.163], d(acc on real and fake)[41.868,41.780], g(loss)[0.343]\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
      ">237, c(loss and accuracy)[1.289,11], d(loss on real and fake)[1.162,1.163], d(acc on real and fake)[41.875,41.787], g(loss)[0.342]\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      ">238, c(loss and accuracy)[1.289,11], d(loss on real and fake)[1.162,1.163], d(acc on real and fake)[41.884,41.796], g(loss)[0.342]\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      ">239, c(loss and accuracy)[1.289,11], d(loss on real and fake)[1.162,1.163], d(acc on real and fake)[41.904,41.816], g(loss)[0.342]\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
      ">240, c(loss and accuracy)[1.288,11], d(loss on real and fake)[1.162,1.164], d(acc on real and fake)[41.896,41.808], g(loss)[0.342]\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
      ">241, c(loss and accuracy)[1.288,11], d(loss on real and fake)[1.163,1.164], d(acc on real and fake)[41.900,41.813], g(loss)[0.341]\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
      ">242, c(loss and accuracy)[1.288,11], d(loss on real and fake)[1.163,1.164], d(acc on real and fake)[41.899,41.812], g(loss)[0.341]\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
      ">243, c(loss and accuracy)[1.288,11], d(loss on real and fake)[1.163,1.165], d(acc on real and fake)[41.903,41.817], g(loss)[0.341]\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
      ">244, c(loss and accuracy)[1.289,11], d(loss on real and fake)[1.164,1.165], d(acc on real and fake)[41.903,41.818], g(loss)[0.341]\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
      ">245, c(loss and accuracy)[1.288,11], d(loss on real and fake)[1.164,1.165], d(acc on real and fake)[41.924,41.839], g(loss)[0.340]\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
      ">246, c(loss and accuracy)[1.288,11], d(loss on real and fake)[1.164,1.165], d(acc on real and fake)[41.937,41.852], g(loss)[0.340]\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
      ">247, c(loss and accuracy)[1.288,11], d(loss on real and fake)[1.164,1.166], d(acc on real and fake)[41.935,41.850], g(loss)[0.340]\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
      ">248, c(loss and accuracy)[1.288,11], d(loss on real and fake)[1.164,1.166], d(acc on real and fake)[41.945,41.861], g(loss)[0.340]\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
      ">249, c(loss and accuracy)[1.288,11], d(loss on real and fake)[1.165,1.166], d(acc on real and fake)[41.940,41.855], g(loss)[0.340]\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
      ">250, c(loss and accuracy)[1.288,11], d(loss on real and fake)[1.165,1.166], d(acc on real and fake)[41.954,41.870], g(loss)[0.339]\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
      ">251, c(loss and accuracy)[1.288,11], d(loss on real and fake)[1.165,1.167], d(acc on real and fake)[41.954,41.871], g(loss)[0.339]\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      ">252, c(loss and accuracy)[1.288,11], d(loss on real and fake)[1.166,1.167], d(acc on real and fake)[41.944,41.861], g(loss)[0.339]\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
      ">253, c(loss and accuracy)[1.288,11], d(loss on real and fake)[1.166,1.168], d(acc on real and fake)[41.949,41.866], g(loss)[0.339]\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
      ">254, c(loss and accuracy)[1.288,11], d(loss on real and fake)[1.166,1.168], d(acc on real and fake)[41.951,41.868], g(loss)[0.339]\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
      ">255, c(loss and accuracy)[1.288,11], d(loss on real and fake)[1.167,1.168], d(acc on real and fake)[41.953,41.871], g(loss)[0.338]\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      ">256, c(loss and accuracy)[1.288,12], d(loss on real and fake)[1.167,1.168], d(acc on real and fake)[41.955,41.873], g(loss)[0.338]\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
      ">257, c(loss and accuracy)[1.288,12], d(loss on real and fake)[1.167,1.168], d(acc on real and fake)[41.959,41.877], g(loss)[0.338]\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      ">258, c(loss and accuracy)[1.288,12], d(loss on real and fake)[1.168,1.169], d(acc on real and fake)[41.965,41.884], g(loss)[0.338]\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      ">259, c(loss and accuracy)[1.288,12], d(loss on real and fake)[1.168,1.169], d(acc on real and fake)[41.961,41.880], g(loss)[0.338]\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      ">260, c(loss and accuracy)[1.287,12], d(loss on real and fake)[1.168,1.169], d(acc on real and fake)[41.961,41.881], g(loss)[0.338]\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      ">261, c(loss and accuracy)[1.287,12], d(loss on real and fake)[1.168,1.169], d(acc on real and fake)[41.971,41.891], g(loss)[0.337]\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      ">262, c(loss and accuracy)[1.287,12], d(loss on real and fake)[1.168,1.170], d(acc on real and fake)[41.977,41.897], g(loss)[0.337]\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      ">263, c(loss and accuracy)[1.287,12], d(loss on real and fake)[1.169,1.170], d(acc on real and fake)[41.985,41.905], g(loss)[0.337]\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      ">264, c(loss and accuracy)[1.287,12], d(loss on real and fake)[1.169,1.170], d(acc on real and fake)[41.992,41.913], g(loss)[0.337]\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      ">265, c(loss and accuracy)[1.286,12], d(loss on real and fake)[1.169,1.170], d(acc on real and fake)[41.983,41.904], g(loss)[0.337]\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      ">266, c(loss and accuracy)[1.286,12], d(loss on real and fake)[1.169,1.171], d(acc on real and fake)[41.985,41.906], g(loss)[0.337]\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      ">267, c(loss and accuracy)[1.286,12], d(loss on real and fake)[1.170,1.171], d(acc on real and fake)[41.987,41.908], g(loss)[0.337]\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      ">268, c(loss and accuracy)[1.286,12], d(loss on real and fake)[1.170,1.171], d(acc on real and fake)[41.985,41.907], g(loss)[0.336]\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      ">269, c(loss and accuracy)[1.286,12], d(loss on real and fake)[1.170,1.171], d(acc on real and fake)[41.987,41.909], g(loss)[0.336]\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      ">270, c(loss and accuracy)[1.286,12], d(loss on real and fake)[1.170,1.171], d(acc on real and fake)[41.991,41.913], g(loss)[0.336]\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      ">271, c(loss and accuracy)[1.285,12], d(loss on real and fake)[1.170,1.171], d(acc on real and fake)[42.000,41.923], g(loss)[0.336]\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      ">272, c(loss and accuracy)[1.285,12], d(loss on real and fake)[1.170,1.172], d(acc on real and fake)[41.996,41.919], g(loss)[0.336]\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      ">273, c(loss and accuracy)[1.285,12], d(loss on real and fake)[1.171,1.172], d(acc on real and fake)[42.006,41.929], g(loss)[0.336]\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      ">274, c(loss and accuracy)[1.285,13], d(loss on real and fake)[1.171,1.172], d(acc on real and fake)[41.987,41.911], g(loss)[0.336]\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      ">275, c(loss and accuracy)[1.285,13], d(loss on real and fake)[1.171,1.173], d(acc on real and fake)[41.978,41.902], g(loss)[0.336]\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      ">276, c(loss and accuracy)[1.284,13], d(loss on real and fake)[1.172,1.173], d(acc on real and fake)[41.982,41.906], g(loss)[0.336]\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      ">277, c(loss and accuracy)[1.284,13], d(loss on real and fake)[1.172,1.173], d(acc on real and fake)[41.986,41.910], g(loss)[0.336]\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      ">278, c(loss and accuracy)[1.284,13], d(loss on real and fake)[1.172,1.173], d(acc on real and fake)[41.995,41.919], g(loss)[0.335]\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      ">279, c(loss and accuracy)[1.284,13], d(loss on real and fake)[1.172,1.173], d(acc on real and fake)[42.007,41.932], g(loss)[0.335]\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      ">280, c(loss and accuracy)[1.283,13], d(loss on real and fake)[1.172,1.173], d(acc on real and fake)[42.014,41.939], g(loss)[0.335]\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      ">281, c(loss and accuracy)[1.283,13], d(loss on real and fake)[1.172,1.173], d(acc on real and fake)[42.012,41.938], g(loss)[0.335]\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      ">282, c(loss and accuracy)[1.283,13], d(loss on real and fake)[1.172,1.173], d(acc on real and fake)[42.016,41.941], g(loss)[0.335]\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      ">283, c(loss and accuracy)[1.283,13], d(loss on real and fake)[1.172,1.173], d(acc on real and fake)[42.027,41.952], g(loss)[0.335]\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      ">284, c(loss and accuracy)[1.283,13], d(loss on real and fake)[1.172,1.174], d(acc on real and fake)[42.034,41.960], g(loss)[0.335]\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      ">285, c(loss and accuracy)[1.282,13], d(loss on real and fake)[1.173,1.174], d(acc on real and fake)[42.037,41.963], g(loss)[0.335]\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      ">286, c(loss and accuracy)[1.282,13], d(loss on real and fake)[1.173,1.174], d(acc on real and fake)[42.035,41.962], g(loss)[0.335]\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      ">287, c(loss and accuracy)[1.282,13], d(loss on real and fake)[1.173,1.174], d(acc on real and fake)[42.037,41.963], g(loss)[0.335]\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      ">288, c(loss and accuracy)[1.282,13], d(loss on real and fake)[1.174,1.175], d(acc on real and fake)[42.042,41.969], g(loss)[0.334]\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      ">289, c(loss and accuracy)[1.281,14], d(loss on real and fake)[1.174,1.175], d(acc on real and fake)[42.047,41.974], g(loss)[0.334]\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      ">290, c(loss and accuracy)[1.281,14], d(loss on real and fake)[1.174,1.175], d(acc on real and fake)[42.048,41.976], g(loss)[0.334]\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      ">291, c(loss and accuracy)[1.281,14], d(loss on real and fake)[1.174,1.175], d(acc on real and fake)[42.052,41.979], g(loss)[0.334]\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      ">292, c(loss and accuracy)[1.281,14], d(loss on real and fake)[1.175,1.176], d(acc on real and fake)[42.045,41.973], g(loss)[0.334]\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      ">293, c(loss and accuracy)[1.280,14], d(loss on real and fake)[1.175,1.176], d(acc on real and fake)[42.046,41.974], g(loss)[0.334]\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      ">294, c(loss and accuracy)[1.280,14], d(loss on real and fake)[1.175,1.176], d(acc on real and fake)[42.051,41.980], g(loss)[0.334]\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      ">295, c(loss and accuracy)[1.280,14], d(loss on real and fake)[1.175,1.176], d(acc on real and fake)[42.056,41.985], g(loss)[0.334]\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      ">296, c(loss and accuracy)[1.280,14], d(loss on real and fake)[1.175,1.177], d(acc on real and fake)[42.054,41.983], g(loss)[0.333]\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      ">297, c(loss and accuracy)[1.279,14], d(loss on real and fake)[1.176,1.177], d(acc on real and fake)[42.049,41.978], g(loss)[0.333]\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      ">298, c(loss and accuracy)[1.279,14], d(loss on real and fake)[1.176,1.177], d(acc on real and fake)[42.054,41.983], g(loss)[0.333]\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      ">299, c(loss and accuracy)[1.279,14], d(loss on real and fake)[1.176,1.177], d(acc on real and fake)[42.069,41.998], g(loss)[0.333]\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      ">300, c(loss and accuracy)[1.278,14], d(loss on real and fake)[1.176,1.177], d(acc on real and fake)[42.072,42.002], g(loss)[0.333]\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      "Classifier Accuracy: 32.049%\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      ">301, c(loss and accuracy)[0.989,32], d(loss on real and fake)[1.176,1.177], d(acc on real and fake)[42.082,42.012], g(loss)[0.333]\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      ">302, c(loss and accuracy)[0.989,32], d(loss on real and fake)[1.176,1.177], d(acc on real and fake)[42.085,42.015], g(loss)[0.333]\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      ">303, c(loss and accuracy)[0.989,32], d(loss on real and fake)[1.176,1.177], d(acc on real and fake)[42.091,42.021], g(loss)[0.333]\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      ">304, c(loss and accuracy)[0.989,32], d(loss on real and fake)[1.176,1.177], d(acc on real and fake)[42.089,42.020], g(loss)[0.333]\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      ">305, c(loss and accuracy)[0.989,32], d(loss on real and fake)[1.176,1.177], d(acc on real and fake)[42.087,42.018], g(loss)[0.333]\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      ">306, c(loss and accuracy)[0.989,32], d(loss on real and fake)[1.176,1.177], d(acc on real and fake)[42.098,42.029], g(loss)[0.332]\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      ">307, c(loss and accuracy)[0.989,32], d(loss on real and fake)[1.176,1.177], d(acc on real and fake)[42.103,42.034], g(loss)[0.332]\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      ">308, c(loss and accuracy)[0.990,32], d(loss on real and fake)[1.176,1.177], d(acc on real and fake)[42.109,42.041], g(loss)[0.332]\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      ">309, c(loss and accuracy)[0.990,32], d(loss on real and fake)[1.177,1.178], d(acc on real and fake)[42.105,42.037], g(loss)[0.332]\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      ">310, c(loss and accuracy)[0.989,32], d(loss on real and fake)[1.177,1.178], d(acc on real and fake)[42.097,42.029], g(loss)[0.332]\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      ">311, c(loss and accuracy)[0.989,32], d(loss on real and fake)[1.177,1.178], d(acc on real and fake)[42.106,42.039], g(loss)[0.332]\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      ">312, c(loss and accuracy)[0.989,32], d(loss on real and fake)[1.177,1.178], d(acc on real and fake)[42.125,42.058], g(loss)[0.332]\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
      ">313, c(loss and accuracy)[0.989,32], d(loss on real and fake)[1.177,1.178], d(acc on real and fake)[42.125,42.058], g(loss)[0.332]\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      ">314, c(loss and accuracy)[0.989,32], d(loss on real and fake)[1.177,1.178], d(acc on real and fake)[42.123,42.056], g(loss)[0.332]\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      ">315, c(loss and accuracy)[0.990,32], d(loss on real and fake)[1.177,1.179], d(acc on real and fake)[42.129,42.062], g(loss)[0.332]\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      ">316, c(loss and accuracy)[0.990,33], d(loss on real and fake)[1.178,1.179], d(acc on real and fake)[42.135,42.068], g(loss)[0.332]\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      ">317, c(loss and accuracy)[0.990,33], d(loss on real and fake)[1.178,1.179], d(acc on real and fake)[42.123,42.057], g(loss)[0.331]\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      ">318, c(loss and accuracy)[0.990,33], d(loss on real and fake)[1.179,1.180], d(acc on real and fake)[42.115,42.049], g(loss)[0.331]\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      ">319, c(loss and accuracy)[0.990,33], d(loss on real and fake)[1.179,1.180], d(acc on real and fake)[42.119,42.053], g(loss)[0.331]\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      ">320, c(loss and accuracy)[0.990,33], d(loss on real and fake)[1.179,1.180], d(acc on real and fake)[42.117,42.052], g(loss)[0.331]\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      ">321, c(loss and accuracy)[0.990,33], d(loss on real and fake)[1.180,1.181], d(acc on real and fake)[42.114,42.048], g(loss)[0.331]\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      ">322, c(loss and accuracy)[0.990,33], d(loss on real and fake)[1.180,1.181], d(acc on real and fake)[42.115,42.050], g(loss)[0.331]\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      ">323, c(loss and accuracy)[0.990,33], d(loss on real and fake)[1.180,1.181], d(acc on real and fake)[42.118,42.053], g(loss)[0.331]\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      ">324, c(loss and accuracy)[0.990,33], d(loss on real and fake)[1.180,1.182], d(acc on real and fake)[42.119,42.054], g(loss)[0.331]\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      ">325, c(loss and accuracy)[0.990,33], d(loss on real and fake)[1.181,1.182], d(acc on real and fake)[42.122,42.057], g(loss)[0.331]\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
      ">326, c(loss and accuracy)[0.990,33], d(loss on real and fake)[1.181,1.182], d(acc on real and fake)[42.126,42.061], g(loss)[0.330]\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      ">327, c(loss and accuracy)[0.990,33], d(loss on real and fake)[1.181,1.182], d(acc on real and fake)[42.133,42.069], g(loss)[0.330]\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
      ">328, c(loss and accuracy)[0.990,33], d(loss on real and fake)[1.181,1.182], d(acc on real and fake)[42.139,42.075], g(loss)[0.330]\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      ">329, c(loss and accuracy)[0.990,33], d(loss on real and fake)[1.182,1.183], d(acc on real and fake)[42.131,42.067], g(loss)[0.330]\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
      ">330, c(loss and accuracy)[0.990,33], d(loss on real and fake)[1.182,1.183], d(acc on real and fake)[42.137,42.073], g(loss)[0.330]\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
      ">331, c(loss and accuracy)[0.990,33], d(loss on real and fake)[1.182,1.183], d(acc on real and fake)[42.135,42.071], g(loss)[0.330]\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
      ">332, c(loss and accuracy)[0.990,33], d(loss on real and fake)[1.182,1.183], d(acc on real and fake)[42.142,42.078], g(loss)[0.330]\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
      ">333, c(loss and accuracy)[0.990,33], d(loss on real and fake)[1.182,1.184], d(acc on real and fake)[42.143,42.080], g(loss)[0.329]\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
      ">334, c(loss and accuracy)[0.990,33], d(loss on real and fake)[1.183,1.184], d(acc on real and fake)[42.153,42.090], g(loss)[0.329]\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
      ">335, c(loss and accuracy)[0.990,33], d(loss on real and fake)[1.183,1.184], d(acc on real and fake)[42.157,42.094], g(loss)[0.329]\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
      ">336, c(loss and accuracy)[0.990,33], d(loss on real and fake)[1.183,1.184], d(acc on real and fake)[42.156,42.094], g(loss)[0.329]\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
      ">337, c(loss and accuracy)[0.990,33], d(loss on real and fake)[1.183,1.184], d(acc on real and fake)[42.158,42.095], g(loss)[0.329]\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
      ">338, c(loss and accuracy)[0.990,33], d(loss on real and fake)[1.184,1.185], d(acc on real and fake)[42.157,42.095], g(loss)[0.329]\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
      ">339, c(loss and accuracy)[0.990,33], d(loss on real and fake)[1.184,1.185], d(acc on real and fake)[42.161,42.099], g(loss)[0.329]\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
      ">340, c(loss and accuracy)[0.990,33], d(loss on real and fake)[1.184,1.185], d(acc on real and fake)[42.168,42.106], g(loss)[0.329]\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
      ">341, c(loss and accuracy)[0.990,33], d(loss on real and fake)[1.184,1.185], d(acc on real and fake)[42.175,42.113], g(loss)[0.329]\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
      ">342, c(loss and accuracy)[0.990,33], d(loss on real and fake)[1.184,1.185], d(acc on real and fake)[42.184,42.123], g(loss)[0.328]\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
      ">343, c(loss and accuracy)[0.990,33], d(loss on real and fake)[1.185,1.186], d(acc on real and fake)[42.188,42.127], g(loss)[0.328]\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
      ">344, c(loss and accuracy)[0.990,33], d(loss on real and fake)[1.185,1.186], d(acc on real and fake)[42.189,42.128], g(loss)[0.328]\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
      ">345, c(loss and accuracy)[0.990,34], d(loss on real and fake)[1.185,1.186], d(acc on real and fake)[42.192,42.130], g(loss)[0.328]\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
      ">346, c(loss and accuracy)[0.990,34], d(loss on real and fake)[1.186,1.187], d(acc on real and fake)[42.187,42.126], g(loss)[0.328]\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
      ">347, c(loss and accuracy)[0.990,34], d(loss on real and fake)[1.186,1.187], d(acc on real and fake)[42.188,42.127], g(loss)[0.328]\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
      ">348, c(loss and accuracy)[0.990,34], d(loss on real and fake)[1.186,1.187], d(acc on real and fake)[42.193,42.132], g(loss)[0.328]\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
      ">349, c(loss and accuracy)[0.990,34], d(loss on real and fake)[1.187,1.188], d(acc on real and fake)[42.189,42.129], g(loss)[0.327]\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
      ">350, c(loss and accuracy)[0.990,34], d(loss on real and fake)[1.187,1.188], d(acc on real and fake)[42.190,42.130], g(loss)[0.327]\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
      ">351, c(loss and accuracy)[0.990,34], d(loss on real and fake)[1.187,1.189], d(acc on real and fake)[42.191,42.131], g(loss)[0.327]\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
      ">352, c(loss and accuracy)[0.990,34], d(loss on real and fake)[1.188,1.189], d(acc on real and fake)[42.196,42.136], g(loss)[0.327]\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
      ">353, c(loss and accuracy)[0.990,34], d(loss on real and fake)[1.188,1.189], d(acc on real and fake)[42.203,42.143], g(loss)[0.327]\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step \n",
      ">354, c(loss and accuracy)[0.990,34], d(loss on real and fake)[1.188,1.189], d(acc on real and fake)[42.211,42.151], g(loss)[0.327]\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
      ">355, c(loss and accuracy)[0.990,34], d(loss on real and fake)[1.188,1.189], d(acc on real and fake)[42.216,42.156], g(loss)[0.327]\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
      ">356, c(loss and accuracy)[0.990,34], d(loss on real and fake)[1.189,1.190], d(acc on real and fake)[42.217,42.157], g(loss)[0.326]\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
      ">357, c(loss and accuracy)[0.990,34], d(loss on real and fake)[1.189,1.190], d(acc on real and fake)[42.213,42.154], g(loss)[0.326]\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
      ">358, c(loss and accuracy)[0.990,34], d(loss on real and fake)[1.189,1.191], d(acc on real and fake)[42.210,42.151], g(loss)[0.326]\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      ">359, c(loss and accuracy)[0.990,34], d(loss on real and fake)[1.190,1.191], d(acc on real and fake)[42.213,42.155], g(loss)[0.326]\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      ">360, c(loss and accuracy)[0.990,34], d(loss on real and fake)[1.190,1.191], d(acc on real and fake)[42.216,42.157], g(loss)[0.326]\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      ">361, c(loss and accuracy)[0.990,34], d(loss on real and fake)[1.190,1.191], d(acc on real and fake)[42.208,42.150], g(loss)[0.325]\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
      ">362, c(loss and accuracy)[0.990,34], d(loss on real and fake)[1.191,1.192], d(acc on real and fake)[42.201,42.142], g(loss)[0.325]\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      ">363, c(loss and accuracy)[0.990,34], d(loss on real and fake)[1.191,1.192], d(acc on real and fake)[42.200,42.142], g(loss)[0.325]\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      ">364, c(loss and accuracy)[0.990,34], d(loss on real and fake)[1.191,1.192], d(acc on real and fake)[42.205,42.147], g(loss)[0.325]\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      ">365, c(loss and accuracy)[0.990,34], d(loss on real and fake)[1.191,1.192], d(acc on real and fake)[42.211,42.153], g(loss)[0.325]\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      ">366, c(loss and accuracy)[0.990,34], d(loss on real and fake)[1.191,1.192], d(acc on real and fake)[42.209,42.152], g(loss)[0.325]\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      ">367, c(loss and accuracy)[0.990,34], d(loss on real and fake)[1.192,1.193], d(acc on real and fake)[42.207,42.150], g(loss)[0.325]\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
      ">368, c(loss and accuracy)[0.990,34], d(loss on real and fake)[1.192,1.193], d(acc on real and fake)[42.214,42.156], g(loss)[0.325]\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      ">369, c(loss and accuracy)[0.989,35], d(loss on real and fake)[1.192,1.193], d(acc on real and fake)[42.220,42.163], g(loss)[0.325]\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      ">370, c(loss and accuracy)[0.989,35], d(loss on real and fake)[1.192,1.193], d(acc on real and fake)[42.218,42.161], g(loss)[0.325]\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
      ">371, c(loss and accuracy)[0.989,35], d(loss on real and fake)[1.192,1.193], d(acc on real and fake)[42.211,42.154], g(loss)[0.325]\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      ">372, c(loss and accuracy)[0.989,35], d(loss on real and fake)[1.192,1.193], d(acc on real and fake)[42.217,42.160], g(loss)[0.325]\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      ">373, c(loss and accuracy)[0.989,35], d(loss on real and fake)[1.192,1.193], d(acc on real and fake)[42.221,42.165], g(loss)[0.325]\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      ">374, c(loss and accuracy)[0.989,35], d(loss on real and fake)[1.192,1.193], d(acc on real and fake)[42.225,42.168], g(loss)[0.325]\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      ">375, c(loss and accuracy)[0.989,35], d(loss on real and fake)[1.192,1.193], d(acc on real and fake)[42.230,42.173], g(loss)[0.325]\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
      ">376, c(loss and accuracy)[0.989,35], d(loss on real and fake)[1.193,1.193], d(acc on real and fake)[42.224,42.168], g(loss)[0.324]\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      ">377, c(loss and accuracy)[0.989,35], d(loss on real and fake)[1.193,1.194], d(acc on real and fake)[42.222,42.166], g(loss)[0.324]\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
      ">378, c(loss and accuracy)[0.988,35], d(loss on real and fake)[1.193,1.194], d(acc on real and fake)[42.221,42.165], g(loss)[0.324]\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
      ">379, c(loss and accuracy)[0.988,35], d(loss on real and fake)[1.193,1.194], d(acc on real and fake)[42.226,42.170], g(loss)[0.324]\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
      ">380, c(loss and accuracy)[0.988,35], d(loss on real and fake)[1.193,1.194], d(acc on real and fake)[42.236,42.180], g(loss)[0.324]\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
      ">381, c(loss and accuracy)[0.988,35], d(loss on real and fake)[1.193,1.194], d(acc on real and fake)[42.235,42.180], g(loss)[0.324]\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
      ">382, c(loss and accuracy)[0.988,35], d(loss on real and fake)[1.193,1.194], d(acc on real and fake)[42.239,42.183], g(loss)[0.324]\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
      ">383, c(loss and accuracy)[0.988,36], d(loss on real and fake)[1.193,1.194], d(acc on real and fake)[42.233,42.178], g(loss)[0.324]\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
      ">384, c(loss and accuracy)[0.988,36], d(loss on real and fake)[1.193,1.194], d(acc on real and fake)[42.240,42.185], g(loss)[0.324]\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
      ">385, c(loss and accuracy)[0.988,36], d(loss on real and fake)[1.194,1.195], d(acc on real and fake)[42.237,42.182], g(loss)[0.324]\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
      ">386, c(loss and accuracy)[0.988,36], d(loss on real and fake)[1.194,1.195], d(acc on real and fake)[42.240,42.185], g(loss)[0.324]\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
      ">387, c(loss and accuracy)[0.987,36], d(loss on real and fake)[1.194,1.195], d(acc on real and fake)[42.241,42.186], g(loss)[0.324]\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
      ">388, c(loss and accuracy)[0.987,36], d(loss on real and fake)[1.194,1.195], d(acc on real and fake)[42.245,42.191], g(loss)[0.324]\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
      ">389, c(loss and accuracy)[0.987,36], d(loss on real and fake)[1.194,1.195], d(acc on real and fake)[42.246,42.192], g(loss)[0.324]\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
      ">390, c(loss and accuracy)[0.987,36], d(loss on real and fake)[1.194,1.195], d(acc on real and fake)[42.252,42.197], g(loss)[0.323]\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
      ">391, c(loss and accuracy)[0.987,36], d(loss on real and fake)[1.194,1.195], d(acc on real and fake)[42.262,42.208], g(loss)[0.323]\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
      ">392, c(loss and accuracy)[0.987,36], d(loss on real and fake)[1.195,1.196], d(acc on real and fake)[42.261,42.207], g(loss)[0.323]\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
      ">393, c(loss and accuracy)[0.986,36], d(loss on real and fake)[1.195,1.196], d(acc on real and fake)[42.262,42.209], g(loss)[0.323]\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      ">394, c(loss and accuracy)[0.986,36], d(loss on real and fake)[1.195,1.196], d(acc on real and fake)[42.262,42.208], g(loss)[0.323]\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
      ">395, c(loss and accuracy)[0.986,36], d(loss on real and fake)[1.195,1.196], d(acc on real and fake)[42.267,42.214], g(loss)[0.323]\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      ">396, c(loss and accuracy)[0.986,36], d(loss on real and fake)[1.196,1.196], d(acc on real and fake)[42.272,42.218], g(loss)[0.323]\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      ">397, c(loss and accuracy)[0.986,36], d(loss on real and fake)[1.196,1.197], d(acc on real and fake)[42.271,42.218], g(loss)[0.323]\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      ">398, c(loss and accuracy)[0.986,37], d(loss on real and fake)[1.196,1.197], d(acc on real and fake)[42.283,42.230], g(loss)[0.323]\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      ">399, c(loss and accuracy)[0.986,37], d(loss on real and fake)[1.196,1.197], d(acc on real and fake)[42.291,42.238], g(loss)[0.322]\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      ">400, c(loss and accuracy)[0.985,37], d(loss on real and fake)[1.196,1.197], d(acc on real and fake)[42.292,42.239], g(loss)[0.322]\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      "Classifier Accuracy: 69.043%\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      ">401, c(loss and accuracy)[0.826,69], d(loss on real and fake)[1.196,1.197], d(acc on real and fake)[42.295,42.242], g(loss)[0.322]\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      ">402, c(loss and accuracy)[0.826,69], d(loss on real and fake)[1.196,1.197], d(acc on real and fake)[42.290,42.238], g(loss)[0.322]\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      ">403, c(loss and accuracy)[0.826,69], d(loss on real and fake)[1.197,1.198], d(acc on real and fake)[42.296,42.243], g(loss)[0.322]\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      ">404, c(loss and accuracy)[0.826,69], d(loss on real and fake)[1.197,1.198], d(acc on real and fake)[42.297,42.245], g(loss)[0.322]\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      ">405, c(loss and accuracy)[0.826,69], d(loss on real and fake)[1.197,1.198], d(acc on real and fake)[42.300,42.248], g(loss)[0.322]\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      ">406, c(loss and accuracy)[0.826,69], d(loss on real and fake)[1.197,1.198], d(acc on real and fake)[42.292,42.240], g(loss)[0.321]\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      ">407, c(loss and accuracy)[0.827,69], d(loss on real and fake)[1.198,1.199], d(acc on real and fake)[42.292,42.240], g(loss)[0.321]\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      ">408, c(loss and accuracy)[0.826,69], d(loss on real and fake)[1.198,1.199], d(acc on real and fake)[42.294,42.243], g(loss)[0.321]\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      ">409, c(loss and accuracy)[0.826,69], d(loss on real and fake)[1.198,1.199], d(acc on real and fake)[42.297,42.246], g(loss)[0.321]\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      ">410, c(loss and accuracy)[0.826,69], d(loss on real and fake)[1.198,1.199], d(acc on real and fake)[42.298,42.246], g(loss)[0.321]\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      ">411, c(loss and accuracy)[0.826,69], d(loss on real and fake)[1.199,1.199], d(acc on real and fake)[42.295,42.243], g(loss)[0.321]\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
      ">412, c(loss and accuracy)[0.826,69], d(loss on real and fake)[1.199,1.199], d(acc on real and fake)[42.301,42.250], g(loss)[0.321]\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      ">413, c(loss and accuracy)[0.826,69], d(loss on real and fake)[1.199,1.200], d(acc on real and fake)[42.298,42.247], g(loss)[0.321]\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      ">414, c(loss and accuracy)[0.826,69], d(loss on real and fake)[1.199,1.200], d(acc on real and fake)[42.295,42.244], g(loss)[0.321]\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      ">415, c(loss and accuracy)[0.826,69], d(loss on real and fake)[1.199,1.200], d(acc on real and fake)[42.297,42.246], g(loss)[0.321]\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      ">416, c(loss and accuracy)[0.826,69], d(loss on real and fake)[1.200,1.201], d(acc on real and fake)[42.292,42.242], g(loss)[0.321]\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      ">417, c(loss and accuracy)[0.826,69], d(loss on real and fake)[1.200,1.201], d(acc on real and fake)[42.287,42.236], g(loss)[0.320]\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      ">418, c(loss and accuracy)[0.826,69], d(loss on real and fake)[1.200,1.201], d(acc on real and fake)[42.291,42.240], g(loss)[0.320]\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      ">419, c(loss and accuracy)[0.826,69], d(loss on real and fake)[1.200,1.201], d(acc on real and fake)[42.287,42.236], g(loss)[0.320]\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      ">420, c(loss and accuracy)[0.826,69], d(loss on real and fake)[1.201,1.202], d(acc on real and fake)[42.280,42.230], g(loss)[0.320]\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      ">421, c(loss and accuracy)[0.826,69], d(loss on real and fake)[1.201,1.202], d(acc on real and fake)[42.285,42.235], g(loss)[0.320]\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
      ">422, c(loss and accuracy)[0.826,69], d(loss on real and fake)[1.201,1.202], d(acc on real and fake)[42.284,42.233], g(loss)[0.320]\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      ">423, c(loss and accuracy)[0.826,69], d(loss on real and fake)[1.201,1.202], d(acc on real and fake)[42.285,42.235], g(loss)[0.320]\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      ">424, c(loss and accuracy)[0.826,69], d(loss on real and fake)[1.202,1.202], d(acc on real and fake)[42.288,42.238], g(loss)[0.320]\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      ">425, c(loss and accuracy)[0.826,69], d(loss on real and fake)[1.202,1.203], d(acc on real and fake)[42.290,42.240], g(loss)[0.319]\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
      ">426, c(loss and accuracy)[0.826,69], d(loss on real and fake)[1.202,1.203], d(acc on real and fake)[42.293,42.243], g(loss)[0.319]\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
      ">427, c(loss and accuracy)[0.826,69], d(loss on real and fake)[1.202,1.203], d(acc on real and fake)[42.291,42.241], g(loss)[0.319]\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
      ">428, c(loss and accuracy)[0.825,69], d(loss on real and fake)[1.202,1.203], d(acc on real and fake)[42.291,42.242], g(loss)[0.319]\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
      ">429, c(loss and accuracy)[0.825,69], d(loss on real and fake)[1.203,1.203], d(acc on real and fake)[42.293,42.244], g(loss)[0.319]\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
      ">430, c(loss and accuracy)[0.825,69], d(loss on real and fake)[1.203,1.204], d(acc on real and fake)[42.296,42.247], g(loss)[0.319]\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
      ">431, c(loss and accuracy)[0.825,69], d(loss on real and fake)[1.203,1.204], d(acc on real and fake)[42.289,42.240], g(loss)[0.319]\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
      ">432, c(loss and accuracy)[0.825,68], d(loss on real and fake)[1.203,1.204], d(acc on real and fake)[42.290,42.241], g(loss)[0.319]\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
      ">433, c(loss and accuracy)[0.825,68], d(loss on real and fake)[1.203,1.204], d(acc on real and fake)[42.288,42.239], g(loss)[0.319]\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
      ">434, c(loss and accuracy)[0.825,69], d(loss on real and fake)[1.203,1.204], d(acc on real and fake)[42.293,42.244], g(loss)[0.319]\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
      ">435, c(loss and accuracy)[0.825,69], d(loss on real and fake)[1.204,1.204], d(acc on real and fake)[42.293,42.245], g(loss)[0.318]\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
      ">436, c(loss and accuracy)[0.825,68], d(loss on real and fake)[1.204,1.204], d(acc on real and fake)[42.301,42.252], g(loss)[0.318]\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
      ">437, c(loss and accuracy)[0.825,69], d(loss on real and fake)[1.204,1.205], d(acc on real and fake)[42.308,42.260], g(loss)[0.318]\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
      ">438, c(loss and accuracy)[0.825,68], d(loss on real and fake)[1.204,1.205], d(acc on real and fake)[42.311,42.263], g(loss)[0.318]\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
      ">439, c(loss and accuracy)[0.825,68], d(loss on real and fake)[1.204,1.205], d(acc on real and fake)[42.315,42.267], g(loss)[0.318]\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
      ">440, c(loss and accuracy)[0.825,68], d(loss on real and fake)[1.204,1.205], d(acc on real and fake)[42.316,42.268], g(loss)[0.318]\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
      ">441, c(loss and accuracy)[0.825,68], d(loss on real and fake)[1.204,1.205], d(acc on real and fake)[42.320,42.272], g(loss)[0.318]\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
      ">442, c(loss and accuracy)[0.825,68], d(loss on real and fake)[1.204,1.205], d(acc on real and fake)[42.330,42.282], g(loss)[0.318]\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
      ">443, c(loss and accuracy)[0.825,68], d(loss on real and fake)[1.204,1.205], d(acc on real and fake)[42.333,42.286], g(loss)[0.317]\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
      ">444, c(loss and accuracy)[0.825,68], d(loss on real and fake)[1.204,1.205], d(acc on real and fake)[42.331,42.284], g(loss)[0.317]\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
      ">445, c(loss and accuracy)[0.825,68], d(loss on real and fake)[1.204,1.205], d(acc on real and fake)[42.336,42.289], g(loss)[0.317]\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
      ">446, c(loss and accuracy)[0.825,68], d(loss on real and fake)[1.205,1.205], d(acc on real and fake)[42.333,42.286], g(loss)[0.317]\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
      ">447, c(loss and accuracy)[0.826,68], d(loss on real and fake)[1.205,1.206], d(acc on real and fake)[42.336,42.289], g(loss)[0.317]\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
      ">448, c(loss and accuracy)[0.826,68], d(loss on real and fake)[1.205,1.206], d(acc on real and fake)[42.340,42.292], g(loss)[0.317]\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
      ">449, c(loss and accuracy)[0.826,68], d(loss on real and fake)[1.205,1.206], d(acc on real and fake)[42.342,42.295], g(loss)[0.317]\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
      ">450, c(loss and accuracy)[0.826,68], d(loss on real and fake)[1.205,1.206], d(acc on real and fake)[42.345,42.298], g(loss)[0.317]\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
      ">451, c(loss and accuracy)[0.825,68], d(loss on real and fake)[1.205,1.206], d(acc on real and fake)[42.345,42.298], g(loss)[0.317]\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
      ">452, c(loss and accuracy)[0.825,68], d(loss on real and fake)[1.205,1.206], d(acc on real and fake)[42.343,42.296], g(loss)[0.317]\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
      ">453, c(loss and accuracy)[0.825,68], d(loss on real and fake)[1.205,1.206], d(acc on real and fake)[42.350,42.304], g(loss)[0.317]\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
      ">454, c(loss and accuracy)[0.825,68], d(loss on real and fake)[1.206,1.206], d(acc on real and fake)[42.354,42.307], g(loss)[0.316]\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
      ">455, c(loss and accuracy)[0.825,68], d(loss on real and fake)[1.206,1.206], d(acc on real and fake)[42.363,42.316], g(loss)[0.316]\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
      ">456, c(loss and accuracy)[0.825,68], d(loss on real and fake)[1.205,1.206], d(acc on real and fake)[42.373,42.327], g(loss)[0.316]\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
      ">457, c(loss and accuracy)[0.825,68], d(loss on real and fake)[1.206,1.207], d(acc on real and fake)[42.371,42.325], g(loss)[0.316]\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
      ">458, c(loss and accuracy)[0.825,68], d(loss on real and fake)[1.206,1.207], d(acc on real and fake)[42.372,42.325], g(loss)[0.316]\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
      ">459, c(loss and accuracy)[0.825,68], d(loss on real and fake)[1.206,1.207], d(acc on real and fake)[42.376,42.330], g(loss)[0.316]\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
      ">460, c(loss and accuracy)[0.825,68], d(loss on real and fake)[1.206,1.207], d(acc on real and fake)[42.376,42.330], g(loss)[0.316]\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
      ">461, c(loss and accuracy)[0.825,68], d(loss on real and fake)[1.207,1.207], d(acc on real and fake)[42.371,42.325], g(loss)[0.316]\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
      ">462, c(loss and accuracy)[0.825,68], d(loss on real and fake)[1.207,1.207], d(acc on real and fake)[42.374,42.328], g(loss)[0.316]\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
      ">463, c(loss and accuracy)[0.825,68], d(loss on real and fake)[1.207,1.208], d(acc on real and fake)[42.377,42.332], g(loss)[0.316]\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
      ">464, c(loss and accuracy)[0.825,68], d(loss on real and fake)[1.207,1.208], d(acc on real and fake)[42.372,42.327], g(loss)[0.315]\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
      ">465, c(loss and accuracy)[0.825,68], d(loss on real and fake)[1.207,1.208], d(acc on real and fake)[42.371,42.326], g(loss)[0.315]\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
      ">466, c(loss and accuracy)[0.825,68], d(loss on real and fake)[1.208,1.208], d(acc on real and fake)[42.374,42.328], g(loss)[0.315]\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
      ">467, c(loss and accuracy)[0.825,69], d(loss on real and fake)[1.208,1.208], d(acc on real and fake)[42.376,42.331], g(loss)[0.315]\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
      ">468, c(loss and accuracy)[0.824,69], d(loss on real and fake)[1.208,1.209], d(acc on real and fake)[42.376,42.331], g(loss)[0.315]\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
      ">469, c(loss and accuracy)[0.824,69], d(loss on real and fake)[1.208,1.209], d(acc on real and fake)[42.376,42.330], g(loss)[0.315]\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
      ">470, c(loss and accuracy)[0.824,69], d(loss on real and fake)[1.208,1.209], d(acc on real and fake)[42.376,42.331], g(loss)[0.315]\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
      ">471, c(loss and accuracy)[0.824,69], d(loss on real and fake)[1.209,1.209], d(acc on real and fake)[42.370,42.325], g(loss)[0.315]\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
      ">472, c(loss and accuracy)[0.824,69], d(loss on real and fake)[1.209,1.210], d(acc on real and fake)[42.372,42.327], g(loss)[0.315]\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
      ">473, c(loss and accuracy)[0.824,69], d(loss on real and fake)[1.209,1.210], d(acc on real and fake)[42.362,42.317], g(loss)[0.315]\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
      ">474, c(loss and accuracy)[0.824,69], d(loss on real and fake)[1.209,1.210], d(acc on real and fake)[42.359,42.314], g(loss)[0.314]\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
      ">475, c(loss and accuracy)[0.823,69], d(loss on real and fake)[1.209,1.210], d(acc on real and fake)[42.365,42.320], g(loss)[0.314]\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
      ">476, c(loss and accuracy)[0.823,69], d(loss on real and fake)[1.210,1.210], d(acc on real and fake)[42.364,42.319], g(loss)[0.314]\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
      ">477, c(loss and accuracy)[0.823,69], d(loss on real and fake)[1.210,1.211], d(acc on real and fake)[42.361,42.317], g(loss)[0.314]\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
      ">478, c(loss and accuracy)[0.823,69], d(loss on real and fake)[1.210,1.211], d(acc on real and fake)[42.357,42.313], g(loss)[0.314]\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
      ">479, c(loss and accuracy)[0.823,69], d(loss on real and fake)[1.210,1.211], d(acc on real and fake)[42.353,42.309], g(loss)[0.314]\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
      ">480, c(loss and accuracy)[0.823,69], d(loss on real and fake)[1.210,1.211], d(acc on real and fake)[42.353,42.309], g(loss)[0.314]\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      ">481, c(loss and accuracy)[0.823,69], d(loss on real and fake)[1.210,1.211], d(acc on real and fake)[42.355,42.311], g(loss)[0.314]\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
      ">482, c(loss and accuracy)[0.823,69], d(loss on real and fake)[1.211,1.211], d(acc on real and fake)[42.356,42.312], g(loss)[0.314]\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
      ">483, c(loss and accuracy)[0.823,69], d(loss on real and fake)[1.210,1.211], d(acc on real and fake)[42.364,42.320], g(loss)[0.314]\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
      ">484, c(loss and accuracy)[0.823,69], d(loss on real and fake)[1.211,1.211], d(acc on real and fake)[42.360,42.316], g(loss)[0.314]\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
      ">485, c(loss and accuracy)[0.823,69], d(loss on real and fake)[1.211,1.211], d(acc on real and fake)[42.362,42.319], g(loss)[0.314]\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
      ">486, c(loss and accuracy)[0.823,69], d(loss on real and fake)[1.211,1.212], d(acc on real and fake)[42.359,42.316], g(loss)[0.314]\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
      ">487, c(loss and accuracy)[0.823,69], d(loss on real and fake)[1.211,1.212], d(acc on real and fake)[42.359,42.315], g(loss)[0.314]\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
      ">488, c(loss and accuracy)[0.822,69], d(loss on real and fake)[1.211,1.212], d(acc on real and fake)[42.357,42.314], g(loss)[0.314]\n"
     ]
    }
   ],
   "source": [
    "dataset = load_real_samples(X_train,y_train)\n",
    "\n",
    "best_c_model, best_d_model, best_g_model, best_gan = train_script(latent_dim=500, dataset=dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "11a1529b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"supervised_discrimminator\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"supervised_discrimminator\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ input_layer (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">768</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>)            │        <span style=\"color: #00af00; text-decoration-color: #00af00\">76,900</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ leaky_re_lu_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LeakyReLU</span>)       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)              │           <span style=\"color: #00af00; text-decoration-color: #00af00\">303</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ activation (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)              │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ input_layer (\u001b[38;5;33mInputLayer\u001b[0m)        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m768\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100\u001b[0m)            │        \u001b[38;5;34m76,900\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ leaky_re_lu_1 (\u001b[38;5;33mLeakyReLU\u001b[0m)       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout (\u001b[38;5;33mDropout\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_2 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m3\u001b[0m)              │           \u001b[38;5;34m303\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ activation (\u001b[38;5;33mActivation\u001b[0m)         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m3\u001b[0m)              │             \u001b[38;5;34m0\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">231,611</span> (904.73 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m231,611\u001b[0m (904.73 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">77,203</span> (301.57 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m77,203\u001b[0m (301.57 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Optimizer params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">154,408</span> (603.16 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Optimizer params: \u001b[0m\u001b[38;5;34m154,408\u001b[0m (603.16 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "best_c_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "29415ccd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"discriminator-fake\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"discriminator-fake\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ input_layer (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">768</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>)            │        <span style=\"color: #00af00; text-decoration-color: #00af00\">76,900</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ leaky_re_lu_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LeakyReLU</span>)       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)              │           <span style=\"color: #00af00; text-decoration-color: #00af00\">303</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>)              │            <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ input_layer (\u001b[38;5;33mInputLayer\u001b[0m)        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m768\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100\u001b[0m)            │        \u001b[38;5;34m76,900\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ leaky_re_lu_1 (\u001b[38;5;33mLeakyReLU\u001b[0m)       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout (\u001b[38;5;33mDropout\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_2 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m3\u001b[0m)              │           \u001b[38;5;34m303\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_3 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4\u001b[0m)              │            \u001b[38;5;34m16\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">231,659</span> (904.92 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m231,659\u001b[0m (904.92 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">77,219</span> (301.64 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m77,219\u001b[0m (301.64 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Optimizer params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">154,440</span> (603.29 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Optimizer params: \u001b[0m\u001b[38;5;34m154,440\u001b[0m (603.29 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "best_d_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfaab381",
   "metadata": {},
   "source": [
    "Generating Fake Data for Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c4dfba46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m85/85\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "\u001b[1m255/255\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n"
     ]
    }
   ],
   "source": [
    "## Creating Fake Data\n",
    "\n",
    "n_classes = 3\n",
    "\n",
    "# Augmenting Test Data\n",
    "fake_test_tokens, fake_test_labels = generate_fake_samples(best_g_model, latent_dim=500, n_samples=int(X_test.shape[0] / n_classes))\n",
    "\n",
    "X_test_with_fakes = np.concatenate((X_test, fake_test_tokens), axis = 0)\n",
    "y_test_with_fakes = np.concatenate((y_test, fake_test_labels.flatten()), axis = 0)\n",
    "\n",
    "indices = tf.range(start=0, limit=tf.shape(X_test_with_fakes)[0], dtype=tf.int32)\n",
    "shuffled_indices = tf.random.shuffle(indices)\n",
    "\n",
    "X_test_with_fakes = tf.gather(X_test_with_fakes, shuffled_indices)\n",
    "y_test_with_fakes = tf.gather(y_test_with_fakes, shuffled_indices)\n",
    "\n",
    "\n",
    "#Augmenting Train Data\n",
    "fake_train_tokens, fake_train_labels = generate_fake_samples(best_g_model, latent_dim=500, n_samples=int(X_train.shape[0] / n_classes))\n",
    "\n",
    "X_train_with_fakes = np.concatenate((X_train, fake_train_tokens), axis=0)\n",
    "y_train_with_fakes = np.concatenate((y_train, fake_train_labels.flatten()), axis=0)\n",
    "\n",
    "indices = tf.range(start=0, limit=tf.shape(X_train_with_fakes)[0], dtype=tf.int32)\n",
    "shuffled_indices = tf.random.shuffle(indices)\n",
    "\n",
    "X_train_with_fakes = tf.gather(X_train_with_fakes, shuffled_indices)\n",
    "y_train_with_fakes = tf.gather(y_train_with_fakes, shuffled_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e92279d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'Class distribution in Balanced (Undersampled) Test Dataset')"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(y_test_with_fakes)\n",
    "plt.title(\"Class distribution in Balanced (Undersampled) Test Dataset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9728f744",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'Class distribution in Balanced (Undersampled) Train Dataset')"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(y_train_with_fakes)\n",
    "plt.title(\"Class distribution in Balanced (Undersampled) Train Dataset\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "643b298d",
   "metadata": {},
   "source": [
    "Generate Predictions from the Discriminator Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3905d217",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m340/340\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step  \n",
      "\u001b[1m340/340\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 937us/step - accuracy: 0.6367 - loss: 0.9439\n",
      "\u001b[1m 115/1019\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 881us/step"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/thossain64/anaconda3/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/thossain64/anaconda3/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/thossain64/anaconda3/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1019/1019\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 903us/step\n",
      "\u001b[1m1019/1019\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 931us/step - accuracy: 0.6452 - loss: 0.9265\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/thossain64/anaconda3/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/thossain64/anaconda3/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/thossain64/anaconda3/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "# Test Set Predictions \n",
    "d_test_predictions = best_d_model.predict(X_test_with_fakes)\n",
    "d_test_predicted_labels = d_test_predictions.argmax(axis=1)\n",
    "d_test_loss, d_test_accuracy = best_d_model.evaluate(X_test_with_fakes, y_test_with_fakes)\n",
    "\n",
    "test_discriminator_classification_report = classification_report(y_test_with_fakes, d_test_predicted_labels, target_names=['Hesiod', 'Homer', 'Homeric Hymns', 'Generated CLS'])\n",
    "\n",
    "# Train Set Predictions\n",
    "d_train_preds = best_d_model.predict(X_train_with_fakes)\n",
    "d_train_predicted_labels = d_train_preds.argmax(axis=1)\n",
    "d_train_loss, d_train_acc = best_d_model.evaluate(X_train_with_fakes, y_train_with_fakes)  # Corrected line\n",
    "\n",
    "train_discriminator_classification_report = classification_report(y_train_with_fakes, d_train_predicted_labels, target_names=['Hesiod', 'Homer', 'Homeric Hymns', 'Generated CLS'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b427088",
   "metadata": {},
   "source": [
    "Generate Predictions from the Baseline Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3792e3c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m255/255\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "\u001b[1m255/255\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 901us/step - accuracy: 0.8465 - loss: 0.7011\n",
      "\u001b[1m118/764\u001b[0m \u001b[32m━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 862us/step"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/thossain64/anaconda3/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/thossain64/anaconda3/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/thossain64/anaconda3/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m764/764\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 851us/step\n",
      "\u001b[1m764/764\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 904us/step - accuracy: 0.8506 - loss: 0.6912\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/thossain64/anaconda3/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/thossain64/anaconda3/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/thossain64/anaconda3/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "#Test Set Predictions\n",
    "c_test_predictions = best_c_model.predict(X_test)\n",
    "c_test_predicted_labels = c_test_predictions.argmax(axis=1)\n",
    "c_test_loss, c_test_accuracy = best_c_model.evaluate(X_test, y_test)\n",
    "\n",
    "test_baseline_classification_report = classification_report(y_test, c_test_predicted_labels, target_names=['Hesiod', 'Homer', 'Homeric Hymns'])\n",
    "\n",
    "# Train Set Predictions\n",
    "c_train_predictions = best_c_model.predict(X_train)\n",
    "c_train_predicted_labels = c_train_predictions.argmax(axis=1)\n",
    "c_train_loss, c_train_accuracy = best_c_model.evaluate(X_train, y_train)\n",
    "\n",
    "train_baseline_classification_report = classification_report(y_train, c_train_predicted_labels, target_names=['Hesiod', 'Homer', 'Homeric Hymns'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3b856af",
   "metadata": {},
   "source": [
    "Performance Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "fd829734",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Discriminator Testing Set Performance: Imbalanced Dataset\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "       Hesiod       0.00      0.00      0.00       613\n",
      "        Homer       0.64      1.00      0.78      6931\n",
      "Homeric Hymns       0.00      0.00      0.00       606\n",
      "Generated CLS       0.00      0.00      0.00      2716\n",
      "\n",
      "     accuracy                           0.64     10866\n",
      "    macro avg       0.16      0.25      0.19     10866\n",
      " weighted avg       0.41      0.64      0.50     10866\n",
      "\n",
      "\n",
      "discriminator test loss:  0.9413930773735046\n",
      "discriminator test accuracy:  0.6378611922264099\n"
     ]
    }
   ],
   "source": [
    "print(\"Discriminator Testing Set Performance: Imbalanced Dataset\")\n",
    "print(test_discriminator_classification_report)\n",
    "print(\"\")\n",
    "print(\"discriminator test loss: \", d_test_loss)\n",
    "print(\"discriminator test accuracy: \", d_test_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "850bc9de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline Testing Set Performance: Imbalanced Dataset\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "       Hesiod       0.07      0.01      0.01       613\n",
      "        Homer       0.85      0.99      0.91      6931\n",
      "Homeric Hymns       0.00      0.00      0.00       606\n",
      "\n",
      "     accuracy                           0.84      8150\n",
      "    macro avg       0.31      0.33      0.31      8150\n",
      " weighted avg       0.73      0.84      0.78      8150\n",
      "\n",
      "\n",
      "baseline test loss:  0.7072656154632568\n",
      "baseline test accuracy:  0.842699408531189\n"
     ]
    }
   ],
   "source": [
    "print(\"Baseline Testing Set Performance: Imbalanced Dataset\")\n",
    "print(test_baseline_classification_report)\n",
    "print(\"\")\n",
    "print(\"baseline test loss: \", c_test_loss)\n",
    "print(\"baseline test accuracy: \", c_test_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "328ef4ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Discriminator Training Set Performance: Imbalanced Dataset\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "       Hesiod       0.00      0.00      0.00      1739\n",
      "        Homer       0.64      1.00      0.78     20972\n",
      "Homeric Hymns       0.00      0.00      0.00      1736\n",
      "Generated CLS       0.00      0.00      0.00      8149\n",
      "\n",
      "     accuracy                           0.64     32596\n",
      "    macro avg       0.16      0.25      0.20     32596\n",
      " weighted avg       0.41      0.64      0.50     32596\n",
      "\n",
      "\n",
      "discriminator train loss:  0.9289253354072571\n",
      "discriminator train accuracy:  0.6433918476104736\n"
     ]
    }
   ],
   "source": [
    "print(\"Discriminator Training Set Performance: Imbalanced Dataset\")\n",
    "print(train_discriminator_classification_report)\n",
    "print(\"\")\n",
    "print(\"discriminator train loss: \", d_train_loss)\n",
    "print(\"discriminator train accuracy: \", d_train_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e41d2fe3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline Training Set Performance: Imbalanced Dataset\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "       Hesiod       0.03      0.00      0.01      1739\n",
      "        Homer       0.86      0.99      0.92     20972\n",
      "Homeric Hymns       0.00      0.00      0.00      1736\n",
      "\n",
      "     accuracy                           0.85     24447\n",
      "    macro avg       0.29      0.33      0.31     24447\n",
      " weighted avg       0.74      0.85      0.79     24447\n",
      "\n",
      "\n",
      "baseline train loss:  0.6948044896125793\n",
      "baseline train accuracy:  0.8482022285461426\n"
     ]
    }
   ],
   "source": [
    "print(\"Baseline Training Set Performance: Imbalanced Dataset\")\n",
    "print(train_baseline_classification_report)\n",
    "print(\"\")\n",
    "print(\"baseline train loss: \", c_train_loss)\n",
    "print(\"baseline train accuracy: \", c_train_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96a90d2e",
   "metadata": {},
   "source": [
    "Data Export for Further Analysis "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5bb74105",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Baseline Data Export \n",
    "\n",
    "baseline_test_predictions = pd.DataFrame({\"baseline_test_labels\": y_test, \n",
    "                                          \"baseline_test_preds\": c_test_predicted_labels})\n",
    "\n",
    "baseline_train_predictions = pd.DataFrame({\"baseline_train_labels\": y_train, \n",
    "                                          \"baseline_train_preds\": c_train_predicted_labels})\n",
    "\n",
    "baseline_train_predictions.to_csv(\"imbalanced_baseline_train_predictions.csv\", index=True)\n",
    "baseline_test_predictions.to_csv(\"imbalanced_baseline_test_predictions.csv\", index=True)\n",
    "\n",
    "X_test_numpy = X_test.numpy()\n",
    "X_train_numpy = X_train.numpy()\n",
    "\n",
    "# Convert numpy array to DataFrame\n",
    "X_test_df = pd.DataFrame(X_test_numpy)\n",
    "X_train_df = pd.DataFrame(X_train_numpy)\n",
    "\n",
    "# Save the DataFrame to a CSV file\n",
    "X_test_df.to_csv('imbalanced_baseline_X_test.csv', index=True)\n",
    "X_train_df.to_csv('imbalanced_baseline_X_train.csv', index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "81056ccc",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Discriminator Data Export \n",
    "\n",
    "discrim_test_predictions = pd.DataFrame({\"discrim_test_labels\": y_test_with_fakes, \n",
    "                                         \"discrim_test_preds\": d_test_predicted_labels})\n",
    "\n",
    "discrim_train_predictions = pd.DataFrame({\"discrim_train_labels\": y_train_with_fakes, \n",
    "                                          \"discrim_train_preds\": d_train_predicted_labels})\n",
    "\n",
    "discrim_test_predictions.to_csv(\"imbalanced_discrim_test_predictions.csv\", index=True)\n",
    "discrim_train_predictions.to_csv(\"imbalanced_discrim_train_predictions.csv\", index=True)\n",
    "\n",
    "discrim_X_test_numpy = X_test_with_fakes.numpy()\n",
    "discrim_X_train_numpy = X_train_with_fakes.numpy()\n",
    "\n",
    "discrim_X_test_df = pd.DataFrame(discrim_X_test_numpy)\n",
    "discrim_X_train_df = pd.DataFrame(discrim_X_train_numpy)\n",
    "\n",
    "discrim_X_test_df.to_csv('imbalanced_discrim_X_test.csv', index=True)\n",
    "discrim_X_train_df.to_csv('imbalanced_discrim_X_train.csv', index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bae80918-7b50-4d11-9ad2-a5e7013e1d63",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
